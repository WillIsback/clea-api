WillIsback: Alors voila j'ai commencé par faire un backend puis j'ai décidé de faire une API standalone qui permet d'exploiter une base de donnée de document vectorisé.
J'aimerais crée maintenant clea-doc-loader qui aura pour objectif de prendre permettre de charger, découper et formatter le texte d'un corpus en document prêt à être ajouter dans clea-vectordb.

Et du coup je me suis dit que enfaite clea-api serait un framework avec des sous-modules. Actuellement j'ai un definition local de fastapi dans clea-vectordb et je me demande si je ne devrais pas remonter fastapi d'un niveau pour le que le framework Clea-api integre plusieurs module ? où alors est-il preférable d'imbriquer plusier FASTAPI ?

Voila la structure actuelle :
```
      ~/projet/Clea/clea-api  tree                                                                                ✔
.
└── clea-vectordb
    ├── api
    │   ├── database_endpoint.py
    │   ├── __init__.py
    │   ├── __pycache__
    │   │   ├── database_endpoint.cpython-313.pyc
    │   │   ├── __init__.cpython-313.pyc
    │   │   └── search_endpoint.cpython-313.pyc
    │   └── search_endpoint.py
    ├── CHANGELOG.md
    ├── cliff.toml
    ├── Dockerfile
    ├── LICENSE
    ├── main.py
    ├── __pycache__
    │   └── main.cpython-313.pyc
    ├── pyproject.toml
    ├── README.md
    ├── requirements.txt
    ├── start.sh
    ├── static
    │   └── favicon.ico
    ├── test
    │   ├── __pycache__
    │   │   ├── test_database_endpoint.cpython-313-pytest-8.3.5.pyc
    │   │   └── test_search_endpoint.cpython-313-pytest-8.3.5.pyc
    │   ├── test_database_endpoint.py
    │   └── test_search_endpoint.py
    ├── uv.lock
    └── vectordb
        ├── database.py
        ├── embeddings.py
        ├── __init__.py
        ├── __pycache__
        │   ├── database.cpython-313.pyc
        │   ├── embeddings.cpython-313.pyc
        │   ├── __init__.cpython-313.pyc
        │   ├── ranking.cpython-313.pyc
        │   └── search.cpython-313.pyc
        ├── ranking.py
        └── search.py

10 directories, 32 files
```


GitHub Copilot: Pour structurer votre projet en un framework modulaire tout en exploitant FastAPI, voici quelques recommandations :

### 1. **Remonter FastAPI d'un niveau**
Il est préférable de centraliser l'instance principale de FastAPI au niveau du framework (`clea-api`) et de connecter les sous-modules comme des sous-applications. Cela permet de mieux organiser les modules et de les rendre indépendants tout en les intégrant dans une seule API globale.

### 2. **Nouvelle structure proposée**
Voici une structure modifiée pour intégrer plusieurs modules dans `clea-api` :

```
clea-api/
├── clea_vectordb/           # Module pour la gestion des vecteurs
│   ├── api/
│   │   ├── database_endpoint.py
│   │   └── search_endpoint.py
│   ├── vectordb/
│   │   ├── database.py
│   │   ├── embeddings.py
│   │   ├── ranking.py
│   │   └── search.py
│   ├── test/
│   └── __init__.py
├── clea_doc_loader/         # Nouveau module pour charger et formater les documents
│   ├── loader/
│   │   ├── text_splitter.py
│   │   ├── formatter.py
│   │   └── file_loader.py
│   ├── api/
│   │   └── loader_endpoint.py
│   ├── test/
│   └── __init__.py
├── main.py                  # Point d'entrée principal pour FastAPI
├── pyproject.toml
├── requirements.txt
└── README.md
```

### 3. **Centraliser FastAPI dans `main.py`**
Dans `main.py`, vous pouvez créer une instance principale de FastAPI et inclure les routes des différents modules (`clea_vectordb` et `clea_doc_loader`) comme des sous-applications :

```python
from fastapi import FastAPI
from clea_vectordb.api.database_endpoint import router as vectordb_router
from clea_doc_loader.api.loader_endpoint import router as doc_loader_router

app = FastAPI(title="Clea API Framework")

# Inclure les routes des sous-modules
app.include_router(vectordb_router, prefix="/vectordb", tags=["VectorDB"])
app.include_router(doc_loader_router, prefix="/doc-loader", tags=["DocLoader"])

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8080)
```

### 4. **Créer un sous-module `clea-doc-loader`**
Le module `clea-doc-loader` peut contenir des utilitaires pour charger, découper et formater des documents. Voici un exemple de structure pour ce module :

- **`loader/file_loader.py`** : Charger des fichiers (PDF, TXT, etc.).
- **`loader/text_splitter.py`** : Découper le texte en segments.
- **`loader/formatter.py`** : Formater les segments en documents prêts à être insérés dans `clea_vectordb`.

Exemple d'un endpoint pour charger des documents :

```python
from fastapi import APIRouter, UploadFile
from clea_doc_loader.loader.file_loader import load_file

router = APIRouter()

@router.post("/upload")
async def upload_document(file: UploadFile):
    content = await file.read()
    documents = load_file(content)
    return {"status": "success", "documents": documents}
```

### 5. **Avantages de cette approche**
- **Modularité** : Chaque sous-module est autonome, ce qui facilite la maintenance et l'extension.
- **Centralisation** : Une seule instance FastAPI pour gérer toutes les routes.
- **Réutilisabilité** : Les modules peuvent être utilisés indépendamment ou ensemble dans le framework.

### 6. **Quand imbriquer plusieurs instances FastAPI ?**
Imbriquer plusieurs instances FastAPI est utile si :
- Vous voulez déployer chaque module comme un service indépendant.
- Vous avez des besoins spécifiques (par exemple, des middlewares ou configurations différentes par module).

Cependant, pour un framework unifié comme `clea-api`, centraliser FastAPI est généralement plus simple et efficace.

WillIsback: depuis la migration de mes fichiers projet pyproject.toml .venv et uv.lock j'essaye de reconstuire mes dépendances avec echec :
```log
      ~/projet/Clea/clea-api  uv venv                                                                                                                                                      ✔ 
Using CPython 3.13.3 interpreter at: /usr/bin/python3.13
Creating virtual environment at: .venv
Activate with: source .venv/bin/activate
      ~/projet/Clea/clea-api  uv sync                                                                                                                                                      ✔ 
Resolved 86 packages in 1ms
  × Failed to build `clea-api @ file:///home/william/projet/Clea/clea-api`
  ├─▶ The build backend returned an error
  ╰─▶ Call to `setuptools.build_meta.build_editable` failed (exit status: 1)

      [stderr]
      error: Multiple top-level packages discovered in a flat-layout: ['clea_vectordb', 'clea_doc_loader'].

      To avoid accidental inclusion of unwanted files or directories,
      setuptools will not proceed with this build.

      If you are trying to create a single distribution with multiple packages
      on purpose, you should not rely on automatic discovery.
      Instead, consider the following options:

      1. set up custom discovery (`find` directive with `include` or `exclude`)
      2. use a `src-layout`
      3. explicitly set `py_modules` or `packages` with a list of names

      To find more information, look for "package discovery" on setuptools docs.

      hint: This usually indicates a problem with the package or the build environment.
```


GitHub Copilot: L'erreur indique que `setuptools` ne parvient pas à découvrir correctement les packages dans votre projet, car plusieurs packages de haut niveau (clea_vectordb et clea_doc_loader) sont présents dans une structure "flat-layout". Voici comment résoudre ce problème :

---

### Solution 1 : Utiliser un layout basé sur `src/`
Une solution recommandée est de déplacer vos packages dans un répertoire `src/`. Cela permet à `setuptools` de mieux gérer la découverte des packages.

#### Étapes :
1. Réorganisez votre projet pour inclure un dossier `src/` :
   ```
   clea-api/
   ├── src/
   │   ├── clea_vectordb/
   │   ├── clea_doc_loader/
   ├── pyproject.toml
   ├── requirements.txt
   └── ...
   ```

2. Modifiez la configuration de `setuptools` dans pyproject.toml pour indiquer où chercher les packages :
   ```toml
   # filepath: /home/william/projet/Clea/clea-api/pyproject.toml
   [tool.setuptools.packages.find]
   where = ["src"]
   ```

3. Mettez à jour vos imports dans le code pour refléter le chemin `src/`. Par exemple :
   ```python
   from clea_vectordb.api.database_endpoint import router
   ```
   reste inchangé, car `src/` est transparent pour Python.

4. Recréez l'environnement virtuel et synchronisez les dépendances :
   ```bash
   uv venv
   uv sync
   ```

---

### Solution 2 : Configurer manuellement la découverte des packages
Si vous ne souhaitez pas utiliser un layout basé sur `src/`, vous pouvez configurer explicitement les packages à inclure.

#### Étapes :
1. Ajoutez une section `[tool.setuptools.packages.find]` dans pyproject.toml :
   ```toml
   # filepath: /home/william/projet/Clea/clea-api/pyproject.toml
   [tool.setuptools.packages.find]
   include = ["clea_vectordb", "clea_doc_loader"]
   exclude = ["test", "static"]
   ```

2. Recréez l'environnement virtuel et synchronisez les dépendances :
   ```bash
   uv venv
   uv sync
   ```

---

### Solution 3 : Fusionner les packages dans un seul package
Si vous préférez éviter plusieurs packages de haut niveau, vous pouvez fusionner clea_vectordb et clea_doc_loader dans un seul package principal, par exemple `clea_api`.

#### Étapes :
1. Réorganisez votre projet :
   ```
   clea-api/
   ├── clea_api/
   │   ├── vectordb/
   │   ├── doc_loader/
   ├── pyproject.toml
   ├── requirements.txt
   └── ...
   ```

2. Modifiez les imports pour refléter la nouvelle structure :
   ```python
   from clea_api.vectordb.api.database_endpoint import router
   ```

3. Mettez à jour pyproject.toml pour inclure uniquement le package `clea_api` :
   ```toml
   # filepath: /home/william/projet/Clea/clea-api/pyproject.toml
   [tool.setuptools.packages.find]
   where = ["."]
   include = ["clea_api"]
   exclude = ["test", "static"]
   ```

4. Recréez l'environnement virtuel et synchronisez les dépendances :
   ```bash
   uv venv
   uv sync
   ```

---

### Recommandation
La **Solution 1 (layout basé sur `src/`)** est la plus propre et recommandée pour les projets Python modernes. Elle évite les conflits de noms et facilite la gestion des packages.

Si vous optez pour cette solution, voici un résumé des commandes à exécuter après avoir réorganisé le projet :
```bash
mv clea_vectordb clea_doc_loader src/
echo '[tool.setuptools.packages.find]\nwhere = ["src"]' >> pyproject.toml
uv venv
uv sync
```

WillIsback: Voila ma structure projet aide moi à reprendre correctement pyproject :
```shell
      ~/projet/Clea/clea-api  tree                                                                                                                                                         ✔ 
.
├── CHANGELOG.md
├── clea_doc_loader
│   ├── api
│   ├── docs
│   ├── src
│   ├── static
│   └── test
├── clea_vectordb
│   ├── api
│   │   ├── database_endpoint.py
│   │   ├── __init__.py
│   │   └── search_endpoint.py
│   ├── docs
│   ├── src
│   │   ├── database.py
│   │   ├── embeddings.py
│   │   ├── __init__.py
│   │   ├── ranking.py
│   │   └── search.py
│   ├── static
│   │   └── favicon.ico
│   └── test
│       ├── test_database_endpoint.py
│       └── test_search_endpoint.py
├── cliff.toml
├── Dockerfile
├── LICENSE
├── main.py
├── pyproject.toml
├── README.md
├── requirements.txt
├── start.sh
└── uv.lock

13 directories, 21 files
```


GitHub Copilot: Pour résoudre les problèmes liés à la configuration de votre projet et à la découverte des packages, voici une version corrigée et adaptée de votre fichier pyproject.toml en fonction de votre structure actuelle.

---

### Nouvelle version de pyproject.toml

```toml
[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "clea-api"
version = "0.1.0"
description = "Cléa-API: A framework for document load and hybrid search engine combining vector and metadata-based search. CRUD operations are performed using FastAPI."
readme = {file = "README.md", content-type = "text/markdown"}
requires-python = ">=3.11"
license = "MIT"
license-files = ["LICENSE"]
authors = [
    {name = "William", email = "pikachu0442@gmail.com"}
]
keywords = ["vector search", "FastAPI", "pgvector", "hybrid search", "CRUD", "document load"]
classifiers = [
    "Development Status :: 4 - Beta",
    "Intended Audience :: Developers",
    "Topic :: Software Development :: Libraries",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.11",
]
dependencies = [
    "fastapi==0.115.12",
    "uvicorn==0.34.2",
    "sqlalchemy==2.0.40",
    "pgvector==0.4.1",
    "pydantic==2.11.3",
    "pytest==8.3.5",
    "scikit-learn==1.6.1",
    "sentence-transformers==4.1.0",
    "transformers==4.51.3",
    "torch==2.7.0",
    "python-dotenv==1.1.0",
    "requests==2.32.3",
    "typer==0.15.3",
    "watchfiles==1.0.5"
]

[tool.setuptools.packages.find]
where = ["."]
include = ["clea_doc_loader", "clea_vectordb"]
exclude = ["static", "test", "docs"]
```

---

### Explications des modifications

1. **Section `[tool.setuptools.packages.find]` :**
   - J'ai conservé `where = ["."]` pour indiquer que les packages sont à la racine du projet.
   - J'ai inclus explicitement clea_doc_loader et clea_vectordb comme packages.
   - J'ai exclu les répertoires inutiles comme `static`, `test`, et `docs`.

2. **Dépendances :**
   - J'ai gardé uniquement les dépendances essentielles pour votre projet, en supprimant celles qui semblaient inutiles ou redondantes.

3. **Structure du projet :**
   - Votre structure actuelle est compatible avec un layout "flat" (sans `src/`), donc pas besoin de déplacer les fichiers dans un répertoire `src/`.

---

### Étapes pour reconstruire l'environnement

1. **Supprimez l'ancien environnement virtuel :**
   ```bash
   rm -rf .venv
   ```

2. **Recréez l'environnement virtuel :**
   ```bash
   uv venv
   ```

3. **Synchronisez les dépendances :**
   ```bash
   uv sync
   ```

4. **Vérifiez que tout fonctionne :**
   Lancez votre application pour vérifier que les imports et les dépendances sont correctement configurés :
   ```bash
   uvicorn main:app --reload
   ```

---

### Si le problème persiste

Si vous rencontrez toujours des erreurs liées à la découverte des packages, envisagez de passer à un layout basé sur `src/` (Solution 1 dans ma réponse précédente). Cela simplifie souvent la gestion des packages dans des projets complexes.

WillIsback: au vue de ma nouvelle strucute il va falloir refaire mes routes FastApi dans mon main :
```log
      ~/projet/Clea/clea-api  tree                                                                                                                                                         ✔ 
.
├── CHANGELOG.md
├── clea_doc_loader
│   ├── api
│   ├── docs
│   ├── src
│   ├── static
│   └── test
├── clea_vectordb
│   ├── api
│   │   ├── database_endpoint.py
│   │   ├── __init__.py
│   │   └── search_endpoint.py
│   ├── docs
│   ├── src
│   │   ├── database.py
│   │   ├── embeddings.py
│   │   ├── __init__.py
│   │   ├── ranking.py
│   │   └── search.py
│   ├── static
│   │   └── favicon.ico
│   └── test
│       ├── test_database_endpoint.py
│       └── test_search_endpoint.py
├── cliff.toml
├── Dockerfile
├── LICENSE
├── main.py
├── pyproject.toml
├── README.md
├── requirements.txt
├── start.sh
└── uv.lock
```


GitHub Copilot: Pour adapter vos routes FastAPI dans main.py à votre nouvelle structure de projet, voici les modifications nécessaires. Vous devez inclure les routes des modules clea_vectordb et clea_doc_loader en fonction de leur emplacement actuel.

---

### Mise à jour de main.py

Voici une version corrigée de votre fichier main.py :

```python
from fastapi import FastAPI
from fastapi.responses import JSONResponse, FileResponse
from fastapi.exceptions import RequestValidationError
from starlette.exceptions import HTTPException as StarletteHTTPException
from fastapi.middleware.cors import CORSMiddleware
from sqlalchemy.orm import Session
from sqlalchemy import text
from clea_vectordb.api.database_endpoint import router as database_router
from clea_vectordb.api.search_endpoint import router as search_router
from clea_doc_loader.api import router as doc_loader_router
from dotenv import load_dotenv
import os
import subprocess

# Charger les variables d'environnement depuis le fichier .env
load_dotenv()

POSTGRES_USER = os.getenv("DB_USER")
POSTGRES_PASSWORD = os.getenv("DB_PASSWORD")
POSTGRES_DB = os.getenv("DB_NAME")
POSTGRES_HOST = os.getenv("DB_HOST", "localhost")
POSTGRES_PORT = os.getenv("DB_PORT", "5432")

API_HOST = os.getenv("API_HOST", "localhost")
API_PORT = os.getenv("API_PORT", "8080")

# Chemin vers le fichier favicon
FAVICON_PATH = os.path.join(os.path.dirname(__file__), "clea_vectordb", "static", "favicon.ico")

def start_postgres() -> bool:
    """
    Démarre le service PostgreSQL en utilisant les variables d'environnement.
    """
    try:
        print("Démarrage de PostgreSQL...")
        subprocess.run(["sudo", "systemctl", "start", "postgresql"], check=True)
        print("PostgreSQL démarré avec succès.")
        
        # Vérification de la connexion à la base de données
        import psycopg2
        conn = psycopg2.connect(
            dbname=POSTGRES_DB,
            user=POSTGRES_USER,
            password=POSTGRES_PASSWORD,
            host=POSTGRES_HOST,
            port=POSTGRES_PORT
        )
        conn.close()
        print(f"Connexion réussie à la base de données '{POSTGRES_DB}' sur {POSTGRES_HOST}:{POSTGRES_PORT}.")
        return True
    except subprocess.CalledProcessError as e:
        print(f"Erreur lors du démarrage de PostgreSQL : {e}")
        return False
    except Exception as e:
        print(f"Erreur de connexion à la base de données : {e}")
        return False

# Créer l'application FastAPI principale
app = FastAPI(
    title="Clea API",
    description="API pour gérer les documents et effectuer des recherches.",
    version="1.0.0",
    docs_url="/docs",  # Swagger UI
    redoc_url="/redoc"  # ReDoc
)

# Ajouter le middleware CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Autoriser uniquement votre frontend
    allow_credentials=True,
    allow_methods=["*"],  # Autoriser toutes les méthodes HTTP (GET, POST, PUT, DELETE, etc.)
    allow_headers=["*"],  # Autoriser tous les en-têtes
)

# Inclure les routes des différents modules
app.include_router(database_router, prefix="/database", tags=["Database"])
app.include_router(search_router, prefix="/search", tags=["Search"])
app.include_router(doc_loader_router, prefix="/doc-loader", tags=["DocLoader"])

# Gestionnaires d'erreurs globaux
@app.exception_handler(StarletteHTTPException)
async def http_exception_handler(request, exc):
    return JSONResponse({"error": exc.detail}, status_code=exc.status_code)

@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request, exc):
    return JSONResponse({"error": "Invalid request", "details": exc.errors()}, status_code=422)

@app.exception_handler(Exception)
async def global_exception_handler(request, exc):
    return JSONResponse({"error": "Internal server error", "details": str(exc)}, status_code=500)

# Point de santé
@app.get("/")
async def root():
    return {"message": "Clea API is running"}

@app.get("/favicon.ico", include_in_schema=False)
async def favicon():
    """
    Endpoint pour servir la favicon.
    """
    return FileResponse(FAVICON_PATH)

# Initialisation de la base de données
def setup_database():
    """Initialise la base de données et configure pgvector"""
    print("Initialisation de la base de données...")
    from clea_vectordb.src.database import init_db, engine
    init_db()
    
    # Ajout de l'extension pgvector si elle n'existe pas déjà
    with Session(engine) as session:
        try:
            session.execute(text("CREATE EXTENSION IF NOT EXISTS vector"))
            session.commit()
            print("Extension pgvector installée avec succès")
        except Exception as e:
            print(f"Erreur lors de l'installation de pgvector: {e}")
            session.rollback()
            return False
        
        # Ajouter la colonne d'embedding si elle n'existe pas
        try:
            session.execute(text("ALTER TABLE documents ADD COLUMN IF NOT EXISTS embedding vector(768)"))
            session.commit()
            print("Colonne d'embedding ajoutée avec succès")
        except Exception as e:
            print(f"Erreur lors de l'ajout de la colonne d'embedding: {e}")
            session.rollback()
            return False
    
    return True

if __name__ == '__main__':
    print("=== Clea API ===\n")
    if not start_postgres():
        raise RuntimeError("Échec du démarrage de PostgreSQL. Abandon.")
    
    # Étape 1: Configuration de la base de données
    if not setup_database():
        raise RuntimeError("Échec de la configuration de la base de données. Abandon.")
```

---

### Changements effectués

1. **Import des routes :**
   - Les routes de clea_vectordb (`database_endpoint` et `search_endpoint`) sont importées via leurs `router`.
   - Les routes de clea_doc_loader sont également incluses via un `router`.

2. **Chemin vers la favicon :**
   - Le chemin a été mis à jour pour pointer vers favicon.ico.

3. **Inclusion des routes :**
   - Les routes des modules sont incluses avec des préfixes (`/database`, `/search`, `/doc-loader`) pour les organiser.

4. **Initialisation de la base de données :**
   - Les fonctions liées à la base de données utilisent le module `clea_vectordb.src.database`.

---

### Vérification

1. **Lancer l'application :**
   ```bash
   uvicorn main:app --reload
   ```

2. **Tester les endpoints :**
   - Accédez à `/docs` pour voir la documentation Swagger.
   - Testez les routes `/database`, `/search`, et `/doc-loader`.

3. **Assurez-vous que PostgreSQL est démarré :**
   ```bash
   sudo systemctl status postgresql
   ```

WillIsback: est-ce que tous les message d'avertissement sur les imports sont dû à ma mauvaise gestion des paquets avec uv ou alors Pylance n'est pas compatible avec uv ?

Voici ce que le commande `uv pip list` donne :
```shell
      ~/projet/Clea/clea-api  uv pip list                                                                                                                                                  ✔ 
Package                  Version   Editable project location
------------------------ --------- ----------------------------------
annotated-types          0.7.0
anyio                    4.9.0
certifi                  2025.4.26
charset-normalizer       3.4.1
clea-api                 0.1.0     /home/william/projet/Clea/clea-api
click                    8.1.8
fastapi                  0.115.12
filelock                 3.18.0
fsspec                   2025.3.2
greenlet                 3.2.1
h11                      0.16.0
huggingface-hub          0.30.2
idna                     3.10
iniconfig                2.1.0
jinja2                   3.1.6
joblib                   1.4.2
markdown-it-py           3.0.0
markupsafe               3.0.2
mdurl                    0.1.2
mpmath                   1.3.0
networkx                 3.4.2
numpy                    2.2.5
nvidia-cublas-cu12       12.6.4.1
nvidia-cuda-cupti-cu12   12.6.80
nvidia-cuda-nvrtc-cu12   12.6.77
nvidia-cuda-runtime-cu12 12.6.77
nvidia-cudnn-cu12        9.5.1.17
nvidia-cufft-cu12        11.3.0.4
nvidia-cufile-cu12       1.11.1.6
nvidia-curand-cu12       10.3.7.77
nvidia-cusolver-cu12     11.7.1.2
nvidia-cusparse-cu12     12.5.4.2
nvidia-cusparselt-cu12   0.6.3
nvidia-nccl-cu12         2.26.2
nvidia-nvjitlink-cu12    12.6.85
nvidia-nvtx-cu12         12.6.77
packaging                25.0
pgvector                 0.4.1
pillow                   11.2.1
pluggy                   1.5.0
pydantic                 2.11.3
pydantic-core            2.33.1
pygments                 2.19.1
pytest                   8.3.5
python-dotenv            1.1.0
pyyaml                   6.0.2
regex                    2024.11.6
requests                 2.32.3
rich                     14.0.0
safetensors              0.5.3
scikit-learn             1.6.1
scipy                    1.15.2
sentence-transformers    4.1.0
setuptools               80.0.0
shellingham              1.5.4
sniffio                  1.3.1
sqlalchemy               2.0.40
starlette                0.46.2
sympy                    1.14.0
threadpoolctl            3.6.0
tokenizers               0.21.1
torch                    2.7.0
tqdm                     4.67.1
transformers             4.51.3
triton                   3.3.0
typer                    0.15.3
typing-extensions        4.13.2
typing-inspection        0.4.0
urllib3                  2.4.0
uvicorn                  0.34.2
```


GitHub Copilot: Les messages d'avertissement sur les imports dans votre projet peuvent être causés par plusieurs facteurs. Voici une analyse des causes possibles et des solutions :

---

### 1. **Mauvaise gestion des paquets avec `uv`**
`uv` (ou `universal virtualenv`) semble bien gérer vos dépendances, comme le montre la sortie de `uv pip list`. Cependant, si les dépendances ne sont pas correctement installées ou si votre projet n'est pas configuré correctement, cela peut entraîner des problèmes d'import.

#### Vérifications :
- Assurez-vous que votre projet est bien installé en mode **editable** (ce qui semble être le cas avec `clea-api 0.1.0` dans clea-api).
- Vérifiez que vos dépendances sont bien listées dans pyproject.toml et qu'elles sont installées dans l'environnement virtuel.

#### Solution :
Re-synchronisez les dépendances pour vous assurer qu'elles sont correctement installées :
```bash
uv sync
```

---

### 2. **Problème de compatibilité entre Pylance et `uv`**
Pylance (l'extension de langage pour Python dans VS Code) peut ne pas détecter correctement les dépendances ou les modules si :
- L'environnement virtuel n'est pas activé dans VS Code.
- Les chemins des modules ne sont pas correctement configurés.

#### Vérifications :
- Assurez-vous que l'environnement virtuel est bien activé dans VS Code :
  1. Ouvrez la **palette de commandes** (`Ctrl+Shift+P`).
  2. Recherchez et sélectionnez `Python: Select Interpreter`.
  3. Choisissez l'interpréteur Python de votre environnement virtuel (`.venv`).

- Vérifiez que Pylance utilise le bon environnement :
  1. Allez dans les paramètres de VS Code (`Ctrl+,`).
  2. Recherchez `Python › Analysis: Extra Paths`.
  3. Ajoutez le chemin vers votre projet si nécessaire :
     ```json
     [
         "./clea_doc_loader",
         "./clea_vectordb"
     ]
     ```

#### Solution :
Redémarrez VS Code après avoir sélectionné le bon interpréteur et configuré les chemins.

---

### 3. **Problème de structure du projet**
Votre projet utilise une structure "flat" (sans répertoire `src/`), ce qui peut parfois poser problème pour la découverte des modules.

#### Vérifications :
- Assurez-vous que les modules clea_doc_loader et clea_vectordb sont bien détectés comme packages Python.
- Vérifiez que les fichiers `__init__.py` sont présents dans les répertoires clea_doc_loader et clea_vectordb.

#### Solution :
Si les problèmes persistent, envisagez de passer à une structure basée sur `src/` pour éviter les conflits de noms et faciliter la découverte des modules :
```
clea-api/
├── src/
│   ├── clea_doc_loader/
│   ├── clea_vectordb/
```
Mettez ensuite à jour pyproject.toml :
```toml
[tool.setuptools.packages.find]
where = ["src"]
```

---

### 4. **Problème avec les dépendances spécifiques**
Certaines dépendances comme `fastapi`, `sqlalchemy`, ou `pydantic` peuvent ne pas être correctement installées ou détectées.

#### Vérifications :
- Vérifiez que les versions des dépendances sont compatibles entre elles.
- Assurez-vous que les dépendances sont bien installées dans l'environnement virtuel :
  ```bash
  uv pip show fastapi
  uv pip show sqlalchemy
  ```

#### Solution :
Réinstallez les dépendances problématiques :
```bash
uv pip install --force-reinstall fastapi sqlalchemy
```

---

### 5. **Problème avec les imports relatifs**
Si vous utilisez des imports relatifs dans vos modules, cela peut causer des avertissements si les chemins ne sont pas correctement configurés.

#### Vérifications :
- Assurez-vous que les imports dans vos fichiers Python sont corrects. Par exemple :
  ```python
  from clea_vectordb.api.database_endpoint import router as database_router
  ```

#### Solution :
Si vous utilisez des imports relatifs, remplacez-les par des imports absolus pour éviter les conflits.

---

### Résumé des étapes à suivre

1. **Re-synchronisez les dépendances :**
   ```bash
   uv sync
   ```

2. **Activez l'environnement virtuel dans VS Code :**
   - Sélectionnez l'interpréteur Python correspondant à .venv.

3. **Ajoutez les chemins des modules dans les paramètres de Pylance :**
   ```json
   [
       "./clea_doc_loader",
       "./clea_vectordb"
   ]
   ```

4. **Vérifiez la structure du projet :**
   - Assurez-vous que les fichiers `__init__.py` sont présents dans clea_doc_loader et clea_vectordb.

5. **Redémarrez VS Code** pour appliquer les changements.

---

Si les problèmes persistent, partagez les messages d'erreur ou d'avertissement spécifiques pour une analyse plus approfondie.

WillIsback:   File "/home/william/projet/Clea/clea-api/main.py", line 8, in <module>
    from clea_vectordb.api.database_endpoint import router as database_router
ImportError: cannot import name 'router' from 'clea_vectordb.api.database_endpoint' (/home/william/projet/Clea/clea-api/clea_vectordb/api/database_endpoint.py)

GitHub Copilot: L'erreur `ImportError: cannot import name 'router' from 'clea_vectordb.api.database_endpoint'` indique que le module `database_endpoint.py` n'expose pas d'objet nommé `router`. Cela est probablement dû à l'absence de définition explicite d'un objet `APIRouter` nommé `router` dans ce fichier.

---

### Solution : Ajouter un objet `router` dans `database_endpoint.py`

Dans FastAPI, pour inclure des routes dans un fichier, vous devez définir un objet `APIRouter` et l'utiliser pour enregistrer vos endpoints. Voici comment corriger le fichier `database_endpoint.py` :

#### Correction de `database_endpoint.py`

Ajoutez un objet `router` et remplacez `app` par `router` dans vos définitions de routes :

```python
from fastapi import APIRouter, HTTPException, Body
from clea_vectordb.src.database import (
    get_db,
    Document,
    DocumentCreate,
    DocumentResponse,
    DocumentUpdate,
    add_documents,
    delete_document,
    update_document,
)
from typing import List

# Définir un APIRouter
router = APIRouter()

@router.post("/add_document", tags=["Database"], summary="Ajouter des documents", response_model=List[DocumentResponse])
async def add_documents_endpoint(documents: List[DocumentCreate] = Body(...)):
    """
    Ajoute une liste de documents à la base de données.
    """
    results = add_documents(documents)
    if results["errors"]:
        raise ValueError(f"Erreur lors de l'ajout des documents : {results['errors']}")
    print(f"Documents ajoutés : {results['added']}")
    return [
        {
            "id": doc["id"],
            "title": doc["title"],
            "content": doc.get("content", ""),
            "theme": doc.get("theme", ""),
            "document_type": doc.get("document_type", ""),
            "publish_date": doc.get("publish_date"),
        }
        for doc in results["added"]
    ]

@router.delete("/delete_document", tags=["Database"], summary="Supprimer un document", description="Supprime un document de la base de données en fonction de son ID.")
async def delete_document_endpoint(document_id: int):
    """
    Supprime un document de la base de données.
    """
    result = delete_document(document_id)
    return result

@router.put("/update_document", tags=["Database"], summary="Mettre à jour un document", response_model=DocumentResponse)
async def update_document_endpoint(payload: DocumentUpdate = Body(...)):
    """
    Met à jour un document existant dans la base de données.
    """
    results = update_document(payload)
    if "error" in results:
        raise HTTPException(status_code=400, detail=results["error"])
    print(f"Document mis à jour : {results}")
    return results

@router.get("/list_documents", tags=["Database"], summary="Lister les documents", response_model=List[DocumentResponse])
async def list_documents_endpoint():
    """
    Affiche la liste des documents dans la base de données.
    """
    db = next(get_db())
    documents = db.query(Document).all()
    return documents
```

---

### Explications des changements

1. **Ajout de `router` :**
   - Un objet `APIRouter` nommé `router` est défini pour enregistrer les routes.
   - Cela permet à main.py d'importer `router` et d'inclure les routes dans l'application principale.

2. **Remplacement de `app` par `router` :**
   - Toutes les définitions de routes utilisent maintenant `router` au lieu de `app`.

3. **Structure cohérente :**
   - Ce changement suit les bonnes pratiques de FastAPI pour organiser les routes dans des modules séparés.

---

### Vérification

1. **Redémarrez l'application :**
   ```bash
   uvicorn main:app --reload
   ```

2. **Testez les endpoints :**
   - Accédez à `/docs` pour vérifier que les routes de `database_endpoint` sont bien incluses sous le préfixe `/database`.

---

### Résultat attendu

L'erreur `ImportError: cannot import name 'router'` devrait être résolue, et les routes définies dans `database_endpoint.py` seront accessibles via l'application principale.

WillIsback: et pourtant si, je me demande si c'est un probleme di'import de fastapi

GitHub Copilot: Le problème vient de la confusion entre `APIRouter` et `app`. Dans votre fichier database_endpoint.py, vous avez défini un objet `app` en tant qu'instance de `APIRouter`, mais dans FastAPI, il est préférable d'utiliser un nom explicite comme `router` pour éviter toute confusion avec l'objet `app` principal de l'application.

Voici comment corriger votre fichier database_endpoint.py :

---

### Correction de database_endpoint.py

Remplacez `app = APIRouter()` par `router = APIRouter()` et mettez à jour toutes les routes pour utiliser `router` au lieu de `app`.

```python
from fastapi import APIRouter, HTTPException, Body
from clea_vectordb.src.database import get_db, Document, DocumentCreate, DocumentResponse, DocumentUpdate, add_documents, delete_document, update_document
from typing import List

# Définir un APIRouter
router = APIRouter()

@router.post("/add_document", tags=["Database"], summary="Ajouter des documents", response_model=List[DocumentResponse])
async def add_documents_endpoint(documents: List[DocumentCreate] = Body(...)):
    """
    Ajoute une liste de documents à la base de données.

    Chaque document doit contenir les champs suivants :
    - `title`: Titre du document
    - `content`: Contenu du document
    - `theme`: Thème du document
    - `document_type`: Type de document
    - `publish_date`: Date de publication (format ISO)

    Args:
        documents {(List[DocumentCreate])}: Liste des documents à ajouter.

    Returns:
        List[DocumentResponse]: Liste des documents ajoutés avec leurs IDs.
    """
    results = add_documents(documents)
    if results["errors"]:
        raise ValueError(f"Erreur lors de l'ajout des documents : {results['errors']}")
    print(f"Documents ajoutés : {results['added']}")
    return [
        {
            "id": doc["id"],
            "title": doc["title"],
            "content": doc.get("content", ""),  # Si le contenu est vide, utilisez une chaîne vide
            "theme": doc.get("theme", ""),
            "document_type": doc.get("document_type", ""),
            "publish_date": doc.get("publish_date")  # Assurez-vous que cette valeur est une date valide
        }
        for doc in results["added"]
    ]

@router.delete("/delete_document", tags=["Database"], summary="Supprimer un document", description="Supprime un document de la base de données en fonction de son ID.")
async def delete_document_endpoint(document_id: int):
    """
    Supprime un document de la base de données.

    Args:
        document_id (int): ID du document à supprimer.

    Returns:
        dict: Résultat de l'opération.
    """
    result = delete_document(document_id)
    return result

@router.put("/update_document", tags=["Database"], summary="Mettre à jour un document", response_model=DocumentResponse)
async def update_document_endpoint(payload: DocumentUpdate = Body(...)):
    """
    Met à jour un document existant dans la base de données.

    Args:
        payload (DocumentUpdate): Données pour mettre à jour le document.

    Returns:
        DocumentResponse: Document mis à jour.
    """
    results = update_document(payload)
    if "error" in results:
        raise HTTPException(status_code=400, detail=results["error"])
    print(f"Document mis à jour : {results}")
    return results

@router.get("/list_documents", tags=["Database"], summary="Lister les documents", response_model=List[DocumentResponse])
async def list_documents_endpoint():
    """
        Affiche la liste des documents dans la base de données.

        - `id (int)`: Identifiant unique du document.
        - `title (str)`: Titre du document.
        - `content (str)`: Contenu du document.
        - `theme (str)`: Thème du document.
        - `document_type (str)`: Type de document.
        - `publish_date (date)`: Date de publication du document.
        - `embedding (Text)`: Représentation vectorielle du contenu du document.
    """
    db = next(get_db())
    documents = db.query(Document).all()
    return documents
```

---

### Explications des changements

1. **Renommage de `app` en `router` :**
   - `APIRouter` est utilisé pour définir des routes dans un module spécifique. Le nom `router` est plus explicite et évite toute confusion avec l'objet `app` principal de FastAPI.

2. **Mise à jour des routes :**
   - Toutes les routes utilisent maintenant `@router.<method>` au lieu de `@app.<method>`.

3. **Import dans main.py :**
   - Dans main.py, l'import `from clea_vectordb.api.database_endpoint import router as database_router` fonctionnera correctement, car l'objet `router` est maintenant défini.

---

### Vérification

1. **Redémarrez l'application :**
   ```bash
   uvicorn main:app --reload
   ```

2. **Testez les endpoints :**
   - Accédez à `/docs` pour vérifier que les routes de `database_endpoint` sont bien incluses sous le préfixe `/database`.

---

### Résultat attendu

L'erreur `ImportError: cannot import name 'router'` sera résolue, et les routes définies dans database_endpoint.py seront accessibles via l'application principale.

WillIsback: tu veux bien m'aider à résoudre tous les avertissement du Pylance du type ```Import "sqlalchemy.orm" could not be resolvedPylancereportMissingImports``` ?

GitHub Copilot: Les avertissements de Pylance comme `Import "sqlalchemy.orm" could not be resolved` indiquent que Pylance ne parvient pas à localiser les modules Python installés dans votre environnement virtuel. Voici les étapes pour résoudre ce problème :

---

### 1. **Vérifiez que l'environnement virtuel est activé dans VS Code**
Pylance doit utiliser le bon environnement virtuel pour détecter les modules installés.

#### Étapes :
1. Ouvrez la **palette de commandes** dans VS Code (`Ctrl+Shift+P`).
2. Recherchez et sélectionnez `Python: Select Interpreter`.
3. Choisissez l'interpréteur Python correspondant à votre environnement virtuel .venv. Il devrait ressembler à ceci :
   ```
   .venv/bin/python
   ```

4. Redémarrez VS Code après avoir sélectionné l'interpréteur.

---

### 2. **Vérifiez que les dépendances sont installées**
Assurez-vous que les modules nécessaires (comme `sqlalchemy`) sont bien installés dans l'environnement virtuel.

#### Étapes :
1. Activez l'environnement virtuel dans le terminal :
   ```bash
   source .venv/bin/activate
   ```

2. Vérifiez que `sqlalchemy` est installé :
   ```bash
   pip show sqlalchemy
   ```

   Si ce n'est pas le cas, installez-le :
   ```bash
   pip install sqlalchemy
   ```

3. Synchronisez les dépendances si vous utilisez `uv` :
   ```bash
   uv sync
   ```

---

### 3. **Ajoutez les chemins des modules dans les paramètres de Pylance**
Si Pylance ne détecte toujours pas les modules, vous pouvez ajouter manuellement les chemins des modules à la configuration de Pylance.

#### Étapes :
1. Ouvrez les paramètres de VS Code (`Ctrl+,`).
2. Recherchez `Python › Analysis: Extra Paths`.
3. Ajoutez les chemins suivants (en fonction de votre structure) :
   ```json
   [
       "./clea_vectordb",
       "./clea_doc_loader",
       "./.venv/lib/python3.13/site-packages"
   ]
   ```

4. Redémarrez VS Code après avoir modifié les paramètres.

---

### 4. **Vérifiez la version de Python utilisée**
Assurez-vous que la version de Python utilisée par Pylance correspond à celle de votre environnement virtuel.

#### Étapes :
1. Dans le terminal, vérifiez la version de Python utilisée :
   ```bash
   python --version
   ```

2. Assurez-vous que cette version correspond à celle de votre environnement virtuel.

---

### 5. **Réinstallez Pylance**
Si le problème persiste, essayez de réinstaller l'extension Pylance dans VS Code.

#### Étapes :
1. Ouvrez l'onglet des extensions (`Ctrl+Shift+X`).
2. Recherchez `Pylance` et désinstallez-le.
3. Réinstallez l'extension.

---

### 6. **Désactivez temporairement les avertissements de Pylance**
Si vous voulez continuer à travailler sans être bloqué par les avertissements, vous pouvez désactiver temporairement les rapports d'erreurs pour les imports manquants.

#### Étapes :
1. Ouvrez les paramètres de VS Code (`Ctrl+,`).
2. Recherchez `Pylance › Type Checking Mode`.
3. Changez la valeur en `basic` ou `off`.

---

### Résumé des commandes à exécuter

1. Activez l'environnement virtuel :
   ```bash
   source .venv/bin/activate
   ```

2. Vérifiez les modules installés :
   ```bash
   pip show sqlalchemy
   ```

3. Synchronisez les dépendances :
   ```bash
   uv sync
   ```

4. Ajoutez les chemins des modules dans les paramètres de Pylance :
   ```json
   [
       "./clea_vectordb",
       "./clea_doc_loader",
       "./.venv/lib/python3.13/site-packages"
   ]
   ```

5. Redémarrez VS Code.

---

Si le problème persiste après ces étapes, il pourrait être utile de partager plus de détails sur votre configuration ou les messages d'erreur spécifiques.

WillIsback: le module clea_vectordb est prêt, on peut s'attaquer maintenant au module clea_doc_loader.
On va commencer par coder les codes sources qui serviront le endpoints de se module.

Commençons par le clea_doc_loader/src/text_file_loader.py qui prend un fichier parmis les fichiers compatible. Comme on travail avec une base de connaissance textuel je pense que commencer par définir qu'elle format de fichier on peut traiter en entrée est important. 
Il faut donc une structure qui accepte :
-pdf
-docx
-excel
-json
-txt
-html

à savoir que les formats particulier comme pdf et excel vont necessiter des modules d'extraction pour que la qualité de l'information soit conservé.

GitHub Copilot: Voici un début de code pour le fichier text_file_loader.py. Ce module inclut une structure pour gérer les différents formats de fichiers (PDF, DOCX, Excel, JSON, TXT, HTML) et utilise des bibliothèques spécifiques pour extraire le contenu des formats particuliers comme PDF et Excel.

```python
import os
from typing import Union, List
from PyPDF2 import PdfReader
import docx
import pandas as pd
import json
from bs4 import BeautifulSoup

class UnsupportedFileTypeError(Exception):
    """Exception levée lorsque le type de fichier n'est pas pris en charge."""
    pass

def extract_text_from_pdf(file_path: str) -> str:
    """Extrait le texte d'un fichier PDF."""
    reader = PdfReader(file_path)
    text = ""
    for page in reader.pages:
        text += page.extract_text()
    return text

def extract_text_from_docx(file_path: str) -> str:
    """Extrait le texte d'un fichier DOCX."""
    doc = docx.Document(file_path)
    return "\n".join([paragraph.text for paragraph in doc.paragraphs])

def extract_text_from_excel(file_path: str) -> str:
    """Extrait le texte d'un fichier Excel."""
    data = pd.read_excel(file_path, sheet_name=None)  # Lit toutes les feuilles
    text = ""
    for sheet_name, sheet_data in data.items():
        text += f"Sheet: {sheet_name}\n"
        text += sheet_data.to_string(index=False)
        text += "\n"
    return text

def extract_text_from_json(file_path: str) -> str:
    """Extrait le texte d'un fichier JSON."""
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return json.dumps(data, indent=4)

def extract_text_from_txt(file_path: str) -> str:
    """Extrait le texte d'un fichier TXT."""
    with open(file_path, 'r', encoding='utf-8') as f:
        return f.read()

def extract_text_from_html(file_path: str) -> str:
    """Extrait le texte d'un fichier HTML."""
    with open(file_path, 'r', encoding='utf-8') as f:
        soup = BeautifulSoup(f, 'html.parser')
    return soup.get_text()

def load_text_from_file(file_path: str) -> str:
    """
    Charge le texte d'un fichier en fonction de son type.

    Args:
        file_path (str): Chemin vers le fichier.

    Returns:
        str: Contenu textuel du fichier.

    Raises:
        UnsupportedFileTypeError: Si le type de fichier n'est pas pris en charge.
    """
    _, file_extension = os.path.splitext(file_path)
    file_extension = file_extension.lower()

    if file_extension == ".pdf":
        return extract_text_from_pdf(file_path)
    elif file_extension == ".docx":
        return extract_text_from_docx(file_path)
    elif file_extension in [".xls", ".xlsx"]:
        return extract_text_from_excel(file_path)
    elif file_extension == ".json":
        return extract_text_from_json(file_path)
    elif file_extension == ".txt":
        return extract_text_from_txt(file_path)
    elif file_extension == ".html":
        return extract_text_from_html(file_path)
    else:
        raise UnsupportedFileTypeError(f"Le type de fichier '{file_extension}' n'est pas pris en charge.")
```

---

### Explications

1. **Gestion des formats de fichiers :**
   - Chaque format de fichier (PDF, DOCX, Excel, JSON, TXT, HTML) a une fonction dédiée pour extraire le contenu textuel.
   - Les bibliothèques utilisées :
     - `PyPDF2` pour les PDF.
     - `python-docx` pour les fichiers DOCX.
     - `pandas` pour les fichiers Excel.
     - `json` pour les fichiers JSON.
     - `BeautifulSoup` pour les fichiers HTML.

2. **Structure modulaire :**
   - Chaque type de fichier a sa propre fonction d'extraction, ce qui facilite la maintenance et l'extension du code.

3. **Gestion des erreurs :**
   - Une exception personnalisée `UnsupportedFileTypeError` est levée si le type de fichier n'est pas pris en charge.

4. **Encodage :**
   - Les fichiers texte et JSON sont lus avec l'encodage UTF-8 pour garantir la compatibilité avec les caractères spéciaux.

---

### Dépendances nécessaires

Pour que ce code fonctionne, vous devez installer les bibliothèques suivantes dans votre environnement virtuel avec `uv` :

```bash
uv pip install PyPDF2 python-docx pandas beautifulsoup4 lxml
```

---

### Étapes suivantes

1. **Créer des tests unitaires :**
   - Testez chaque fonction avec des fichiers d'exemple pour vérifier que l'extraction fonctionne correctement.

2. **Intégrer ce module avec les endpoints FastAPI :**
   - Créez un endpoint dans loader.py pour permettre le téléchargement et le traitement des fichiers via l'API.

3. **Gérer les cas particuliers :**
   - Ajoutez des logs ou des mécanismes de gestion des erreurs pour les fichiers corrompus ou mal formatés.

WillIsback: parmis tous ces fichiers je dois determiner desquelles je peux extraires des métadatas ? tout en sachant que j'essaye de me rapprocher au mieux du format Document defini dans la base de donnée PostgreSQL :
```python
# Modèle pour les documents
class Document(Base):
    """
    @class Document
    @brief Modèle SQLAlchemy représentant un document dans la base de données.
    @structure:
        - id (int): Identifiant unique du document.
        - title (str): Titre du document.
        - content (str): Contenu du document.
        - theme (str): Thème du document.
        - document_type (str): Type de document.
        - publish_date (date): Date de publication du document.
        - embedding (Text): Représentation vectorielle du contenu du document.
    """
    __tablename__ = "documents"

    id = Column(Integer, primary_key=True, index=True)
    title = Column(String(255), nullable=False)
    content = Column(Text, nullable=False)
    theme = Column(String(100), nullable=False)
    document_type = Column(String(100), nullable=False)
    publish_date = Column(Date, nullable=False)
    embedding = Column(Text, nullable=True)
```


GitHub Copilot: Pour déterminer les fichiers à partir desquels vous pouvez extraire des métadonnées qui correspondent au modèle `Document`, voici une analyse des formats pris en charge et des métadonnées potentielles que vous pouvez extraire :

---

### Analyse des formats de fichiers

1. **PDF (`.pdf`)**
   - Métadonnées possibles :
     - `title`: Peut être extrait des propriétés du document (si disponibles).
     - `content`: Texte extrait des pages.
     - `document_type`: Peut être défini comme `"PDF"`.
     - `publish_date`: Peut être extrait des propriétés du document (si disponibles).
   - Extraction des métadonnées : Utilisez `PyPDF2` pour accéder aux propriétés du document.

2. **DOCX (`.docx`)**
   - Métadonnées possibles :
     - `title`: Peut être extrait des propriétés du document (si disponibles).
     - `content`: Texte extrait des paragraphes.
     - `document_type`: Peut être défini comme `"DOCX"`.
     - `publish_date`: Peut être extrait des propriétés du document (si disponibles).
   - Extraction des métadonnées : Utilisez `python-docx` pour accéder aux propriétés.

3. **Excel (`.xls`, `.xlsx`)**
   - Métadonnées possibles :
     - `title`: Peut être défini comme le nom du fichier.
     - `content`: Texte extrait des cellules (concaténé).
     - `document_type`: Peut être défini comme `"Excel"`.
     - `publish_date`: Non disponible directement.
   - Extraction des métadonnées : Utilisez `pandas` pour lire les feuilles et concaténer les données.

4. **JSON (`.json`)**
   - Métadonnées possibles :
     - `title`: Peut être défini comme le nom du fichier.
     - `content`: Contenu brut ou formaté du fichier JSON.
     - `document_type`: Peut être défini comme `"JSON"`.
     - `publish_date`: Non disponible directement.
   - Extraction des métadonnées : Utilisez `json` pour lire et formater les données.

5. **TXT (`.txt`)**
   - Métadonnées possibles :
     - `title`: Peut être défini comme le nom du fichier.
     - `content`: Contenu brut du fichier.
     - `document_type`: Peut être défini comme `"TXT"`.
     - `publish_date`: Non disponible directement.
   - Extraction des métadonnées : Lecture simple du fichier.

6. **HTML (`.html`)**
   - Métadonnées possibles :
     - `title`: Peut être extrait de la balise `<title>` (si disponible).
     - `content`: Texte brut extrait du HTML.
     - `document_type`: Peut être défini comme `"HTML"`.
     - `publish_date`: Peut être extrait des balises `<meta>` (si disponibles).
   - Extraction des métadonnées : Utilisez `BeautifulSoup` pour analyser le HTML.

---

### Implémentation pour extraire les métadonnées

Voici une version mise à jour de votre classe `file_extractor` pour extraire les métadonnées en fonction du format de fichier :

```python
import os
from PyPDF2 import PdfReader
import docx
import pandas as pd
import json
from bs4 import BeautifulSoup
from datetime import datetime


class UnsupportedFileTypeError(Exception):
    """Exception levée lorsque le type de fichier n'est pas pris en charge."""
    pass


class file_extractor:
    def __init__(self, file_path: str):
        self.file_path = file_path
        self.file_extension = os.path.splitext(file_path)[1].lower()

    def extract_metadata_from_pdf(self) -> dict:
        """Extrait les métadonnées d'un fichier PDF."""
        reader = PdfReader(self.file_path)
        info = reader.metadata
        return {
            "title": info.title if info and info.title else os.path.basename(self.file_path),
            "content": self.extract_text_from_pdf(),
            "document_type": "PDF",
            "publish_date": info.creation_date if info and info.creation_date else None,
        }

    def extract_metadata_from_docx(self) -> dict:
        """Extrait les métadonnées d'un fichier DOCX."""
        doc = docx.Document(self.file_path)
        core_properties = doc.core_properties
        return {
            "title": core_properties.title if core_properties.title else os.path.basename(self.file_path),
            "content": "\n".join([paragraph.text for paragraph in doc.paragraphs]),
            "document_type": "DOCX",
            "publish_date": core_properties.created if core_properties.created else None,
        }

    def extract_metadata_from_excel(self) -> dict:
        """Extrait les métadonnées d'un fichier Excel."""
        data = pd.read_excel(self.file_path, sheet_name=None)
        content = ""
        for sheet_name, sheet_data in data.items():
            content += f"Sheet: {sheet_name}\n"
            content += sheet_data.to_string(index=False)
            content += "\n"
        return {
            "title": os.path.basename(self.file_path),
            "content": content,
            "document_type": "Excel",
            "publish_date": None,
        }

    def extract_metadata_from_json(self) -> dict:
        """Extrait les métadonnées d'un fichier JSON."""
        with open(self.file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return {
            "title": os.path.basename(self.file_path),
            "content": json.dumps(data, indent=4),
            "document_type": "JSON",
            "publish_date": None,
        }

    def extract_metadata_from_txt(self) -> dict:
        """Extrait les métadonnées d'un fichier TXT."""
        with open(self.file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        return {
            "title": os.path.basename(self.file_path),
            "content": content,
            "document_type": "TXT",
            "publish_date": None,
        }

    def extract_metadata_from_html(self) -> dict:
        """Extrait les métadonnées d'un fichier HTML."""
        with open(self.file_path, 'r', encoding='utf-8') as f:
            soup = BeautifulSoup(f, 'html.parser')
        title = soup.title.string if soup.title else os.path.basename(self.file_path)
        return {
            "title": title,
            "content": soup.get_text(),
            "document_type": "HTML",
            "publish_date": None,  # Peut être extrait des balises <meta> si nécessaire
        }

    def extract_metadata(self) -> dict:
        """
        Extrait les métadonnées en fonction du type de fichier.

        Returns:
            dict: Métadonnées extraites.
        """
        if self.file_extension == ".pdf":
            return self.extract_metadata_from_pdf()
        elif self.file_extension == ".docx":
            return self.extract_metadata_from_docx()
        elif self.file_extension in [".xls", ".xlsx"]:
            return self.extract_metadata_from_excel()
        elif self.file_extension == ".json":
            return self.extract_metadata_from_json()
        elif self.file_extension == ".txt":
            return self.extract_metadata_from_txt()
        elif self.file_extension == ".html":
            return self.extract_metadata_from_html()
        else:
            raise UnsupportedFileTypeError(f"Le type de fichier '{self.file_extension}' n'est pas pris en charge.")
```

---

### Explications

1. **Métadonnées extraites :**
   - `title`: Nom du fichier ou titre extrait des propriétés.
   - `content`: Contenu textuel extrait.
   - `document_type`: Type de fichier (PDF, DOCX, etc.).
   - `publish_date`: Date de création ou de publication (si disponible).

2. **Gestion des formats spécifiques :**
   - Les formats PDF et DOCX utilisent leurs métadonnées intégrées.
   - Les autres formats utilisent des valeurs par défaut ou des extractions simples.

3. **Retour sous forme de dictionnaire :**
   - Les métadonnées sont renvoyées sous forme de dictionnaire, prêtes à être insérées dans la base de données.

---

### Étapes suivantes

1. **Tester chaque méthode avec des fichiers d'exemple.**
2. **Créer un endpoint FastAPI pour traiter les fichiers et renvoyer les métadonnées.**
3. **Ajouter des logs pour gérer les erreurs et les fichiers corrompus.**

WillIsback: j'ai un probleme sur la gestion de la metadata du fichier docx car le module docx semble utiliser coreproperties pour crée des metadata plutot de les lires :
 ```
     id: Mapped[int] = mapped_column(Integer, primary_key=True, index=True)
    title: Mapped[str] = mapped_column(String(255), nullable=False)
    content: Mapped[str] = mapped_column(Text, nullable=False)
    theme: Mapped[str] = mapped_column(String(100), nullable=False)
    document_type: Mapped[str] = mapped_column(String(100), nullable=False)
    publish_date: Mapped[date] = mapped_column(Date, nullable=False)
    embedding: Mapped[Optional[str]] = mapped_column(Text, nullable=True)
```


GitHub Copilot: Le problème que vous rencontrez avec les métadonnées des fichiers DOCX provient du fait que le module `python-docx` (souvent importé sous le nom `docx`) ne permet pas de lire directement les métadonnées des fichiers DOCX. La classe `core_properties` de `python-docx` est utilisée pour définir ou modifier les métadonnées, mais elle ne permet pas de lire les métadonnées existantes d'un fichier DOCX.

Pour résoudre ce problème, vous pouvez utiliser une autre bibliothèque appelée `olefile` pour lire les métadonnées des fichiers DOCX. Voici comment procéder :

---

### Solution : Utiliser `olefile` pour lire les métadonnées des fichiers DOCX

Le format DOCX est basé sur des fichiers ZIP contenant plusieurs fichiers XML. Les métadonnées sont généralement stockées dans un fichier appelé `docProps/core.xml`. Vous pouvez extraire et lire ce fichier pour obtenir les métadonnées.

#### Exemple de code pour extraire les métadonnées d'un fichier DOCX

```python
import zipfile
import os
from xml.etree import ElementTree as ET


def extract_metadata_from_docx(file_path: str) -> dict:
    """
    Extrait les métadonnées d'un fichier DOCX.

    Args:
        file_path (str): Chemin vers le fichier DOCX.

    Returns:
        dict: Métadonnées extraites.
    """
    if not zipfile.is_zipfile(file_path):
        raise ValueError(f"Le fichier {file_path} n'est pas un fichier DOCX valide.")

    metadata = {
        "title": os.path.basename(file_path),
        "content": "",
        "document_type": "DOCX",
        "publish_date": None,
    }

    try:
        with zipfile.ZipFile(file_path, 'r') as docx_zip:
            # Vérifier si le fichier contenant les métadonnées existe
            if "docProps/core.xml" in docx_zip.namelist():
                with docx_zip.open("docProps/core.xml") as core_xml:
                    tree = ET.parse(core_xml)
                    root = tree.getroot()

                    # Espaces de noms XML pour les métadonnées
                    namespaces = {
                        "cp": "http://schemas.openxmlformats.org/package/2006/metadata/core-properties",
                        "dc": "http://purl.org/dc/elements/1.1/",
                    }

                    # Extraire le titre
                    title = root.find("dc:title", namespaces)
                    if title is not None and title.text:
                        metadata["title"] = title.text

                    # Extraire la date de publication
                    created = root.find("cp:created", namespaces)
                    if created is not None and created.text:
                        metadata["publish_date"] = created.text

            # Extraire le contenu textuel du fichier DOCX
            metadata["content"] = extract_text_from_docx(file_path)

    except Exception as e:
        print(f"Erreur lors de l'extraction des métadonnées du fichier DOCX : {e}")

    return metadata


def extract_text_from_docx(file_path: str) -> str:
    """
    Extrait le texte d'un fichier DOCX.

    Args:
        file_path (str): Chemin vers le fichier DOCX.

    Returns:
        str: Contenu textuel du fichier.
    """
    import docx
    doc = docx.Document(file_path)
    return "\n".join([paragraph.text for paragraph in doc.paragraphs])
```

---

### Explications

1. **Lecture des métadonnées :**
   - Le fichier `docProps/core.xml` contient les métadonnées principales d'un fichier DOCX.
   - Les métadonnées sont extraites en utilisant `xml.etree.ElementTree` pour analyser le fichier XML.

2. **Espaces de noms XML :**
   - Les métadonnées utilisent des espaces de noms spécifiques (`cp` pour les propriétés principales et `dc` pour les métadonnées Dublin Core).
   - Ces espaces de noms doivent être spécifiés pour accéder aux éléments XML.

3. **Gestion des erreurs :**
   - Si le fichier DOCX ne contient pas de métadonnées ou si le fichier `docProps/core.xml` est manquant, des valeurs par défaut sont utilisées.

4. **Extraction du contenu textuel :**
   - Le contenu textuel est extrait séparément en utilisant `python-docx`.

---

### Exemple d'utilisation

```python
file_path = "example.docx"
metadata = extract_metadata_from_docx(file_path)
print(metadata)
```

---

### Installation des dépendances

Assurez-vous que les bibliothèques nécessaires sont installées dans votre environnement WSL avec `uv` :

```bash
uv pip install python-docx
```

---

### Résultat attendu

Pour un fichier DOCX valide, vous obtiendrez un dictionnaire contenant les métadonnées et le contenu textuel :

```json
{
    "title": "Titre du document",
    "content": "Contenu textuel extrait du fichier DOCX.",
    "document_type": "DOCX",
    "publish_date": "2025-04-30T12:34:56Z"
}
```

---

### Étapes suivantes

1. **Intégrer cette fonction dans votre classe `file_extractor`.**
2. **Tester avec différents fichiers DOCX pour vérifier la robustesse.**
3. **Gérer les cas où les métadonnées sont absentes ou incomplètes.**

WillIsback: rédige le unit test pour tester le script en prenant soin de nettoyer derriere le passage du test

GitHub Copilot: Voici un exemple de test unitaire pour le fichier text_file_loader.py. Ce test couvre les principales fonctionnalités de la classe `file_extractor` et s'assure que les fichiers temporaires créés pendant les tests sont nettoyés après leur exécution.

```python
import os
import pytest
from clea_doc_loader.src.text_file_loader import file_extractor, UnsupportedFileTypeError

# Crée un répertoire temporaire pour les tests
@pytest.fixture
def temp_dir(tmp_path):
    return tmp_path

def create_temp_file(temp_dir, filename, content):
    """Crée un fichier temporaire pour les tests."""
    file_path = temp_dir / filename
    with open(file_path, "w", encoding="utf-8") as f:
        f.write(content)
    return file_path

def test_extract_text_from_txt(temp_dir):
    """Test pour extraire le texte d'un fichier TXT."""
    file_path = create_temp_file(temp_dir, "test.txt", "Ceci est un test.")
    extractor = file_extractor(str(file_path))
    assert extractor.extract_text_from_txt() == "Ceci est un test."

def test_extract_text_from_json(temp_dir):
    """Test pour extraire le texte d'un fichier JSON."""
    file_path = create_temp_file(temp_dir, "test.json", '{"key": "value"}')
    extractor = file_extractor(str(file_path))
    assert extractor.extract_text_from_json() == '{\n    "key": "value"\n}'

def test_extract_text_from_html(temp_dir):
    """Test pour extraire le texte d'un fichier HTML."""
    html_content = "<html><head><title>Test</title></head><body>Bonjour</body></html>"
    file_path = create_temp_file(temp_dir, "test.html", html_content)
    extractor = file_extractor(str(file_path))
    assert extractor.extract_text_from_html() == "Test\nBonjour"

def test_extract_text_from_unsupported_file(temp_dir):
    """Test pour vérifier le comportement avec un type de fichier non pris en charge."""
    file_path = create_temp_file(temp_dir, "test.unsupported", "Contenu non pris en charge.")
    extractor = file_extractor(str(file_path))
    with pytest.raises(UnsupportedFileTypeError):
        extractor.extract_metadata()

def test_extract_metadata_from_txt(temp_dir):
    """Test pour extraire les métadonnées d'un fichier TXT."""
    file_path = create_temp_file(temp_dir, "test.txt", "Ceci est un test.")
    extractor = file_extractor(str(file_path))
    metadata = extractor.extract_metadata_from_txt()
    assert metadata["title"] == "test.txt"
    assert metadata["content"] == "Ceci est un test."
    assert metadata["document_type"] == "TXT"
    assert metadata["publish_date"] is None

def test_extract_metadata_from_json(temp_dir):
    """Test pour extraire les métadonnées d'un fichier JSON."""
    file_path = create_temp_file(temp_dir, "test.json", '{"key": "value"}')
    extractor = file_extractor(str(file_path))
    metadata = extractor.extract_metadata_from_json()
    assert metadata["title"] == "test.json"
    assert metadata["content"] == '{\n    "key": "value"\n}'
    assert metadata["document_type"] == "JSON"
    assert metadata["publish_date"] is None

def test_cleanup(temp_dir):
    """Test pour vérifier que les fichiers temporaires sont supprimés après les tests."""
    file_path = create_temp_file(temp_dir, "test_cleanup.txt", "Ceci est un test.")
    assert os.path.exists(file_path)
    os.remove(file_path)
    assert not os.path.exists(file_path)
```

---

### Explications

1. **Utilisation de `pytest` :**
   - La bibliothèque `pytest` est utilisée pour gérer les tests unitaires.
   - Le décorateur `@pytest.fixture` crée un répertoire temporaire pour les tests.

2. **Création de fichiers temporaires :**
   - La fonction `create_temp_file` crée des fichiers temporaires dans le répertoire temporaire fourni par `pytest`.

3. **Tests des fonctionnalités principales :**
   - Chaque méthode de la classe `file_extractor` est testée avec un fichier correspondant.
   - Les tests vérifient que le contenu ou les métadonnées extraites correspondent aux attentes.

4. **Gestion des fichiers non pris en charge :**
   - Un test vérifie que l'exception `UnsupportedFileTypeError` est levée pour les fichiers avec une extension non prise en charge.

5. **Nettoyage des fichiers temporaires :**
   - Le test `test_cleanup` s'assure que les fichiers temporaires sont supprimés après les tests.

---

### Commande pour exécuter les tests

Pour exécuter les tests dans un environnement WSL avec OpenSUSE Tumbleweed, utilisez la commande suivante :

```bash
uv pip install pytest
pytest clea_doc_loader/test/src/test_text_file_loader.py
```

---

### Résultat attendu

Tous les tests doivent passer avec succès, et les fichiers temporaires doivent être supprimés après l'exécution des tests.

WillIsback: PyPDF2 is deprecated. Please move to the pypdf library instead

GitHub Copilot: To address the deprecation of `PyPDF2`, you can migrate to the `pypdf` library, which is its modern replacement. Below is the updated text_file_loader.py file with `pypdf` replacing `PyPDF2`. Additionally, I'll ensure the code is compatible with your WSL OpenSUSE Tumbleweed environment and uses `uv` for dependency management.

---

### Updated text_file_loader.py

```python
import os
from pypdf import PdfReader
from docx import Document
import zipfile
from xml.etree import ElementTree as ET
import pandas as pd
import json
from bs4 import BeautifulSoup


class UnsupportedFileTypeError(Exception):
    """Exception levée lorsque le type de fichier n'est pas pris en charge."""
    pass


class file_extractor:
    def __init__(self, file_path: str):
        self.file_path = file_path
        self.file_extension = os.path.splitext(file_path)[1].lower()

    def extract_text_from_pdf(self) -> str:
        """Extrait le texte d'un fichier PDF."""
        reader = PdfReader(self.file_path)
        text = ""
        for page in reader.pages:
            text += page.extract_text()
        return text

    def extract_text_from_docx(self) -> str:
        """Extrait le texte d'un fichier DOCX."""
        doc = Document(self.file_path)
        return "\n".join([paragraph.text for paragraph in doc.paragraphs])

    def extract_text_from_excel(self) -> str:
        """Extrait le texte d'un fichier Excel."""
        data = pd.read_excel(self.file_path, sheet_name=None)  # Lit toutes les feuilles
        text = ""
        for sheet_name, sheet_data in data.items():
            text += f"Sheet: {sheet_name}\n"
            text += sheet_data.to_string(index=False)
            text += "\n"
        return text

    def extract_text_from_json(self) -> str:
        """Extrait le texte d'un fichier JSON."""
        with open(self.file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        return json.dumps(data, indent=4)

    def extract_text_from_txt(self) -> str:
        """Extrait le texte d'un fichier TXT."""
        with open(self.file_path, "r", encoding="utf-8") as f:
            return f.read()

    def extract_text_from_html(self) -> str:
        """Extrait le texte d'un fichier HTML."""
        with open(self.file_path, "r", encoding="utf-8") as f:
            soup = BeautifulSoup(f, "html.parser")
        return soup.get_text()

    def extract_metadata_from_pdf(self) -> dict:
        """Extrait les métadonnées d'un fichier PDF."""
        reader = PdfReader(self.file_path)
        info = reader.metadata
        return {
            "title": info.get("/Title", os.path.basename(self.file_path)),
            "content": self.extract_text_from_pdf(),
            "document_type": "PDF",
            "publish_date": info.get("/CreationDate", None),
        }

    def extract_metadata_from_docx(self) -> dict:
        """Extrait les métadonnées d'un fichier DOCX."""
        if not zipfile.is_zipfile(self.file_path):
            raise ValueError(f"Le fichier {self.file_path} n'est pas un fichier DOCX valide.")

        metadata = {
            "title": os.path.basename(self.file_path),
            "content": "",
            "document_type": "DOCX",
            "publish_date": None,
        }

        try:
            with zipfile.ZipFile(self.file_path, "r") as docx_zip:
                if "docProps/core.xml" in docx_zip.namelist():
                    with docx_zip.open("docProps/core.xml") as core_xml:
                        tree = ET.parse(core_xml)
                        root = tree.getroot()
                        namespaces = {
                            "cp": "http://schemas.openxmlformats.org/package/2006/metadata/core-properties",
                            "dc": "http://purl.org/dc/elements/1.1/",
                        }
                        title = root.find("dc:title", namespaces)
                        if title is not None and title.text:
                            metadata["title"] = title.text
                        created = root.find("cp:created", namespaces)
                        if created is not None and created.text:
                            metadata["publish_date"] = created.text
            metadata["content"] = self.extract_text_from_docx()
        except Exception as e:
            print(f"Erreur lors de l'extraction des métadonnées du fichier DOCX : {e}")

        return metadata

    def extract_metadata(self) -> dict:
        """Extrait les métadonnées en fonction du type de fichier."""
        if self.file_extension == ".pdf":
            return self.extract_metadata_from_pdf()
        elif self.file_extension == ".docx":
            return self.extract_metadata_from_docx()
        elif self.file_extension in [".xls", ".xlsx"]:
            return self.extract_text_from_excel()
        elif self.file_extension == ".json":
            return self.extract_text_from_json()
        elif self.file_extension == ".txt":
            return self.extract_text_from_txt()
        elif self.file_extension == ".html":
            return self.extract_text_from_html()
        else:
            raise UnsupportedFileTypeError(f"Le type de fichier '{self.file_extension}' n'est pas pris en charge.")
```

---

### Key Changes

1. **Replaced `PyPDF2` with `pypdf`:**
   - `PdfReader` from `pypdf` is used to extract text and metadata from PDF files.
   - Metadata extraction uses the `metadata` attribute of `PdfReader`.

2. **Improved Metadata Handling:**
   - For PDFs, metadata like `/Title` and `/CreationDate` are extracted.
   - For DOCX files, metadata is extracted from `docProps/core.xml` using `xml.etree.ElementTree`.

3. **Error Handling:**
   - Added checks for invalid DOCX files using `zipfile.is_zipfile`.

---

### Dependencies

Install the required dependencies using `uv`:

```bash
uv pip install pypdf python-docx pandas beautifulsoup4 lxml
```

---

### Testing

Run the unit tests to ensure the updated code works as expected:

```bash
pytest clea_doc_loader/test/src/test_text_file_loader.py
```

This ensures compatibility with your WSL OpenSUSE Tumbleweed environment and validates the functionality of the updated text_file_loader.py.

WillIsback: alors je sais pas si en html il a retour à la ligne mais il semble que la mise en forme du text ne soit pas conservé par l'extraction de Beautiful soup :
```log
clea_doc_loader/test/src/test_text_file_loader.py ..F....                                                            [100%]

========================================================= FAILURES =========================================================
_______________________________________________ test_extract_text_from_html ________________________________________________

temp_dir = PosixPath('/tmp/pytest-of-william/pytest-3/test_extract_text_from_html0')

    def test_extract_text_from_html(temp_dir):
        """Test pour extraire le texte d'un fichier HTML."""
        html_content = (
            "<html><head><title>Test</title></head></br><body>Bonjour</body></html>"
        )
        file_path = create_temp_file(temp_dir, "test.html", html_content)
        extractor = file_extractor(str(file_path))
>       assert extractor.extract_text_from_html() == "Test\nBonjour"
E       AssertionError: assert 'TestBonjour' == 'Test\nBonjour'
E
E         - Test
E         - Bonjour
E         + TestBonjour
E         ? ++++

clea_doc_loader/test/src/test_text_file_loader.py:44: AssertionError
================================================= short test summary info ==================================================
FAILED clea_doc_loader/test/src/test_text_file_loader.py::test_extract_text_from_html - AssertionError: assert 'TestBonjour' == 'Test\nBonjour'
=============================================== 1 failed, 6 passed in 0.66s ================================================
```


GitHub Copilot: Le problème vient du fait que `BeautifulSoup.get_text()` ne conserve pas les retours à la ligne ou les espaces entre les balises HTML. Par défaut, il concatène tout le texte en une seule chaîne. Pour résoudre ce problème, vous pouvez utiliser l'argument `separator` de la méthode `get_text()` pour spécifier un séparateur, comme un retour à la ligne (`\n`).

Voici la correction à apporter dans la méthode `extract_text_from_html` de votre fichier `text_file_loader.py` :

---

### Correction dans `text_file_loader.py`

```python
def extract_text_from_html(self) -> str:
    """Extrait le texte d'un fichier HTML."""
    with open(self.file_path, "r", encoding="utf-8") as f:
        soup = BeautifulSoup(f, "html.parser")
    return soup.get_text(separator="\n")  # Utilise '\n' comme séparateur pour conserver les retours à la ligne
```

---

### Explications

1. **`separator="\n"` :**
   - Cet argument indique à `BeautifulSoup.get_text()` d'insérer un retour à la ligne (`\n`) entre les blocs de texte séparés par des balises HTML.

2. **Effet attendu :**
   - Le texte extrait conservera les retours à la ligne, ce qui correspondra mieux à la mise en forme du HTML.

---

### Mise à jour du test `test_extract_text_from_html`

Assurez-vous que le test utilise un HTML valide et que le contenu attendu inclut les retours à la ligne :

```python
def test_extract_text_from_html(temp_dir):
    """Test pour extraire le texte d'un fichier HTML."""
    html_content = "<html><head><title>Test</title></head><body>Bonjour</body></html>"
    file_path = create_temp_file(temp_dir, "test.html", html_content)
    extractor = file_extractor(str(file_path))
    assert extractor.extract_text_from_html() == "Test\nBonjour"
```

---

### Résultat attendu après correction

Lorsque vous exécutez les tests avec `pytest`, le test `test_extract_text_from_html` devrait maintenant passer avec succès :

```bash
pytest clea_doc_loader/test/src/test_text_file_loader.py
```

Sortie attendue :

```log
================================================= test session starts ==================================================
collected 7 items                                                                                                     

clea_doc_loader/test/src/test_text_file_loader.py .......                                                              [100%]

================================================== 7 passed in 0.65s ==================================================
```

---

### Commande pour installer BeautifulSoup (si nécessaire)

Si BeautifulSoup n'est pas encore installé dans votre environnement, utilisez `uv` pour l'installer :

```bash
uv pip install beautifulsoup4 lxml
```

---

Avec cette correction, votre méthode `extract_text_from_html` conservera les retours à la ligne, et le test unitaire validera correctement cette fonctionnalité.

WillIsback: comme tu l'as compris notre base de donnée structure et classe des documents à partir de texte non structuré.

Il est donc impératif qu'avant l'ajout des dites-documents tout les champs obligatoires soit renseigné :
```
title: Titre du document
content: Contenu du document
theme: Thème du document
document_type: Type de document
publish_date: Date de publication (format ISO)
```

Si à l'extraction du document l'un de ses champs n'est pas renseigné alors il faut le flag d'une manière pour l'initialisé avec des valeurs par défauts.


GitHub Copilot: Pour garantir que tous les champs obligatoires sont renseignés avant l'ajout des documents dans la base de données, nous pouvons modifier la classe `file_extractor` pour inclure une méthode qui vérifie et initialise les champs manquants avec des valeurs par défaut. Voici une mise à jour de votre code pour inclure cette fonctionnalité.

---

### Mise à jour de text_file_loader.py

```python
import os
from pypdf import PdfReader
from docx import Document
import zipfile
from xml.etree import ElementTree as ET
import pandas as pd
import json
from bs4 import BeautifulSoup
from datetime import datetime


class UnsupportedFileTypeError(Exception):
    """Exception levée lorsque le type de fichier n'est pas pris en charge."""
    pass


class file_extractor:
    def __init__(self, file_path: str):
        self.file_path = file_path
        self.file_extension = os.path.splitext(file_path)[1].lower()

    def extract_text_from_pdf(self) -> str:
        """Extrait le texte d'un fichier PDF."""
        reader = PdfReader(self.file_path)
        text = ""
        for page in reader.pages:
            text += page.extract_text()
        return text

    def extract_text_from_docx(self) -> str:
        """Extrait le texte d'un fichier DOCX."""
        doc = Document(self.file_path)
        return "\n".join([paragraph.text for paragraph in doc.paragraphs])

    def extract_text_from_excel(self) -> str:
        """Extrait le texte d'un fichier Excel."""
        data = pd.read_excel(self.file_path, sheet_name=None)  # Lit toutes les feuilles
        text = ""
        for sheet_name, sheet_data in data.items():
            text += f"Sheet: {sheet_name}\n"
            text += sheet_data.to_string(index=False)
            text += "\n"
        return text

    def extract_text_from_json(self) -> str:
        """Extrait le texte d'un fichier JSON."""
        with open(self.file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        return json.dumps(data, indent=4)

    def extract_text_from_txt(self) -> str:
        """Extrait le texte d'un fichier TXT."""
        with open(self.file_path, "r", encoding="utf-8") as f:
            return f.read()

    def extract_text_from_html(self) -> str:
        """Extrait le texte d'un fichier HTML."""
        with open(self.file_path, "r", encoding="utf-8") as f:
            soup = BeautifulSoup(f, "html.parser")
        return soup.get_text(separator="\n")

    def extract_metadata_from_pdf(self) -> dict:
        """Extrait les métadonnées d'un fichier PDF."""
        reader = PdfReader(self.file_path)
        info = reader.metadata
        return {
            "title": info.get("/Title", os.path.basename(self.file_path)),
            "content": self.extract_text_from_pdf(),
            "document_type": "PDF",
            "publish_date": info.get("/CreationDate", None),
        }

    def extract_metadata_from_docx(self) -> dict:
        """Extrait les métadonnées d'un fichier DOCX."""
        if not zipfile.is_zipfile(self.file_path):
            raise ValueError(f"Le fichier {self.file_path} n'est pas un fichier DOCX valide.")

        metadata = {
            "title": os.path.basename(self.file_path),
            "content": "",
            "document_type": "DOCX",
            "publish_date": None,
        }

        try:
            with zipfile.ZipFile(self.file_path, "r") as docx_zip:
                if "docProps/core.xml" in docx_zip.namelist():
                    with docx_zip.open("docProps/core.xml") as core_xml:
                        tree = ET.parse(core_xml)
                        root = tree.getroot()
                        namespaces = {
                            "cp": "http://schemas.openxmlformats.org/package/2006/metadata/core-properties",
                            "dc": "http://purl.org/dc/elements/1.1/",
                        }
                        title = root.find("dc:title", namespaces)
                        if title is not None and title.text:
                            metadata["title"] = title.text
                        created = root.find("cp:created", namespaces)
                        if created is not None and created.text:
                            metadata["publish_date"] = created.text
            metadata["content"] = self.extract_text_from_docx()
        except Exception as e:
            print(f"Erreur lors de l'extraction des métadonnées du fichier DOCX : {e}")

        return metadata

    def validate_and_initialize_metadata(self, metadata: dict) -> dict:
        """
        Valide et initialise les champs manquants des métadonnées avec des valeurs par défaut.

        Args:
            metadata (dict): Métadonnées extraites.

        Returns:
            dict: Métadonnées avec les champs obligatoires initialisés.
        """
        defaults = {
            "title": "Titre inconnu",
            "content": "Contenu non disponible",
            "theme": "Thème générique",
            "document_type": "Type inconnu",
            "publish_date": datetime.now().isoformat(),
        }

        for key, default_value in defaults.items():
            if not metadata.get(key):
                metadata[key] = default_value

        return metadata

    def extract_metadata(self) -> dict:
        """Extrait les métadonnées en fonction du type de fichier."""
        if self.file_extension == ".pdf":
            metadata = self.extract_metadata_from_pdf()
        elif self.file_extension == ".docx":
            metadata = self.extract_metadata_from_docx()
        elif self.file_extension in [".xls", ".xlsx"]:
            metadata = self.extract_metadata_from_excel()
        elif self.file_extension == ".json":
            metadata = self.extract_metadata_from_json()
        elif self.file_extension == ".txt":
            metadata = self.extract_metadata_from_txt()
        elif self.file_extension == ".html":
            metadata = self.extract_metadata_from_html()
        else:
            raise UnsupportedFileTypeError(f"Le type de fichier '{self.file_extension}' n'est pas pris en charge.")

        # Valider et initialiser les champs manquants
        return self.validate_and_initialize_metadata(metadata)
```

---

### Explications des modifications

1. **Nouvelle méthode `validate_and_initialize_metadata`:**
   - Cette méthode vérifie si les champs obligatoires sont présents dans les métadonnées.
   - Si un champ est manquant ou vide, il est initialisé avec une valeur par défaut.

2. **Valeurs par défaut :**
   - `title`: `"Titre inconnu"`
   - `content`: `"Contenu non disponible"`
   - `theme`: `"Thème générique"`
   - `document_type`: `"Type inconnu"`
   - `publish_date`: Date et heure actuelles au format ISO.

3. **Intégration dans `extract_metadata`:**
   - Après l'extraction des métadonnées spécifiques au type de fichier, la méthode `validate_and_initialize_metadata` est appelée pour s'assurer que tous les champs obligatoires sont renseignés.

---

### Étapes suivantes

1. **Tester la méthode `validate_and_initialize_metadata`:**
   - Ajoutez des tests unitaires pour vérifier que les champs manquants sont correctement initialisés.

2. **Installer les dépendances nécessaires :**
   ```bash
   uv pip install pypdf python-docx pandas beautifulsoup4 lxml
   ```

3. **Exécuter les tests :**
   ```bash
   pytest clea_doc_loader/test/src/test_text_file_loader.py
   ```

Avec cette mise à jour, tous les documents auront des métadonnées complètes avant d'être ajoutés à la base de données.

WillIsback: enfaite il va falloir fussioner extract_text et extract metadata pour avoir pour chaque type de fichier une fonction extract data qui se charge d'extraire toutes les données possibles en conformité des attentes : 
```python
# Modèle pour les documents
class Document(Base):
    """
    @class Document
    @brief Modèle SQLAlchemy représentant un document dans la base de données.
    @structure:
        - id (int): Identifiant unique du document.
        - title (str): Titre du document.
        - content (str): Contenu du document.
        - theme (str): Thème du document.
        - document_type (str): Type de document.
        - publish_date (date): Date de publication du document.
        - embedding (Text): Représentation vectorielle du contenu du document.
    """

    __tablename__ = "documents"

    id: Mapped[int] = mapped_column(Integer, primary_key=True, index=True)
    title: Mapped[str] = mapped_column(String(255), nullable=False)
    content: Mapped[str] = mapped_column(Text, nullable=False)
    theme: Mapped[str] = mapped_column(String(100), nullable=False)
    document_type: Mapped[str] = mapped_column(String(100), nullable=False)
    publish_date: Mapped[date] = mapped_column(Date, nullable=False)
    embedding: Mapped[Optional[str]] = mapped_column(Text, nullable=True)
    ```
    

GitHub Copilot: Voici une version mise à jour de la classe `file_extractor` qui fusionne les méthodes `extract_text` et `extract_metadata` en une seule méthode `extract_data`. Cette méthode extrait toutes les données nécessaires pour être en conformité avec le modèle `Document` défini dans votre base de données.

---

### Mise à jour de `file_extractor` avec `extract_data`

```python
import os
from pypdf import PdfReader
from docx import Document
import zipfile
from xml.etree import ElementTree as ET
import pandas as pd
import json
from bs4 import BeautifulSoup
from datetime import datetime


class UnsupportedFileTypeError(Exception):
    """Exception levée lorsque le type de fichier n'est pas pris en charge."""
    pass


class file_extractor:
    def __init__(self, file_path: str):
        self.file_path = file_path
        self.file_extension = os.path.splitext(file_path)[1].lower()

    def extract_data_from_pdf(self) -> dict:
        """Extrait les données d'un fichier PDF."""
        reader = PdfReader(self.file_path)
        info = reader.metadata
        text = ""
        for page in reader.pages:
            text += page.extract_text()

        return {
            "title": info.get("/Title", os.path.basename(self.file_path)),
            "content": text,
            "theme": "Thème générique",  # Valeur par défaut
            "document_type": "PDF",
            "publish_date": info.get("/CreationDate", datetime.now().isoformat()),
        }

    def extract_data_from_docx(self) -> dict:
        """Extrait les données d'un fichier DOCX."""
        if not zipfile.is_zipfile(self.file_path):
            raise ValueError(f"Le fichier {self.file_path} n'est pas un fichier DOCX valide.")

        metadata = {
            "title": os.path.basename(self.file_path),
            "content": "",
            "theme": "Thème générique",  # Valeur par défaut
            "document_type": "DOCX",
            "publish_date": datetime.now().isoformat(),  # Valeur par défaut
        }

        try:
            with zipfile.ZipFile(self.file_path, "r") as docx_zip:
                if "docProps/core.xml" in docx_zip.namelist():
                    with docx_zip.open("docProps/core.xml") as core_xml:
                        tree = ET.parse(core_xml)
                        root = tree.getroot()
                        namespaces = {
                            "cp": "http://schemas.openxmlformats.org/package/2006/metadata/core-properties",
                            "dc": "http://purl.org/dc/elements/1.1/",
                        }
                        title = root.find("dc:title", namespaces)
                        if title is not None and title.text:
                            metadata["title"] = title.text
                        created = root.find("cp:created", namespaces)
                        if created is not None and created.text:
                            metadata["publish_date"] = created.text
            metadata["content"] = self.extract_text_from_docx()
        except Exception as e:
            print(f"Erreur lors de l'extraction des métadonnées du fichier DOCX : {e}")

        return metadata

    def extract_data_from_excel(self) -> dict:
        """Extrait les données d'un fichier Excel."""
        data = pd.read_excel(self.file_path, sheet_name=None)
        content = ""
        for sheet_name, sheet_data in data.items():
            content += f"Sheet: {sheet_name}\n"
            content += sheet_data.to_string(index=False)
            content += "\n"

        return {
            "title": os.path.basename(self.file_path),
            "content": content,
            "theme": "Thème générique",  # Valeur par défaut
            "document_type": "Excel",
            "publish_date": datetime.now().isoformat(),  # Valeur par défaut
        }

    def extract_data_from_json(self) -> dict:
        """Extrait les données d'un fichier JSON."""
        with open(self.file_path, "r", encoding="utf-8") as f:
            data = json.load(f)

        return {
            "title": os.path.basename(self.file_path),
            "content": json.dumps(data, indent=4),
            "theme": "Thème générique",  # Valeur par défaut
            "document_type": "JSON",
            "publish_date": datetime.now().isoformat(),  # Valeur par défaut
        }

    def extract_data_from_txt(self) -> dict:
        """Extrait les données d'un fichier TXT."""
        with open(self.file_path, "r", encoding="utf-8") as f:
            content = f.read()

        return {
            "title": os.path.basename(self.file_path),
            "content": content,
            "theme": "Thème générique",  # Valeur par défaut
            "document_type": "TXT",
            "publish_date": datetime.now().isoformat(),  # Valeur par défaut
        }

    def extract_data_from_html(self) -> dict:
        """Extrait les données d'un fichier HTML."""
        with open(self.file_path, "r", encoding="utf-8") as f:
            soup = BeautifulSoup(f, "html.parser")
        title = soup.title.string if soup.title else os.path.basename(self.file_path)

        return {
            "title": title,
            "content": soup.get_text(separator="\n"),
            "theme": "Thème générique",  # Valeur par défaut
            "document_type": "HTML",
            "publish_date": datetime.now().isoformat(),  # Valeur par défaut
        }

    def extract_data(self) -> dict:
        """Extrait les données en fonction du type de fichier."""
        if self.file_extension == ".pdf":
            return self.extract_data_from_pdf()
        elif self.file_extension == ".docx":
            return self.extract_data_from_docx()
        elif self.file_extension in [".xls", ".xlsx"]:
            return self.extract_data_from_excel()
        elif self.file_extension == ".json":
            return self.extract_data_from_json()
        elif self.file_extension == ".txt":
            return self.extract_data_from_txt()
        elif self.file_extension == ".html":
            return self.extract_data_from_html()
        else:
            raise UnsupportedFileTypeError(
                f"Le type de fichier '{self.file_extension}' n'est pas pris en charge."
            )
```

---

### Explications des modifications

1. **Fusion des méthodes :**
   - Les méthodes `extract_text` et `extract_metadata` ont été fusionnées en une seule méthode `extract_data` pour chaque type de fichier.
   - Chaque méthode retourne un dictionnaire contenant tous les champs nécessaires pour le modèle `Document`.

2. **Valeurs par défaut :**
   - Les champs obligatoires (`title`, `content`, `theme`, `document_type`, `publish_date`) sont initialisés avec des valeurs par défaut si les données ne sont pas disponibles.

3. **Gestion des types de fichiers :**
   - La méthode `extract_data` appelle la méthode spécifique au type de fichier en fonction de l'extension.

4. **Compatibilité avec le modèle `Document` :**
   - Les données extraites sont conformes à la structure du modèle `Document`.

---

### Étapes suivantes

1. **Installer les dépendances nécessaires :**
   ```bash
   uv pip install pypdf python-docx pandas beautifulsoup4 lxml
   ```

2. **Tester la méthode `extract_data` :**
   - Ajoutez des tests unitaires pour vérifier que les données extraites sont conformes au modèle `Document`.

3. **Intégrer cette classe dans votre pipeline de traitement des documents.**

Avec cette mise à jour, la classe `file_extractor` est prête à extraire toutes les données nécessaires pour structurer et classer les documents dans votre base de données.

WillIsback: basé sur les exemple de json_extractor et pdf_extractor aide moi à rédiger html_extractor et txt_extractor :


GitHub Copilot: Voici les versions mises à jour de `HtmlExtractor` et `TxtExtractor`, basées sur les exemples de `JsonExtractor` et `PdfExtractor`. Ces classes respectent la structure et les conventions des autres extracteurs.

---

### `HtmlExtractor`

```python
from pathlib import Path
from datetime import date
from typing import Iterator
from bs4 import BeautifulSoup
from data_extractor import ExtractedDocument
from base import BaseExtractor


class HtmlExtractor(BaseExtractor):
    def __init__(self, file_path: str):
        super().__init__(file_path)
        self.file_path = Path(file_path)
        try:
            with open(self.file_path, "r", encoding="utf-8") as f:
                self.soup = BeautifulSoup(f, "html.parser")
        except Exception as e:
            raise ValueError(f"Erreur lors de la lecture du fichier HTML : {e}")

    def extract_many(self, max_length: int = 1000) -> Iterator[ExtractedDocument]:
        content = self.soup.get_text(separator="\n").strip()
        if not content:
            return

        title = self.soup.title.string if self.soup.title else self.file_path.name
        for i, chunk in enumerate(self._split_text(content, max_length)):
            yield ExtractedDocument(
                title=f"{title} (part {i + 1})",
                content=chunk,
                theme="Thème générique",
                document_type="HTML",
                publish_date=date.today(),
                embedding=None,
            )

    def _split_text(self, text: str, max_length: int) -> list[str]:
        from textwrap import wrap

        return wrap(text, max_length, break_long_words=False, replace_whitespace=False)
```

---

### `TxtExtractor`

```python
from pathlib import Path
from datetime import date
from typing import Iterator
from data_extractor import ExtractedDocument
from base import BaseExtractor


class TxtExtractor(BaseExtractor):
    def __init__(self, file_path: str):
        super().__init__(file_path)
        self.file_path = Path(file_path)
        try:
            with open(self.file_path, "r", encoding="utf-8") as f:
                self.content = f.read()
        except Exception as e:
            raise ValueError(f"Erreur lors de la lecture du fichier TXT : {e}")

# filepath: /home/william/projet/Clea/clea-api/clea_doc_loader/src/data_extractor/txt_extractor.py
from pathlib import Path
from datetime import date
from typing import Iterator
from data_extractor import ExtractedDocument
from base import BaseExtractor


class TxtExtractor(BaseExtractor):
    def __init__(self, file_path: str):
        super().__init__(file_path)
        self.file_path = Path(file_path)
        try:
            with open(self.file_path, "r", encoding="utf-8") as f:
                self.content = f.read()
        except Exception as e:
            raise ValueError(f"Erreur lors de la lecture du fichier TXT : {e}")



WillIsback: ```log
============================================================================================ ERRORS =============================================================================================
______________________________________________________________ ERROR collecting clea_doc_loader/test/src/test_text_file_loader.py _______________________________________________________________
ImportError while importing test module '/home/william/projet/Clea/clea-api/clea_doc_loader/test/src/test_text_file_loader.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
/usr/lib64/python3.13/importlib/__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
clea_doc_loader/test/src/test_text_file_loader.py:5: in <module>
    from clea_doc_loader.src import DocsLoader, ExtractedDocument, UnsupportedFileTypeError
clea_doc_loader/src/__init__.py:1: in <module>
    from data_extractor import (
E   ModuleNotFoundError: No module named 'data_extractor'
```

Je ne comprends pas où ça coince je pense avoir bien exposé sur les plusieurs niveau. Voici la structure :
```
      ~/projet/Clea/clea-api/clea_doc_loader  tree                                                                                                                           ✔  clea-api   
.
├── api
│   ├── __init__.py
│   └── loader.py
├── docs
├── src
│   ├── data_extractor
│   │   ├── base.py
│   │   ├── docx_extractor.py
│   │   ├── html_extractor.py
│   │   ├── __init__.py
│   │   ├── json_extractor.py
│   │   ├── pdf_extractor.py
│   │   └── txt_extractor.py
│   ├── docs_loader.py
│   ├── extractor_factory.py
│   ├── __init__.py
│   └── __pycache__
│       └── __init__.cpython-313.pyc
├── static
└── test
    └── src
        ├── __pycache__
        │   └── test_text_file_loader.cpython-313-pytest-8.3.5.pyc
        └── test_text_file_loader.py

10 directories, 15 files
```


GitHub Copilot: Le problème vient du fait que Python ne trouve pas le module `data_extractor` parce que le répertoire `src/data_extractor` n'est pas inclus dans le `PYTHONPATH`. Par défaut, Python ne considère pas les sous-dossiers comme des modules à moins qu'ils soient explicitement ajoutés au chemin d'importation.

Voici comment résoudre ce problème :

---

### 1. **Vérifiez les imports dans `clea_doc_loader/src/__init

WillIsback: ```log
============================================================================================ ERRORS =============================================================================================
______________________________________________________________ ERROR collecting clea_doc_loader/test/src/test_text_file_loader.py _______________________________________________________________
ImportError while importing test module '/home/william/projet/Clea/clea-api/clea_doc_loader/test/src/test_text_file_loader.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
/usr/lib64/python3.13/importlib/__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
clea_doc_loader/test/src/test_text_file_loader.py:5: in <module>
    from clea_doc_loader.src import DocsLoader, ExtractedDocument, UnsupportedFileTypeError
clea_doc_loader/src/__init__.py:1: in <module>
    from data_extractor import (
E   ModuleNotFoundError: No module named 'data_extractor'
```

Je ne comprends pas où ça coince je pense avoir bien exposé sur les plusieurs niveau. Voici la structure :
```
      ~/projet/Clea/clea-api/clea_doc_loader  tree                                                                                                                           ✔  clea-api   
.
├── api
│   ├── __init__.py
│   └── loader.py
├── docs
├── src
│   ├── data_extractor
│   │   ├── base.py
│   │   ├── docx_extractor.py
│   │   ├── html_extractor.py
│   │   ├── __init__.py
│   │   ├── json_extractor.py
│   │   ├── pdf_extractor.py
│   │   └── txt_extractor.py
│   ├── docs_loader.py
│   ├── extractor_factory.py
│   ├── __init__.py
│   └── __pycache__
│       └── __init__.cpython-313.pyc
├── static
└── test
    └── src
        ├── __pycache__
        │   └── test_text_file_loader.cpython-313-pytest-8.3.5.pyc
        └── test_text_file_loader.py

10 directories, 15 files
```


GitHub Copilot: Enable o4-mini (Preview) for all clients
Enable access to the latest o4-mini model from OpenAI. [Learn more about how GitHub Copilot serves o4-mini](https://docs.github.com/en/copilot/using-github-copilot/ai-models/using-openai-o4-mini-in-github-copilot).

WillIsback: @GitHubCopilot Enable: "Enable o4-mini (Preview) for all clients"

GitHub Copilot: Pour que vos modules soient correctement importables, il faut :

1. Marquer clea_doc_loader comme package en ajoutant un `__init__.py` à sa racine.  
2. Corriger tous les imports dans `src/` et `src/data_extractor/` pour utiliser des imports relatifs.  

````python
# Ce fichier vide rend clea_doc_loader importable comme package
````

````python
# avant
# from data_extractor import (
#     ExtractedDocument, BaseExtractor, DocxExtractor, PdfExtractor,
#     JsonExtractor, HtmlExtractor, TxtExtractor,
# )
# corrigé :
from .data_extractor import (
    ExtractedDocument,
    BaseExtractor,
    DocxExtractor,
    PdfExtractor,
    JsonExtractor,
    HtmlExtractor,
    TxtExtractor,
)
from .extractor_factory import get_extractor, UnsupportedFileTypeError
from .docs_loader import DocsLoader

__all__ = [
    "ExtractedDocument",
    "BaseExtractor",
    "DocxExtractor",
    "PdfExtractor",
    "JsonExtractor",
    "HtmlExtractor",
    "TxtExtractor",
    "get_extractor",
    "UnsupportedFileTypeError",
    "DocsLoader",
]
````

````python
# avant
# from docx_extractor import DocxExtractor
# from pdf_extractor import PdfExtractor
# from json_extractor import JsonExtractor
# from html_extractor import HtmlExtractor
# from txt_extractor import TxtExtractor
# from base import BaseExtractor
# corrigé :
from .docx_extractor import DocxExtractor
from .pdf_extractor import PdfExtractor
from .json_extractor import JsonExtractor
from .html_extractor import HtmlExtractor
from .txt_extractor import TxtExtractor
from .base import BaseExtractor

from datetime import date
from typing import List
from pydantic import BaseModel, Field

class ExtractedDocument(BaseModel):
    """Schéma commun de sortie pour tous les extracteurs de documents."""
    title: str = Field(..., description="Titre du document")
    content: str = Field(..., description="Contenu du document")
    theme: str = Field(..., description="Thème du document")
    document_type: str = Field(..., description="Type de document")
    publish_date: str = Field(..., description="Date de publication (ISO)")

def split_document(
    doc: ExtractedDocument, max_length: int = 1000
) -> List[ExtractedDocument]:
    # implémentation…
    ...
    
__all__ = [
    "DocxExtractor",
    "PdfExtractor",
    "JsonExtractor",
    "HtmlExtractor",
    "TxtExtractor",
    "BaseExtractor",
    "ExtractedDocument",
    "split_document",
]
````

````python
from pathlib import Path
from datetime import date
from typing import Iterator
from bs4 import BeautifulSoup
from . import ExtractedDocument
from .base import BaseExtractor

class HtmlExtractor(BaseExtractor):
    def __init__(self, file_path: str):
        super().__init__(file_path)
        self.file_path = Path(file_path)
        self.content = self._load_file()
        self.metadata = self._extract_metadata()

    def _load_file(self) -> str:
        return BeautifulSoup(self.file_path.read_text(encoding="utf-8"), "html.parser")\
            .get_text(separator="\n")

    def _extract_metadata(self) -> dict:
        soup = BeautifulSoup(self.file_path.read_text(encoding="utf-8"), "html.parser")
        title = soup.title.string if soup.title else self.file_path.name
        return {
            "title": title,
            "content": self.content,
            "theme": "Thème générique",
            "document_type": "HTML",
            "publish_date": date.today().isoformat(),
        }

    def extract_many(self, max_length: int = 1000) -> Iterator[ExtractedDocument]:
        text = self.metadata["content"]
        chunks = self._split_text(text, max_length)
        for chunk in chunks:
            yield ExtractedDocument(**{**self.metadata, "content": chunk})
````

````python
from pathlib import Path
from datetime import date
from typing import Iterator
from textwrap import wrap
from . import ExtractedDocument
from .base import BaseExtractor

class TxtExtractor(BaseExtractor):
    def __init__(self, file_path: str):
        super().__init__(file_path)
        self.file_path = Path(file_path)
        self.content = self.file_path.read_text(encoding="utf-8")
        self.metadata = {
            "title": self.file_path.name,
            "content": self.content,
            "theme": "Thème générique",
            "document_type": "TXT",
            "publish_date": date.today().isoformat(),
        }

    def extract_many(self, max_length: int = 1000) -> Iterator[ExtractedDocument]:
        chunks = wrap(self.content, max_length, break_long_words=False, replace_whitespace=False)
        for chunk in chunks:
            yield ExtractedDocument(**{**self.metadata, "content": chunk})
````

````python
from pathlib import Path
from datetime import datetime
from typing import Iterator
from pypdf import PdfReader
from . import ExtractedDocument
from .base import BaseExtractor

class PdfExtractor(BaseExtractor):
    def __init__(self, file_path: str):
        super().__init__(file_path)
        self.file_path = Path(file_path)
        self.reader = PdfReader(self.file_path)
        self.metadata = self._extract_metadata()

    def _extract_metadata(self) -> dict:
        info = self.reader.metadata or {}
        text = "".join(p.extract_text() or "" for p in self.reader.pages)
        return {
            "title": info.get("/Title", self.file_path.name),
            "content": text,
            "theme": "Thème générique",
            "document_type": "PDF",
            "publish_date": info.get("/CreationDate", datetime.now().isoformat()),
        }

    def extract_many(self, max_length: int = 1000) -> Iterator[ExtractedDocument]:
        chunks = self._split_text(self.metadata["content"], max_length)
        for chunk in chunks:
            yield ExtractedDocument(**{**self.metadata, "content": chunk})
````

````python
from pathlib import Path
from datetime import date
from typing import Iterator
import json
from . import ExtractedDocument
from .base import BaseExtractor

class JsonExtractor(BaseExtractor):
    def __init__(self, file_path: str):
        super().__init__(file_path)
        self.file_path = Path(file_path)
        try:
            self.data = json.loads(self.file_path.read_text(encoding="utf-8"))
        except json.JSONDecodeError as e:
            raise ValueError(f"JSON invalide: {e}")
        self.metadata = {
            "title": self.file_path.name,
            "theme": "Thème générique",
            "document_type": "JSON",
            "publish_date": date.today().isoformat(),
        }

    def extract_many(self, max_length: int = 1000) -> Iterator[ExtractedDocument]:
        for entry in self.data:
            entry_content = entry.get("content", "")
            chunks = self._split_text(entry_content, max_length)
            meta = {**self.metadata, **entry}
            for chunk in chunks:
                yield ExtractedDocument(**{**meta, "content": chunk})
````

Après ça, exécutez sous WSL OpenSUSE Tumbleweed :

```bash
uv venv
uv sync
pytest clea_doc_loader/test/src/test_text_file_loader.py
```

WillIsback:     from . import ExtractedDocument
E   ImportError: cannot import name 'ExtractedDocument' from partially initialized module 'clea_doc_loader.src.data_extractor' (most likely due to a circular import) (/home/william/projet/Clea/clea-api/clea_doc_loader/src/data_extractor/__init__.py)

GitHub Copilot: Voici une solution qui casse la dépendance circulaire en extrayant `ExtractedDocument` et `split_document` dans un module à part (`models.py`), puis en important ces définitions depuis chacun des extracteurs.

```python
from datetime import date
from typing import Optional, List
from pydantic import BaseModel, Field
from textwrap import wrap

class ExtractedDocument(BaseModel):
    """Schéma commun de sortie pour tous les extracteurs de documents."""
    title: str = Field(..., description="Titre du document")
    content: str = Field(..., description="Contenu brut extrait")
    theme: str = Field(..., description="Thème du document")
    document_type: str = Field(..., description="Type (PDF, DOCX, etc.)")
    publish_date: date = Field(..., description="Date de publication")
    embedding: Optional[str] = Field(None, description="Représentation vectorielle (optionnelle)")

def split_document(doc: ExtractedDocument, max_length: int = 1000) -> List[ExtractedDocument]:
    """Split un document long en plusieurs chunks d'ExtractedDocument."""
    chunks = wrap(
        doc.content, max_length, break_long_words=False, replace_whitespace=False
    )
    return [
        ExtractedDocument(
            title=f"{doc.title} (part {i + 1})",
            content=chunk,
            theme=doc.theme,
            document_type=doc.document_type,
            publish_date=doc.publish_date,
            embedding=None,
        )
        for i, chunk in enumerate(chunks)
    ]
```

```python
from .models import ExtractedDocument, split_document
from .docx_extractor import DocxExtractor
from .pdf_extractor import PdfExtractor
from .json_extractor import JsonExtractor
from .html_extractor import HtmlExtractor
from .txt_extractor import TxtExtractor
from .base import BaseExtractor

__all__ = [
    "ExtractedDocument",
    "split_document",
    "DocxExtractor",
    "PdfExtractor",
    "JsonExtractor",
    "HtmlExtractor",
    "TxtExtractor",
    "BaseExtractor",
]
```

Modifiez ensuite chaque extracteur pour importer **uniquement** `ExtractedDocument` depuis `models` :

```python
from pathlib import Path
from datetime import date
from typing import Iterator
from bs4 import BeautifulSoup
from .models import ExtractedDocument
from .base import BaseExtractor

class HtmlExtractor(BaseExtractor):
    def __init__(...):
        ...
```

```python
from pathlib import Path
from datetime import date
from typing import Iterator
from textwrap import wrap
from .models import ExtractedDocument
from .base import BaseExtractor

class TxtExtractor(BaseExtractor):
    ...
```

```python
from pathlib import Path
from datetime import datetime
from typing import Iterator
from pypdf import PdfReader
from .models import ExtractedDocument
from .base import BaseExtractor

class PdfExtractor(BaseExtractor):
    ...
```

```python
from pathlib import Path
from datetime import date
from typing import Iterator
import json
from .models import ExtractedDocument
from .base import BaseExtractor

class JsonExtractor(BaseExtractor):
    ...
```

Enfin, installez/synchronisez avec `uv` et relancez vos tests :

```bash
uv venv
uv sync
pytest clea_doc_loader/test/src/test_text_file_loader.py


WillIsback: test log :
```log
test/src/test_text_file_loader.py ..F..                                                                                                                                                   [100%]

=========================================================================================== FAILURES ============================================================================================
____________________________________________________________________________________ test_extract_from_json _____________________________________________________________________________________

temp_dir = PosixPath('/tmp/pytest-of-william/pytest-0/test_extract_from_json0')

    def test_extract_from_json(temp_dir):
        content = [
            {
                "title": "Doc JSON",
                "content": "Texte JSON 1",
                "theme": "Test",
                "document_type": "JSON",
                "publish_date": str(date.today()),
            }
        ]
        file_path = create_temp_file(temp_dir, "test.json", json.dumps(content))
        loader = DocsLoader(str(file_path))
        docs = list(loader.extract_documents(max_length=50))
        assert len(docs) == 1
>       assert docs[0].title == "Doc JSON"
E       AssertionError: assert 'Doc JSON (part 1)' == 'Doc JSON'
E         
E         - Doc JSON
E         + Doc JSON (part 1)

test/src/test_text_file_loader.py:56: AssertionError
```


GitHub Copilot: Voici un correctif sur la fonction `split_document` (dans `base.py`) pour ne numéroter les « parties » qu’à partir de 2 chunks. Si le texte tient en un seul chunk, on garde le titre tel quel.

```python
from textwrap import wrap
from typing import List
from .models import ExtractedDocument

def split_document(
    doc: ExtractedDocument, max_length: int = 1000
) -> List[ExtractedDocument]:
    """
    Coupe doc.content en morceaux de longueur <= max_length,
    ne renumérote le titre qu'à partir de 2 chunks.
    """
    chunks = wrap(
        doc.content,
        max_length,
        break_long_words=False,
        replace_whitespace=False
    )

    # si un seul chunk, on renvoie l'objet d'origine
    if len(chunks) <= 1:
        return [doc]

    # sinon, on ajoute "(part i)" au titre
    parts: List[ExtractedDocument] = []
    for i, chunk in enumerate(chunks, start=1):
        parts.append(
            ExtractedDocument(
                title=f"{doc.title} (part {i})",
                content=chunk,
                theme=doc.theme,
                document_type=doc.document_type,
                publish_date=doc.publish_date,
                embedding=doc.embedding,
            )
        )
    return parts
```

Après ce patch, relancez sous WSL OpenSUSE Tumbleweed :

```bash
uv sync
pytest clea_doc_loader/test/src/test_text_file_loader.py
```

WillIsback: je ne comprends pas ce test :
```log
temp_dir = PosixPath('/tmp/pytest-of-william/pytest-3/test_extract_from_json0')

    def test_extract_from_json(temp_dir):
        content = [
            {
                "title": "Doc JSON",
                "content": "Texte JSON 1",
                "theme": "Test",
                "document_type": "JSON",
                "publish_date": str(date.today()),
            }
        ]
        file_path = create_temp_file(temp_dir, "test.json", json.dumps(content))
        loader = DocsLoader(str(file_path))
        docs = list(loader.extract_documents(max_length=50))
>       assert len(docs) == 1
E       AssertionError: assert 2 == 1
E        +  where 2 = len([ExtractedDocument(title='Doc JSON', content='Texte JSON 1', theme='Test', document_type='JSON', publish_date=datetime..., content='Texte JSON 1', theme='Test', document_type='JSON', publish_date=datetime.date(2025, 4, 30), embedding=None)])

test/src/test_text_file_loader.py:55: AssertionError
==================================================================================== short test summary info ====================================================================================
FAILED test/src/test_text_file_loader.py::test_extract_from_json - AssertionError: assert 2 == 1
```


GitHub Copilot: Le test échoue parce que votre `JsonExtractor.extract_many` faisait un `wrap(...)` direct et générait toujours au moins deux chunks (même quand le texte tient en un seul), puis rebaptisait systématiquement le titre avec `(part 1)`, `(part 2)`, etc.  

La solution est d’utiliser votre utilitaire `split_document` (qui renvoie exactement 1 chunk quand le contenu ne dépasse pas `max_length`) et de n’ajouter le suffixe `(part i)` **qu’à partir du 2ᵉ chunk**.  

Voici le patch à appliquer à  
json_extractor.py :

````python
from pathlib import Path
from datetime import date
from typing import Iterator
import json
from .models import ExtractedDocument
from .base import BaseExtractor, split_document


class JsonExtractor(BaseExtractor):
    def __init__(self, file_path: str):
        super().__init__(file_path)
        self.file_path = Path(file_path)
        try:
            self.data = json.loads(self.file_path.read_text(encoding="utf-8"))
        except json.JSONDecodeError as e:
            raise ValueError(f"JSON invalide: {e}")
        # valeurs par défaut si la clé fait défaut
        self.metadata = {
            "title": self.file_path.name,
            "theme": "Thème générique",
            "document_type": "JSON",
            "publish_date": date.today().isoformat(),
        }

    def extract_many(self, max_length: int = 1000) -> Iterator[ExtractedDocument]:
        for entry in self.data:
            # Merge entrée et metadata par défaut
            fields = {
                "title": entry.get("title", self.metadata["title"]),
                "content": entry.get("content", ""),
                "theme": entry.get("theme", self.metadata["theme"]),
                "document_type": entry.get("document_type", self.metadata["document_type"]),
                "publish_date": entry.get("publish_date", self.metadata["publish_date"]),
            }
            doc = ExtractedDocument(**fields)
            # split_document renvoie [doc] si doc.content <= max_length
            for chunk in split_document(doc, max_length):
                yield chunk
````

Ensuite, resynchronisez et relancez vos tests sous WSL OpenSUSE Tumbleweed :

```bash
uv sync
pytest clea_doc_loader/test/src/test_text_file_loader.py
```

WillIsback: est-ce que je dois utiliser l'utiliaire split document sur l'ensemble des mes script d'extraction ? 

GitHub Copilot: Oui, c’est fortement conseillé pour uniformiser votre découpage et éviter de dupliquer la logique de `wrap()` dans chaque extracteur. Vous pouvez alors :

1. Construire un `ExtractedDocument` « brut » avec tout le contenu.  
2. Appeler `split_document(doc, max_length)` pour récupérer la liste de chunks.  
3. Yielder chacun de ces chunks.  

Exemple de refactoring pour `PdfExtractor` :

````python
from pathlib import Path
from datetime import datetime
from typing import Iterator
from pypdf import PdfReader
from .models import ExtractedDocument
from .base import BaseExtractor, split_document

class PdfExtractor(BaseExtractor):
    def __init__(self, file_path: str):
        super().__init__(file_path)
        self.file_path = Path(file_path)
        self.reader = PdfReader(self.file_path)
        self.metadata = self._extract_metadata()

    def _extract_metadata(self) -> dict:
        info = self.reader.metadata or {}
        text = "".join(p.extract_text() or "" for p in self.reader.pages)
        return {
            "title": info.get("/Title", self.file_path.name),
            "content": text,
            "theme": info.get("/Subject", "Thème générique"),
            "document_type": "PDF",
            "publish_date": info.get("/CreationDate", datetime.now().isoformat()),
        }

    def extract_many(self, max_length: int = 1000) -> Iterator[ExtractedDocument]:
        # 1) Crée le document complet
        doc = ExtractedDocument(**self.metadata)
        # 2) Découpe et yield chaque chunk
        for chunk in split_document(doc, max_length):
            yield chunk
````

Vous ferez la même chose dans HtmlExtractor, TxtExtractor, JsonExtractor et DocxExtractor :  
- Récupérez d’abord `metadata = { …, "content": full_text, … }`.  
- Créez `doc = ExtractedDocument(**metadata)`.  
- Puis `for chunk in split_document(doc, max_length): yield chunk`.

WillIsback: mais de faire comme ça je risque de surcharger la mémoire vive avec un gros corpus ? Car j'ai l'impression que je decoupe un fichier pleinenement chargé plutot qu'au fur et à mesure ? à moins que je me trompe ?

GitHub Copilot: Pour limiter l’empreinte mémoire, vous pouvez découper « à la volée » sans charger tout le texte d’un coup : on parcourt les pages, on accumule dans un tampon et on émet un chunk dès qu’on dépasse la taille max. Par exemple dans `PdfExtractor` :

````python
from pathlib import Path
from datetime import datetime
from typing import Iterator
from pypdf import PdfReader
from .base import BaseExtractor, ExtractedDocument

class PdfExtractor(BaseExtractor):
    def __init__(self, file_path: str):
        super().__init__(file_path)
        self.file_path = Path(file_path)
        self.reader = PdfReader(self.file_path)

    def extract_many(self, max_length: int = 1000) -> Iterator[ExtractedDocument]:
        """Itère page par page, bufferise et yield dès que max_length est atteint."""
        info = self.reader.metadata or {}
        meta = {
            "title": info.get("/Title", self.file_path.name),
            "theme": info.get("/Subject", "Thème générique"),
            "document_type": "PDF",
            "publish_date": info.get("/CreationDate", datetime.now().isoformat()),
        }
        buffer = ""
        for page in self.reader.pages:
            text = page.extract_text() or ""
            buffer += text
            # tant qu'on a un chunk complet, on l'émet
            while len(buffer) >= max_length:
                chunk, buffer = buffer[:max_length], buffer[max_length:]
                yield ExtractedDocument(**{**meta, "content": chunk})
        # reste final
        if buffer:
            yield ExtractedDocument(**{**meta, "content": buffer})
````

Avec cette approche, vous n’avez jamais en mémoire que le tampon courant (≤ max_length + taille d’une page).

WillIsback: c'est donc une meilleur solution que mon utilitaire split_document. Pense tu qu'il est possible de definir un utilitaire uniforme pour réaliser cette tâche pour l'ensemble des extracteur (html, json, docx, txt, pdf) ?

GitHub Copilot: On peut en effet extraire et découper « à la volée » tous les formats avec un utilitaire unique qui prend en entrée :

- un méta‑dictionnaire (`title`, `theme`, …)  
- un itérateur de segments textuels (`str`)  
- une taille maxi par chunk  

et qui renvoie un `Iterator[ExtractedDocument]`.  

Par exemple, dans **clea_doc_loader/src/data_extractor/base.py**, ajoutez :

````python
from typing import Iterator, Dict
from .models import ExtractedDocument

def stream_split(
    meta: Dict, source: Iterator[str], max_length: int
) -> Iterator[ExtractedDocument]:
    """
    Découpe un flot de segments textuels en chunks de longueur <= max_length.
    Une fois le buffer rempli, on émet un ExtractedDocument avec content=chunk.
    """
    buffer = ""
    for segment in source:
        if not segment:
            continue
        buffer += segment
        while len(buffer) >= max_length:
            chunk, buffer = buffer[:max_length], buffer[max_length:]
            yield ExtractedDocument(**{**meta, "content": chunk})
    if buffer:
        yield ExtractedDocument(**{**meta, "content": buffer})
````

Ensuite, dans chaque extracteur vous :

1. Construisez votre méta-dict (sans champ `content`).  
2. Créez un itérateur de segments pour ce format (pages, lignes, paragraphes, `soup.stripped_strings`, etc.).  
3. Appelez `stream_split(meta, source, max_length)`.

Exemple **PdfExtractor** :

````python
from pypdf import PdfReader
from pathlib import Path
from datetime import datetime
from typing import Iterator
from .base import BaseExtractor, stream_split

class PdfExtractor(BaseExtractor):
    def __init__(self, file_path: str):
        super().__init__(file_path)
        self.file_path = Path(file_path)
        self.reader = PdfReader(self.file_path)
        info = self.reader.metadata or {}
        self.meta = {
            "title": info.get("/Title", self.file_path.name),
            "theme": info.get("/Subject", "Thème générique"),
            "document_type": "PDF",
            "publish_date": info.get("/CreationDate", datetime.now().isoformat()),
        }

    def extract_many(self, max_length: int = 1000) -> Iterator[ExtractedDocument]:
        pages = (page.extract_text() or "" for page in self.reader.pages)
        yield from stream_split(self.meta, pages, max_length)
````

Et **TxtExtractor** :

````python
from pathlib import Path
from datetime import date
from typing import Iterator
from .base import BaseExtractor, stream_split

class TxtExtractor(BaseExtractor):
    def __init__(self, file_path: str):
        super().__init__(file_path)
        self.file_path = Path(file_path)
        self.meta = {
            "title": self.file_path.name,
            "theme": "Thème générique",
            "document_type": "TXT",
            "publish_date": date.today().isoformat(),
        }

    def extract_many(self, max_length: int = 1000) -> Iterator[ExtractedDocument]:
        lines = (line for line in self.file_path.read_text(encoding="utf-8").splitlines(keepends=True))
        yield from stream_split(self.meta, lines, max_length)
````

**HtmlExtractor**, **JsonExtractor** et **DocxExtractor** se gèrent de la même façon :

- HTML : `source = soup.stripped_strings`  
- JSON  : `for entry in data: source = [entry.get("content","")]`  
- DOCX  : `source = (p.text for p in Document(file_path).paragraphs)`  

Cela standardise la consommation mémoire (on n’a jamais qu’un buffer ≤ max_length + une page/ligne) et supprime la duplication de `wrap()`.

WillIsback: est faire cela c'est bien mais si on n'ecrit pas en sortie tout ces splited document ils sont toujours en mémoire, ça risque de poser probleme si un fichier de plusieurs Go. est-ce que écrire le tampon au fur et à mesure dans un fichier temp qui sera manger une fois envoyer à la base de donnée libère de l'espace mémoire vive en consommant du travail sur lecture ecriture du disque ?

GitHub Copilot: Écrire vos chunks sur disque dès qu’ils sont produits permet en effet de ne jamais accumuler tout le contenu en RAM : vous y gagnez en empreinte mémoire au prix d’un peu de latence disque. Voici un petit utilitaire à ajouter dans `base.py` :

```python
import tempfile
import os
from typing import Iterator, Dict
from .models import ExtractedDocument

def stream_split_to_disk(
    meta: Dict, source: Iterator[str], max_length: int
) -> Iterator[ExtractedDocument]:
    """
    Même logique que stream_split, mais on écrit chaque chunk dans un
    NamedTemporaryFile, on yield un ExtractedDocument dont le contenu est
    lu depuis ce fichier, puis on supprime le fichier.
    """
    buffer = ""
    for segment in source:
        if not segment:
            continue
        buffer += segment
        while len(buffer) >= max_length:
            chunk, buffer = buffer[:max_length], buffer[max_length:]
            # on écrit sur disque
            with tempfile.NamedTemporaryFile(mode="w+", delete=False, suffix=".txt") as tmp:
                tmp.write(chunk)
                tmp.flush()
                path = tmp.name

            # on lit immédiatement pour remplir le modèle (ou on peut simplement passer path au loader)
            with open(path, "r", encoding="utf-8") as f:
                content = f.read()

            os.remove(path)

            yield ExtractedDocument(**{**meta, "content": content})

    # reste < max_length
    if buffer:
        with tempfile.NamedTemporaryFile(mode="w+", delete=False, suffix=".txt") as tmp:
            tmp.write(buffer)
            tmp.flush()
            path = tmp.name

        with open(path, "r", encoding="utf-8") as f:
            content = f.read()

        os.remove(path)
        yield ExtractedDocument(**{**meta, "content": content})
```

Et dans votre `PdfExtractor` vous remplacez l’appel à `stream_split` par `stream_split_to_disk` pour ne jamais garder plus d’un chunk en RAM :

```python
from .base import BaseExtractor, stream_split_to_disk
# ...
    def extract_many(self, max_length: int = 1000):
        pages = (page.extract_text() or "" for page in self.reader.pages)
        yield from stream_split_to_disk(self.meta, pages, max_length)
```

Avantages :
- Empreinte mémoire limitée à un buffer ≤ max_length + taille d’un segment.
- Les fichiers temporaires sont automatiquement nettoyés.

Inconvénients :
- Vous payez du I/O disque supplémentaire.
- (Sous WSL/OpenSUSE) assurez‑vous d’avoir un dossier tmp rapide ou orientez `NamedTemporaryFile` vers un ramdisk si besoin.

Pour installer `uv` et tester :

```bash
uv venv
uv sync
pytest clea_doc_loader/test/src/test_text_file_loader.py


WillIsback: refactorise docs_extractor basé sur ce qu'on a fait pour html_extractor :
```python

GitHub Copilot: 

WillIsback: refactorise docs_extractor basé sur ce qu'on a fait pour html_extractor :
```python
class HtmlExtractor(BaseExtractor):
    def __init__(self, file_path: str):
        super().__init__(file_path)
        self.file_path = Path(file_path)
        try:
            with open(self.file_path, "r", encoding="utf-8") as f:
                self.soup = BeautifulSoup(f, "html.parser")
        except Exception as e:
            raise ValueError(f"Erreur lors de la lecture du fichier HTML : {e}")

    def extract_many(self, max_length: int = 1000) -> Iterator[ExtractedDocument]:
        content = self.soup.get_text(separator="\n").strip()
        if not content:
            return

        meta = {
            "title": self.file_path.name,
            "theme": "Thème générique",
            "document_type": "HTML",
            "publish_date": date.today(),
            "embedding": None,
        }

        def stream_segments():
            for line in content.splitlines():
                yield line + "\n"

        yield from stream_split_to_disk(meta, stream_segments(), max_length=max_length)

```


GitHub Copilot: ```python
from pathlib import Path
from datetime import date
from typing import Iterator
from docx import Document
from .base import BaseExtractor, ExtractedDocument, stream_split_to_disk

class DocxExtractor(BaseExtractor):
    def __init__(self, file_path: str):
        super().__init__(file_path)
        self.file_path = Path(file_path)
        try:
            self.document = Document(file_path)
        except Exception as e:
            raise ValueError(f"Erreur lors de la lecture du fichier DOCX : {e}")

    def extract_many(self, max_length: int = 1000) -> Iterator[ExtractedDocument]:
        """
        Itère paragraphe par paragraphe et bufferise en chunks RAM-safe
        via stream_split_to_disk.
        """
        if not self.document.paragraphs:
            return

        meta = {
            "title": self.file_path.name,
            "theme": "Thème générique",
            "document_type": "DOCX",
            "publish_date": date.today(),
            "embedding": None,
        }

        def stream_segments():
            for para in self.document.paragraphs:
                text = para.text.strip()
                if text:
                    yield text + "\n"

        yield from stream_split_to_disk(meta, stream_segments(), max_length=max_length)
```

Pour installer la dépendance sous WSL OpenSUSE Tumbleweed :

```bash
uv pip install python-docx
```

WillIsback: @workspace /explain Argument of type "list[Any]" cannot be assigned to parameter "source" of type "Iterator[str]" in function "stream_split_to_disk"
  "list[Any]" is incompatible with protocol "Iterator[str]"
    "__next__" is not present

Workspace: Voici une correction simple pour lever l’erreur MyPy en passant un itérateur plutôt qu’une liste :

```python
    def extract_many(self, max_length: int = 1000) -> Iterator[ExtractedDocument]:
        for entry in self.entries:
            meta = { … }
            content = entry.get("content", "")
-           yield from stream_split_to_disk(meta, [content], max_length)
+           yield from stream_split_to_disk(meta, iter([content]), max_length)
```

Explication : `stream_split_to_disk` attend un `Iterator[str]`. En enveloppant votre liste dans `iter(...)`, vous fournissez bien un itérateur de chaînes.

WillIsback: définissons le endpoint pour charger le documents, aussi j'aimerais pouvoir analyser le résultat du traitement de l'extraction + split pour verifier son intégriter.

GitHub Copilot: Voici un exemple de POST‑endpoint pour charger un fichier, l’extraire et renvoyer à la fois les chunks et un petit méta‑rapport pour vérifier l’intégrité (nombre de chunks, taille cumulée) :

````python
from fastapi import APIRouter, UploadFile, File, Body, HTTPException
from typing import List
from pydantic import BaseModel
import tempfile, shutil, os

from clea_doc_loader.src.docs_loader import DocsLoader
from clea_doc_loader.src.data_extractor.models import ExtractedDocument

router = APIRouter()

class ExtractionResult(BaseModel):
    count: int
    total_chars: int
    chunks: List[ExtractedDocument]

@router.get(
    "/health",
    tags=["Health"],
    summary="Vérifier la santé de l'API",
)
async def health_check():
    return {"message": "L'API est en cours d'exécution."}

@router.post(
    "/documents",
    tags=["Loader"],
    summary="Charger un document et extraire les chunks",
    response_model=ExtractionResult,
)
async def load_and_extract(
    file: UploadFile = File(...),
    max_length: int = Body(1000, embed=True, description="Taille max d’un chunk"),
):
    # 1) Sauvegarde du fichier uploadé dans un répertoire temporaire
    temp_dir = tempfile.mkdtemp()
    tmp_path = os.path.join(temp_dir, file.filename)
    try:
        with open(tmp_path, "wb") as out:
            shutil.copyfileobj(file.file, out)
        # 2) Extraction
        loader = DocsLoader(tmp_path)
        docs = list(loader.extract_documents(max_length=max_length))
        if not docs:
            raise HTTPException(status_code=422, detail="Aucun contenu extrait.")
        # 3) Calcul d’un rapport d’intégrité
        total_chars = sum(len(d.content) for d in docs)
        return ExtractionResult(count=len(docs), total_chars=total_chars, chunks=docs)
    finally:
        # cleanup du tmp
        file.file.close()
        shutil.rmtree(temp_dir, ignore_errors=True)
````

Installation des dépendances sous WSL OpenSUSE Tumbleweed :

```bash
uv pip install fastapi uvicorn python-docx pypdf beautifulsoup4 lxml
uv sync
```

Vous pourrez ensuite lancer votre API avec :

```bash
uvicorn clea_doc_loader.api.loader_endpoint:router --reload --host 0.0.0.0 --port 8080
```

WillIsback: @workspace /explain No overloads for "join" match the provided arguments, Argument of type "str | None" cannot be assigned to parameter "paths" of type "StrPath" in function "join"
  Type "str | None" is not assignable to type "StrPath"
    Type "None" is not assignable to type "StrPath"
      "None" is not assignable to "str"
      "None" is incompatible with protocol "PathLike[str]"
        "__fspath__" is not present

Workspace: Voici une manière de forcer la présence d’un nom de fichier (non-None) avant l’appel à `os.path.join` :

```python
    async def load_and_extract(
        file: UploadFile = File(...),
        max_length: int = Body(1000, embed=True, description="Taille max d’un chunk"),
    ):
        # 1) Sauvegarde du fichier uploadé dans un répertoire temporaire
        temp_dir = tempfile.mkdtemp()
        filename = file.filename
        if not filename:
            # si filename est None ou chaîne vide, on renvoie une erreur
            raise HTTPException(status_code=400, detail="Nom de fichier manquant.")
        tmp_path = os.path.join(temp_dir, filename)
        try:
            with open(tmp_path, "wb") as out:
                shutil.copyfileobj(file.file, out)
            # …suite du code…
        finally:
            file.file.close()
            shutil.rmtree(temp_dir, ignore_errors=True)
```

Explication :
- Mypy signale que `file.filename` peut être `None`.  
- On vérifie donc explicitement (`if not filename`) et on lève une exception si c’est le cas,  
- puis on peut passer `filename: str` à `os.path.join` sans erreur de type.

WillIsback: aide moi à mieux faire pour mon endpoint

GitHub Copilot: Voici une version améliorée de votre endpoint de chargement de documents, avec plusieurs optimisations pour la robustesse, la sécurité et les performances:

```python
@router.post(
    "/upload",
    tags=["Loader"],
    summary="Charger un document et extraire les chunks",
    response_model=ExtractionResult,
)
async def load_and_extract(
    file: UploadFile = File(...),
    max_length: int = Body(1000, embed=True, description="Taille max d'un chunk"),
    theme: str = Body("Thème générique", embed=True, description="Thème du document"),
):
    # Validation du type de fichier
    supported_formats = [".pdf", ".docx", ".xlsx", ".xls", ".txt", ".json", ".html"]
    filename = file.filename or "unknown_file"
    file_ext = os.path.splitext(filename)[1].lower()
    
    if not any(filename.lower().endswith(ext) for ext in supported_formats):
        raise HTTPException(
            status_code=400,
            detail=f"Format de fichier non supporté. Formats acceptés: {', '.join(supported_formats)}"
        )
    
    # Sécurisation du nom de fichier
    safe_filename = "".join(c for c in filename if c.isalnum() or c in "._- ")
    
    # Création du répertoire temporaire avec plus de sécurité
    temp_dir = tempfile.mkdtemp(prefix="clea_loader_")
    tmp_path = os.path.join(temp_dir, safe_filename)
    
    try:
        # Écriture du fichier avec gestion de la mémoire (lecture par morceaux)
        with open(tmp_path, "wb") as out:
            # Lecture par chunks de 1Mo pour limiter l'utilisation de la mémoire
            chunk_size = 1024 * 1024  # 1Mo
            while chunk := await file.read(chunk_size):
                out.write(chunk)
        
        # Log
        print(f"Fichier '{safe_filename}' sauvegardé temporairement: {tmp_path}")
        
        # Extraction
        try:
            loader = DocsLoader(tmp_path)
            docs = list(loader.extract_documents(max_length=max_length))
            
            if not docs:
                raise HTTPException(status_code=422, detail="Aucun contenu extrait du document.")
                
            # Appliquer le thème spécifié par l'utilisateur, si fourni
            if theme != "Thème générique":
                for doc in docs:
                    doc.theme = theme
            
            # Calcul des statistiques
            total_chars = sum(len(d.content) for d in docs)
            avg_chunk_size = total_chars / len(docs) if docs else 0
            
            return ExtractionResult(
                count=len(docs),
                total_chars=total_chars,
                chunks=docs
            )
            
        except Exception as e:
            # Log de l'erreur et renvoi d'une réponse appropriée
            print(f"Erreur lors de l'extraction: {str(e)}")
            raise HTTPException(
                status_code=500,
                detail=f"Erreur lors de l'extraction du document: {str(e)}"
            )
            
    finally:
        # Nettoyage des ressources
        await file.close()
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir, ignore_errors=True)
            print(f"Répertoire temporaire nettoyé: {temp_dir}")
```

### Améliorations apportées :

1. **Validation et sécurité** :
   - Vérification des formats de fichiers acceptés
   - Nettoyage du nom de fichier pour éviter les problèmes de sécurité
   - Création de répertoires temporaires avec un préfixe identifiable

2. **Performance** :
   - Lecture du fichier par morceaux pour éviter de surcharger la RAM avec de gros fichiers
   - Utilisation de `await file.read()` au lieu de `shutil.copyfileobj()`

3. **Fonctionnalités** :
   - Possibilité de spécifier un thème personnalisé
   - Meilleure gestion des erreurs avec des messages détaillés

4. **Robustesse** :
   - Gestion du cas où `file.filename` est None
   - Nettoyage garanti des fichiers temporaires même en cas d'erreur

### Pour tester ce code sous OpenSUSE Tumbleweed avec uv :

```bash
# Installer les dépendances
uv pip install fastapi uvicorn python-multipart

# Lancer le serveur
uvicorn clea_doc_loader.api.main:app --reload --host 0.0.0.0 --port 8000
```

Vous pourrez alors tester l'API avec un client comme curl, Postman ou directement via l'interface Swagger de FastAPI à l'adresse http://localhost:8000/docs.

WillIsback: aide moi à faire un endpoint qui prend un chemin de fichier en argument

GitHub Copilot: Voici un exemple de **FastAPI endpoint** qui prend un chemin de fichier en argument, charge le fichier, effectue l'extraction et retourne les résultats. Ce code est compatible avec un environnement WSL OpenSUSE Tumbleweed et utilise `uv` comme gestionnaire de paquets.

```python
from fastapi import APIRouter, HTTPException, Query
from typing import List
from pydantic import BaseModel
from pathlib import Path
from clea_doc_loader.src import DocsLoader, ExtractedDocument

router = APIRouter()


class ExtractionResult(BaseModel):
    count: int
    total_chars: int
    chunks: List[ExtractedDocument]


@router.post(
    "/process-file",
    tags=["DocLoader"],
    summary="Traiter un fichier à partir de son chemin",
    response_model=ExtractionResult,
)
async def process_file(
    file_path: str = Query(..., description="Chemin absolu du fichier à traiter"),
    max_length: int = Query(1000, description="Taille maximale d'un chunk"),
    theme: str = Query("Thème générique", description="Thème du document"),
):
    """
    Traite un fichier à partir de son chemin, l'extrait et le divise en chunks.

    Args:
        file_path (str): Chemin absolu du fichier à traiter.
        max_length (int): Taille maximale d'un chunk.
        theme (str): Thème du document.

    Returns:
        ExtractionResult: Résultat de l'extraction.
    """
    # Vérification de l'existence du fichier
    path = Path(file_path)
    if not path.exists() or not path.is_file():
        raise HTTPException(status_code=404, detail=f"Fichier non trouvé : {file_path}")

    try:
        # Chargement et extraction
        loader = DocsLoader(str(path))
        docs = list(loader.extract_documents(max_length=max_length))

        if not docs:
            raise HTTPException(status_code=422, detail="Aucun contenu extrait du document.")

        # Appliquer le thème spécifié
        if theme != "Thème générique":
            for doc in docs:
                doc.theme = theme

        # Calcul des statistiques
        total_chars = sum(len(d.content) for d in docs)

        return ExtractionResult(
            count=len(docs),
            total_chars=total_chars,
            chunks=docs,
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur lors du traitement : {str(e)}")
```

---

### Explications des améliorations

1. **Validation du chemin de fichier** :
   - Vérifie si le fichier existe et s'il s'agit bien d'un fichier avec `Path.exists()` et `Path.is_file()`.

2. **Gestion des erreurs** :
   - Retourne une erreur HTTP 404 si le fichier n'existe pas.
   - Retourne une erreur HTTP 422 si aucun contenu n'est extrait.
   - Capture les exceptions générales et retourne une erreur HTTP 500 avec un message détaillé.

3. **Paramètres flexibles** :
   - `file_path` : Chemin absolu du fichier à traiter.
   - `max_length` : Taille maximale d'un chunk.
   - `theme` : Permet de spécifier un thème personnalisé.

4. **Statistiques d'intégrité** :
   - Retourne le nombre de chunks (`count`) et le nombre total de caractères (`total_chars`).

---

### Installation des dépendances

Pour installer les dépendances nécessaires avec `uv` :

```bash
uv pip install fastapi uvicorn python-docx pypdf beautifulsoup4 lxml
```

---

### Lancer le serveur

Pour lancer le serveur FastAPI :

```bash
uvicorn clea_doc_loader.api.loader_endpoint:router --reload --host 0.0.0.0 --port 8000
```

---

### Tester l'API

1. **Via Swagger UI** :
   - Accédez à `http://localhost:8000/docs` pour tester l'endpoint `/process-file`.

2. **Via `curl`** :
   ```bash
   curl -X POST "http://localhost:8000/process-file?file_path=/chemin/vers/fichier.txt&max_length=500&theme=Rapport" -H "accept: application/json"
   ```

3. **Via Python** :
   ```python
   import requests

   response = requests.post(
       "http://localhost:8000/process-file",
       params={"file_path": "/chemin/vers/fichier.txt", "max_length": 500, "theme": "Rapport"},
   )
   print(response.json())
   ```

WillIsback: redige l'unité de test du endpoint.

GitHub Copilot: Voici un exemple d'unité de test pour le endpoint `/process-file` en utilisant `pytest` et `httpx` pour tester l'API FastAPI. Ce test vérifie que le fichier est correctement traité et que les résultats retournés sont conformes.

```python
import pytest
from fastapi.testclient import TestClient
from clea_doc_loader.api.loader_endpoint import router
from pathlib import Path
import tempfile

# Création d'un client de test pour FastAPI
client = TestClient(router)

@pytest.fixture
def temp_file():
    """Fixture pour créer un fichier temporaire pour les tests."""
    temp_dir = tempfile.mkdtemp()
    file_path = Path(temp_dir) / "test.txt"
    with open(file_path, "w", encoding="utf-8") as f:
        f.write("Ceci est un test.\n" * 10)
    yield file_path
    file_path.unlink()  # Supprime le fichier après le test
    Path(temp_dir).rmdir()  # Supprime le répertoire temporaire

def test_process_file(temp_file):
    """Test du endpoint /process-file."""
    response = client.post(
        "/process-file",
        params={
            "file_path": str(temp_file),
            "max_length": 50,
            "theme": "Test Thème",
        },
    )
    assert response.status_code == 200
    data = response.json()

    # Vérifie que les résultats sont conformes
    assert data["count"] > 0
    assert data["total_chars"] > 0
    assert all("content" in chunk for chunk in data["chunks"])
    assert all(chunk["theme"] == "Test Thème" for chunk in data["chunks"])
```

---

### Explications

1. **`TestClient`** :
   - Utilisé pour simuler des requêtes HTTP vers l'API FastAPI.

2. **Fixture `temp_file`** :
   - Crée un fichier temporaire contenant du texte pour tester le traitement.
   - Nettoie automatiquement le fichier et le répertoire après le test.

3. **Test du endpoint** :
   - Envoie une requête POST au endpoint `/process-file` avec les paramètres nécessaires.
   - Vérifie que le code de réponse est `200`.
   - Vérifie que les résultats contiennent les champs attendus (`count`, `total_chars`, `chunks`).

4. **Assertions** :
   - Vérifie que le nombre de chunks est supérieur à 0.
   - Vérifie que le contenu total est supérieur à 0.
   - Vérifie que chaque chunk contient un champ `content` et que le thème est correctement appliqué.

---

### Installation des dépendances

Pour installer les dépendances nécessaires avec `uv` :

```bash
uv pip install pytest fastapi httpx
```

---

### Exécution des tests

Pour exécuter les tests :

```bash
pytest /home/william/projet/Clea/clea-api/clea_doc_loader/test/endpoint/test_loader_endpoint.py
```