{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Cl\u00e9a-API Framework de chargement de documents , recherche hybride et RAG (vectorielle + m\u00e9tadonn\u00e9es + g\u00e9n\u00e9ration) pour PostgreSQL + pgvector. Con\u00e7u 100 % local & hors-ligne pour vos donn\u00e9es sensibles (m\u00e9dicales, financi\u00e8res, juridiques\u2026). Acc\u00e8s rapides \ud83d\udcda Sujet Documentation Chargement & extraction Extracteurs \u00b7 Segmentation Base de donn\u00e9es & index vectoriels Database Moteur de recherche hybride Search Pipeline end-to-end Pipeline IA g\u00e9n\u00e9rative & RAG (AskAI) AskAI R\u00e9f\u00e9rence API Python (autogen) Doc Loader \u00b7 Vectordb \u00b7 Pipeline OpenAPI / Endpoints REST REST API Caract\u00e9ristiques principales \ud83d\udd12 100 % local & hors-ligne : aucun appel \u00e0 des services externes, parfait pour les donn\u00e9es confidentielles \ud83d\udcc2 Chargement multi-formats : PDF, DOCX, HTML, JSON, TXT\u2026 \ud83e\udde9 Segmentation hi\u00e9rarchique : Section \u2192 Paragraphe \u2192 Chunk \ud83d\udd0d Recherche hybride : ivfflat ou HNSW + filtres SQL + re-ranking Cross-Encoder \ud83e\udd16 RAG avec petits LLMs (AskAI) : g\u00e9n\u00e9ration augment\u00e9e en local (Qwen3-0.6B/1.7B) \u26a1 Pipeline \"one-liner\" : from pipeline import process_and_store process_and_store ( \"report.pdf\" , theme = \"R&D\" ) \ud83d\udee0\ufe0f Extensible : ajoutez un extracteur ou un mod\u00e8le en quelques lignes \ud83d\udc33 Docker-ready & CI-friendly (tests PyTest, MkDocs) Structure du d\u00e9p\u00f4t . \u251c\u2500\u2500 doc_loader/ # Extraction & chargement \u251c\u2500\u2500 vectordb/ # Mod\u00e8les & recherche \u251c\u2500\u2500 pipeline/ # Orchestration end-to-end \u251c\u2500\u2500 askai/ # RAG & g\u00e9n\u00e9ration locale \u251c\u2500\u2500 docs/ # Documentation MkDocs \u2190 vous \u00eates ici \u251c\u2500\u2500 demo/ # Exemples de fichiers \u2514\u2500\u2500 main.py # D\u00e9marreur FastAPI + configuration Installation rapide git clone https://github.com/votre-repo/clea-api.git cd clea-api # D\u00e9pendances Python uv pip install -r requirements.txt uv pip install -r askai/requirements_askai.txt # pour AskAI # Variables d\u2019environnement cp .env.sample .env # (\u00e9ditez DB_USER, DB_PASSWORD, etc.) # Initialiser la DB + extension pgvector uv python -m vectordb.src.database init_db # Lancer l\u2019API ./start.sh # ou uv run main.py L\u2019API sera disponible sur http://localhost:8080 . Con\u00e7u pour un usage local et s\u00e9curis\u00e9 , sans fuite de donn\u00e9es vers le cloud. Voir le README principal pour plus de d\u00e9tails.","title":"Accueil"},{"location":"#clea-api","text":"Framework de chargement de documents , recherche hybride et RAG (vectorielle + m\u00e9tadonn\u00e9es + g\u00e9n\u00e9ration) pour PostgreSQL + pgvector. Con\u00e7u 100 % local & hors-ligne pour vos donn\u00e9es sensibles (m\u00e9dicales, financi\u00e8res, juridiques\u2026).","title":"Cl\u00e9a-API"},{"location":"#acces-rapides","text":"Sujet Documentation Chargement & extraction Extracteurs \u00b7 Segmentation Base de donn\u00e9es & index vectoriels Database Moteur de recherche hybride Search Pipeline end-to-end Pipeline IA g\u00e9n\u00e9rative & RAG (AskAI) AskAI R\u00e9f\u00e9rence API Python (autogen) Doc Loader \u00b7 Vectordb \u00b7 Pipeline OpenAPI / Endpoints REST REST API","title":"Acc\u00e8s rapides \ud83d\udcda"},{"location":"#caracteristiques-principales","text":"\ud83d\udd12 100 % local & hors-ligne : aucun appel \u00e0 des services externes, parfait pour les donn\u00e9es confidentielles \ud83d\udcc2 Chargement multi-formats : PDF, DOCX, HTML, JSON, TXT\u2026 \ud83e\udde9 Segmentation hi\u00e9rarchique : Section \u2192 Paragraphe \u2192 Chunk \ud83d\udd0d Recherche hybride : ivfflat ou HNSW + filtres SQL + re-ranking Cross-Encoder \ud83e\udd16 RAG avec petits LLMs (AskAI) : g\u00e9n\u00e9ration augment\u00e9e en local (Qwen3-0.6B/1.7B) \u26a1 Pipeline \"one-liner\" : from pipeline import process_and_store process_and_store ( \"report.pdf\" , theme = \"R&D\" ) \ud83d\udee0\ufe0f Extensible : ajoutez un extracteur ou un mod\u00e8le en quelques lignes \ud83d\udc33 Docker-ready & CI-friendly (tests PyTest, MkDocs)","title":"Caract\u00e9ristiques principales"},{"location":"#structure-du-depot","text":". \u251c\u2500\u2500 doc_loader/ # Extraction & chargement \u251c\u2500\u2500 vectordb/ # Mod\u00e8les & recherche \u251c\u2500\u2500 pipeline/ # Orchestration end-to-end \u251c\u2500\u2500 askai/ # RAG & g\u00e9n\u00e9ration locale \u251c\u2500\u2500 docs/ # Documentation MkDocs \u2190 vous \u00eates ici \u251c\u2500\u2500 demo/ # Exemples de fichiers \u2514\u2500\u2500 main.py # D\u00e9marreur FastAPI + configuration","title":"Structure du d\u00e9p\u00f4t"},{"location":"#installation-rapide","text":"git clone https://github.com/votre-repo/clea-api.git cd clea-api # D\u00e9pendances Python uv pip install -r requirements.txt uv pip install -r askai/requirements_askai.txt # pour AskAI # Variables d\u2019environnement cp .env.sample .env # (\u00e9ditez DB_USER, DB_PASSWORD, etc.) # Initialiser la DB + extension pgvector uv python -m vectordb.src.database init_db # Lancer l\u2019API ./start.sh # ou uv run main.py L\u2019API sera disponible sur http://localhost:8080 . Con\u00e7u pour un usage local et s\u00e9curis\u00e9 , sans fuite de donn\u00e9es vers le cloud. Voir le README principal pour plus de d\u00e9tails.","title":"Installation rapide"},{"location":"database/","text":"Technologie de stockage Clea-API Ce document pr\u00e9sente l'architecture et les choix technologiques de la base de donn\u00e9es utilis\u00e9e par Clea-API . La solution repose sur PostgreSQL enrichi de l'extension pgvector , pilot\u00e9 via SQLAlchemy . Table des mati\u00e8res Configuration et connexion Mod\u00e8les de donn\u00e9es Document Chunk IndexConfig SearchQuery Indexation vectorielle Sch\u00e9ma global Bonnes pratiques Installation et configuration 1. Configuration et connexion Les param\u00e8tres de connexion sont lus depuis le fichier .env : DB_USER = os . getenv ( \"DB_USER\" , \"postgres\" ) DB_PASSWORD = os . getenv ( \"DB_PASSWORD\" , \"password\" ) DB_HOST = os . getenv ( \"DB_HOST\" , \"localhost\" ) DB_PORT = os . getenv ( \"DB_PORT\" , \"5432\" ) DB_NAME = os . getenv ( \"DB_NAME\" , \"vectordb\" ) DATABASE_URL = f \"postgresql:// { DB_USER } : { DB_PASSWORD } @ { DB_HOST } : { DB_PORT } / { DB_NAME } \" ```` ### Composants principaux * ** Engine SQLAlchemy ** : ` create_engine ( DATABASE_URL ) ` * ** Session factory ** : ``` python SessionLocal = sessionmaker ( bind = engine , autocommit = False , autoflush = False ) ``` * ** Base d\u00e9clarative ** : ` Base = declarative_base () ` ### Utilitaire de session ``` python def get_db (): \"\"\"Cr\u00e9e et retourne une session de base de donn\u00e9es. Yields: Session: Session SQLAlchemy pour les op\u00e9rations de base. \"\"\" db = SessionLocal () try : yield db finally : db . close () Initialisation et mise \u00e0 jour def init_db (): \"\"\"Initialise la base avec pgvector + toutes les tables.\"\"\" with engine . connect () as conn : conn . execute ( text ( \"CREATE EXTENSION IF NOT EXISTS vector\" )) conn . commit () Base . metadata . create_all ( bind = engine ) def update_db (): \"\"\"Cr\u00e9e les tables manquantes apr\u00e8s \u00e9volution des mod\u00e8les.\"\"\" inspector = inspect ( engine ) existing = inspector . get_table_names () to_create = [ name for name in Base . metadata . tables if name not in existing ] if to_create : Base . metadata . create_all ( bind = engine , tables = [ Base . metadata . tables [ n ] for n in to_create ] ) return { \"created\" : to_create , \"existing\" : existing } 2. Mod\u00e8les de donn\u00e9es Toutes les d\u00e9finitions suivantes proviennent de database.py . 2.1. Document class Document ( Base ): __tablename__ = \"documents\" id = mapped_column ( Integer , primary_key = True ) title = mapped_column ( String ( 255 ), nullable = False ) theme = mapped_column ( String ( 100 )) document_type = mapped_column ( String ( 100 )) publish_date = mapped_column ( Date ) corpus_id = mapped_column ( String ( 36 ), index = True ) created_at = mapped_column ( Date , default = datetime . now ) index_needed = mapped_column ( Boolean , default = False ) chunks = relationship ( \"Chunk\" , back_populates = \"document\" , cascade = \"all, delete-orphan\" ) __table_args__ = ( Index ( \"idx_document_theme\" , \"theme\" ), Index ( \"idx_document_type\" , \"document_type\" ), Index ( \"idx_document_date\" , \"publish_date\" ), Index ( \"idx_document_corpus\" , \"corpus_id\" ), ) Stocke les m\u00e9tadonn\u00e9es d\u2019un document (titre, th\u00e8me, type, dates, corpus_id). index_needed : flag pour d\u00e9clencher la (re)cr\u00e9ation de l\u2019index vectoriel. Relation 1\u2013N vers la table chunks . 2.2. Chunk class Chunk ( Base ): __tablename__ = \"chunks\" id = mapped_column ( Integer , primary_key = True ) document_id = mapped_column ( Integer , ForeignKey ( \"documents.id\" , ondelete = \"CASCADE\" ), nullable = False ) content = mapped_column ( Text , nullable = False ) embedding = mapped_column ( Vector ( 768 )) start_char = mapped_column ( Integer ) end_char = mapped_column ( Integer ) hierarchy_level = mapped_column ( Integer , default = 3 ) parent_chunk_id = mapped_column ( Integer , ForeignKey ( \"chunks.id\" , ondelete = \"CASCADE\" )) document = relationship ( \"Document\" , back_populates = \"chunks\" ) parent = relationship ( \"Chunk\" , remote_side = [ id ], back_populates = \"children\" ) children = relationship ( \"Chunk\" , back_populates = \"parent\" , cascade = \"all, delete-orphan\" , single_parent = True ) __table_args__ = ( Index ( \"idx_chunk_document_level\" , \"document_id\" , \"hierarchy_level\" ), Index ( \"idx_chunk_parent\" , \"parent_chunk_id\" ), ) Stocke le texte segment\u00e9 en \u00ab chunks \u00bb hi\u00e9rarchis\u00e9s (niveaux 0 \u00e0 3). embedding : vecteur 768-dimensions via pgvector . Auto-relation parent\u2013enfant pour reconstruire la hi\u00e9rarchie. 2.3. IndexConfig class IndexConfig ( Base ): __tablename__ = \"index_configs\" id = mapped_column ( Integer , primary_key = True ) corpus_id = mapped_column ( String ( 36 ), unique = True , nullable = False ) index_type = mapped_column ( String ( 20 ), default = \"ivfflat\" ) is_indexed = mapped_column ( Boolean , default = False ) chunk_count = mapped_column ( Integer , default = 0 ) last_indexed = mapped_column ( Date , nullable = True ) ivf_lists = mapped_column ( Integer , default = 100 ) hnsw_m = mapped_column ( Integer , default = 16 ) hnsw_ef_construction = mapped_column ( Integer , default = 200 ) Configure le type d\u2019index ( ivfflat ou hnsw ) et ses param\u00e8tres par corpus_id . is_indexed & last_indexed pour suivre l\u2019\u00e9tat de l\u2019index. 2.4. SearchQuery (nouvel historique des recherches) class SearchQuery ( Base ): __tablename__ = \"search_queries\" id = mapped_column ( Integer , primary_key = True ) query_text = mapped_column ( String , nullable = False ) theme = mapped_column ( String , nullable = True ) document_type = mapped_column ( String , nullable = True ) corpus_id = mapped_column ( String , nullable = True ) results_count = mapped_column ( Integer , default = 0 ) confidence_level = mapped_column ( Float , default = 0.0 ) created_at = mapped_column ( DateTime , default = datetime . now ) user_id = mapped_column ( String , nullable = True ) Objectif : historiser chaque action de recherche pour analyser tendances et usage. query_text : texte saisi par l\u2019utilisateur. Filtres optionnels : theme , document_type , corpus_id . results_count : nombre de chunks retourn\u00e9s. confidence_level : score agr\u00e9g\u00e9 ou m\u00e9trique de confiance. created_at : horodatage de la requ\u00eate. user_id : identifiant optionnel de l\u2019utilisateur (SSO, session, etc.). 3. Indexation vectorielle Principes de base Extension pgvector : int\u00e9gr\u00e9e \u00e0 PostgreSQL via CREATE EXTENSION vector Stockage : embeddings conserv\u00e9s dans des colonnes de type Vector(dimension) Index dynamiques : cr\u00e9ation SQL \u00e0 la demande selon la typologie du corpus Types d'index disponibles IVFFLAT (Inverted File with Flat Compression) CREATE INDEX idx_ivfflat ON chunks USING ivfflat ( embedding vector_cosine_ops ) WITH ( lists = 100 ) WHERE document_id IN ( SELECT id FROM documents WHERE corpus_id = 'corpus_xyz' ) Avantages : Rapide \u00e0 construire, efficace pour des volumes moyens (< 300K chunks) Param\u00e9trage : lists (nombre de clusters) - plus \u00e9lev\u00e9 = plus pr\u00e9cis mais plus lent HNSW (Hierarchical Navigable Small World) CREATE INDEX idx_hnsw ON chunks USING hnsw ( embedding vector_cosine_ops ) WITH ( m = 16 , ef_construction = 200 ) WHERE document_id IN ( SELECT id FROM documents WHERE corpus_id = 'corpus_xyz' ) Avantages : Plus pr\u00e9cis, excellentes performances m\u00eame avec des millions de vecteurs Param\u00e9trage : m (connexions par n\u0153ud) - \u00e9quilibre entre vitesse et pr\u00e9cision ef_construction (facteur d'exploration) - plus \u00e9lev\u00e9 = plus pr\u00e9cis mais plus lent \u00e0 construire 4. Sch\u00e9ma global \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u2551 Document \u2551 \u2551 (m\u00e9tadonn\u00e9es)\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u2551 \u2502 \u2551 1:N \u2502 1:N \u2551 \u25bc \u25bc \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u2551 Chunk \u2551 \u2551 Chunk \u2551 \u2551 Niveau 0-2 \u2551\u25c4\u2500\u2500\u2500\u2551 Niveau 3 \u2551 \u2551 (sections) \u2551 \u2551 (d\u00e9tails) \u2551 \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u2502 \u2502 \u25bc \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u2551 IndexConfig \u2551 \u2551 (par corpus)\u2551 \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d Document : contient uniquement les m\u00e9tadonn\u00e9es Chunks : stockent le contenu textuel hi\u00e9rarchique et les embeddings IndexConfig : param\u00e9trage des index vectoriels par corpus 5. Bonnes pratiques Performances \u00c9vitez les transactions longues avec des embeddings : consomment beaucoup de m\u00e9moire Cr\u00e9ez des index par corpus plut\u00f4t qu'un seul global Ajustez les param\u00e8tres selon votre volume : Petits corpus (< 50K chunks) : IVFFLAT avec 50-100 listes Corpus moyens (50K-300K) : IVFFLAT avec 100-300 listes Grands corpus (> 300K) : HNSW avec m=16, ef_construction=200 Monitoring Surveillez last_indexed et chunk_count pour d\u00e9tecter les d\u00e9rives de performance Reconstruisez les index si la recherche se d\u00e9grade ( VACUUM ANALYZE chunks ) S\u00e9curit\u00e9 Limitez les dimensions des vecteurs (768 ici) pour \u00e9viter la surcharge m\u00e9moire Utilisez des corpus_id en UUID pour l'isolation et la s\u00e9curit\u00e9 6. Installation et configuration Pr\u00e9requis PostgreSQL \u2265 14 Extension pgvector install\u00e9e Python \u2265 3.11 Installation sur openSUSE Tumbleweed (WSL) # Installer PostgreSQL et les d\u00e9pendances sudo zypper install postgresql14 postgresql14-server postgresql14-devel git gcc # Installer pgvector depuis les sources git clone https://github.com/pgvector/pgvector.git cd pgvector make sudo make install # Initialiser la base de donn\u00e9es PostgreSQL sudo systemctl enable postgresql sudo systemctl start postgresql sudo -u postgres createuser -s $USER createdb vectordb # Configurer le projet Python cd /chemin/vers/clea-api uv pip install -r requirements.txt # Initialiser la base de donn\u00e9es uv python -m vectordb.src.database Configuration minimale du fichier .env DB_USER=votre_utilisateur DB_PASSWORD=votre_mot_de_passe DB_HOST=localhost DB_PORT=5432 DB_NAME=vectordb Source : database.py Derni\u00e8re mise \u00e0 jour : 03 mai 2025","title":"Base de donn\u00e9es"},{"location":"database/#technologie-de-stockage-clea-api","text":"Ce document pr\u00e9sente l'architecture et les choix technologiques de la base de donn\u00e9es utilis\u00e9e par Clea-API . La solution repose sur PostgreSQL enrichi de l'extension pgvector , pilot\u00e9 via SQLAlchemy .","title":"Technologie de stockage Clea-API"},{"location":"database/#table-des-matieres","text":"Configuration et connexion Mod\u00e8les de donn\u00e9es Document Chunk IndexConfig SearchQuery Indexation vectorielle Sch\u00e9ma global Bonnes pratiques Installation et configuration","title":"Table des mati\u00e8res"},{"location":"database/#1-configuration-et-connexion","text":"Les param\u00e8tres de connexion sont lus depuis le fichier .env : DB_USER = os . getenv ( \"DB_USER\" , \"postgres\" ) DB_PASSWORD = os . getenv ( \"DB_PASSWORD\" , \"password\" ) DB_HOST = os . getenv ( \"DB_HOST\" , \"localhost\" ) DB_PORT = os . getenv ( \"DB_PORT\" , \"5432\" ) DB_NAME = os . getenv ( \"DB_NAME\" , \"vectordb\" ) DATABASE_URL = f \"postgresql:// { DB_USER } : { DB_PASSWORD } @ { DB_HOST } : { DB_PORT } / { DB_NAME } \" ```` ### Composants principaux * ** Engine SQLAlchemy ** : ` create_engine ( DATABASE_URL ) ` * ** Session factory ** : ``` python SessionLocal = sessionmaker ( bind = engine , autocommit = False , autoflush = False ) ``` * ** Base d\u00e9clarative ** : ` Base = declarative_base () ` ### Utilitaire de session ``` python def get_db (): \"\"\"Cr\u00e9e et retourne une session de base de donn\u00e9es. Yields: Session: Session SQLAlchemy pour les op\u00e9rations de base. \"\"\" db = SessionLocal () try : yield db finally : db . close ()","title":"1. Configuration et connexion"},{"location":"database/#initialisation-et-mise-a-jour","text":"def init_db (): \"\"\"Initialise la base avec pgvector + toutes les tables.\"\"\" with engine . connect () as conn : conn . execute ( text ( \"CREATE EXTENSION IF NOT EXISTS vector\" )) conn . commit () Base . metadata . create_all ( bind = engine ) def update_db (): \"\"\"Cr\u00e9e les tables manquantes apr\u00e8s \u00e9volution des mod\u00e8les.\"\"\" inspector = inspect ( engine ) existing = inspector . get_table_names () to_create = [ name for name in Base . metadata . tables if name not in existing ] if to_create : Base . metadata . create_all ( bind = engine , tables = [ Base . metadata . tables [ n ] for n in to_create ] ) return { \"created\" : to_create , \"existing\" : existing }","title":"Initialisation et mise \u00e0 jour"},{"location":"database/#2-modeles-de-donnees","text":"Toutes les d\u00e9finitions suivantes proviennent de database.py .","title":"2. Mod\u00e8les de donn\u00e9es"},{"location":"database/#21-document","text":"class Document ( Base ): __tablename__ = \"documents\" id = mapped_column ( Integer , primary_key = True ) title = mapped_column ( String ( 255 ), nullable = False ) theme = mapped_column ( String ( 100 )) document_type = mapped_column ( String ( 100 )) publish_date = mapped_column ( Date ) corpus_id = mapped_column ( String ( 36 ), index = True ) created_at = mapped_column ( Date , default = datetime . now ) index_needed = mapped_column ( Boolean , default = False ) chunks = relationship ( \"Chunk\" , back_populates = \"document\" , cascade = \"all, delete-orphan\" ) __table_args__ = ( Index ( \"idx_document_theme\" , \"theme\" ), Index ( \"idx_document_type\" , \"document_type\" ), Index ( \"idx_document_date\" , \"publish_date\" ), Index ( \"idx_document_corpus\" , \"corpus_id\" ), ) Stocke les m\u00e9tadonn\u00e9es d\u2019un document (titre, th\u00e8me, type, dates, corpus_id). index_needed : flag pour d\u00e9clencher la (re)cr\u00e9ation de l\u2019index vectoriel. Relation 1\u2013N vers la table chunks .","title":"2.1. Document"},{"location":"database/#22-chunk","text":"class Chunk ( Base ): __tablename__ = \"chunks\" id = mapped_column ( Integer , primary_key = True ) document_id = mapped_column ( Integer , ForeignKey ( \"documents.id\" , ondelete = \"CASCADE\" ), nullable = False ) content = mapped_column ( Text , nullable = False ) embedding = mapped_column ( Vector ( 768 )) start_char = mapped_column ( Integer ) end_char = mapped_column ( Integer ) hierarchy_level = mapped_column ( Integer , default = 3 ) parent_chunk_id = mapped_column ( Integer , ForeignKey ( \"chunks.id\" , ondelete = \"CASCADE\" )) document = relationship ( \"Document\" , back_populates = \"chunks\" ) parent = relationship ( \"Chunk\" , remote_side = [ id ], back_populates = \"children\" ) children = relationship ( \"Chunk\" , back_populates = \"parent\" , cascade = \"all, delete-orphan\" , single_parent = True ) __table_args__ = ( Index ( \"idx_chunk_document_level\" , \"document_id\" , \"hierarchy_level\" ), Index ( \"idx_chunk_parent\" , \"parent_chunk_id\" ), ) Stocke le texte segment\u00e9 en \u00ab chunks \u00bb hi\u00e9rarchis\u00e9s (niveaux 0 \u00e0 3). embedding : vecteur 768-dimensions via pgvector . Auto-relation parent\u2013enfant pour reconstruire la hi\u00e9rarchie.","title":"2.2. Chunk"},{"location":"database/#23-indexconfig","text":"class IndexConfig ( Base ): __tablename__ = \"index_configs\" id = mapped_column ( Integer , primary_key = True ) corpus_id = mapped_column ( String ( 36 ), unique = True , nullable = False ) index_type = mapped_column ( String ( 20 ), default = \"ivfflat\" ) is_indexed = mapped_column ( Boolean , default = False ) chunk_count = mapped_column ( Integer , default = 0 ) last_indexed = mapped_column ( Date , nullable = True ) ivf_lists = mapped_column ( Integer , default = 100 ) hnsw_m = mapped_column ( Integer , default = 16 ) hnsw_ef_construction = mapped_column ( Integer , default = 200 ) Configure le type d\u2019index ( ivfflat ou hnsw ) et ses param\u00e8tres par corpus_id . is_indexed & last_indexed pour suivre l\u2019\u00e9tat de l\u2019index.","title":"2.3. IndexConfig"},{"location":"database/#24-searchquery-nouvel-historique-des-recherches","text":"class SearchQuery ( Base ): __tablename__ = \"search_queries\" id = mapped_column ( Integer , primary_key = True ) query_text = mapped_column ( String , nullable = False ) theme = mapped_column ( String , nullable = True ) document_type = mapped_column ( String , nullable = True ) corpus_id = mapped_column ( String , nullable = True ) results_count = mapped_column ( Integer , default = 0 ) confidence_level = mapped_column ( Float , default = 0.0 ) created_at = mapped_column ( DateTime , default = datetime . now ) user_id = mapped_column ( String , nullable = True ) Objectif : historiser chaque action de recherche pour analyser tendances et usage. query_text : texte saisi par l\u2019utilisateur. Filtres optionnels : theme , document_type , corpus_id . results_count : nombre de chunks retourn\u00e9s. confidence_level : score agr\u00e9g\u00e9 ou m\u00e9trique de confiance. created_at : horodatage de la requ\u00eate. user_id : identifiant optionnel de l\u2019utilisateur (SSO, session, etc.).","title":"2.4. SearchQuery (nouvel historique des recherches)"},{"location":"database/#3-indexation-vectorielle","text":"","title":"3. Indexation vectorielle"},{"location":"database/#principes-de-base","text":"Extension pgvector : int\u00e9gr\u00e9e \u00e0 PostgreSQL via CREATE EXTENSION vector Stockage : embeddings conserv\u00e9s dans des colonnes de type Vector(dimension) Index dynamiques : cr\u00e9ation SQL \u00e0 la demande selon la typologie du corpus","title":"Principes de base"},{"location":"database/#types-dindex-disponibles","text":"","title":"Types d'index disponibles"},{"location":"database/#ivfflat-inverted-file-with-flat-compression","text":"CREATE INDEX idx_ivfflat ON chunks USING ivfflat ( embedding vector_cosine_ops ) WITH ( lists = 100 ) WHERE document_id IN ( SELECT id FROM documents WHERE corpus_id = 'corpus_xyz' ) Avantages : Rapide \u00e0 construire, efficace pour des volumes moyens (< 300K chunks) Param\u00e9trage : lists (nombre de clusters) - plus \u00e9lev\u00e9 = plus pr\u00e9cis mais plus lent","title":"IVFFLAT (Inverted File with Flat Compression)"},{"location":"database/#hnsw-hierarchical-navigable-small-world","text":"CREATE INDEX idx_hnsw ON chunks USING hnsw ( embedding vector_cosine_ops ) WITH ( m = 16 , ef_construction = 200 ) WHERE document_id IN ( SELECT id FROM documents WHERE corpus_id = 'corpus_xyz' ) Avantages : Plus pr\u00e9cis, excellentes performances m\u00eame avec des millions de vecteurs Param\u00e9trage : m (connexions par n\u0153ud) - \u00e9quilibre entre vitesse et pr\u00e9cision ef_construction (facteur d'exploration) - plus \u00e9lev\u00e9 = plus pr\u00e9cis mais plus lent \u00e0 construire","title":"HNSW (Hierarchical Navigable Small World)"},{"location":"database/#4-schema-global","text":"\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u2551 Document \u2551 \u2551 (m\u00e9tadonn\u00e9es)\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u2551 \u2502 \u2551 1:N \u2502 1:N \u2551 \u25bc \u25bc \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u2551 Chunk \u2551 \u2551 Chunk \u2551 \u2551 Niveau 0-2 \u2551\u25c4\u2500\u2500\u2500\u2551 Niveau 3 \u2551 \u2551 (sections) \u2551 \u2551 (d\u00e9tails) \u2551 \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u2502 \u2502 \u25bc \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u2551 IndexConfig \u2551 \u2551 (par corpus)\u2551 \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d Document : contient uniquement les m\u00e9tadonn\u00e9es Chunks : stockent le contenu textuel hi\u00e9rarchique et les embeddings IndexConfig : param\u00e9trage des index vectoriels par corpus","title":"4. Sch\u00e9ma global"},{"location":"database/#5-bonnes-pratiques","text":"","title":"5. Bonnes pratiques"},{"location":"database/#performances","text":"\u00c9vitez les transactions longues avec des embeddings : consomment beaucoup de m\u00e9moire Cr\u00e9ez des index par corpus plut\u00f4t qu'un seul global Ajustez les param\u00e8tres selon votre volume : Petits corpus (< 50K chunks) : IVFFLAT avec 50-100 listes Corpus moyens (50K-300K) : IVFFLAT avec 100-300 listes Grands corpus (> 300K) : HNSW avec m=16, ef_construction=200","title":"Performances"},{"location":"database/#monitoring","text":"Surveillez last_indexed et chunk_count pour d\u00e9tecter les d\u00e9rives de performance Reconstruisez les index si la recherche se d\u00e9grade ( VACUUM ANALYZE chunks )","title":"Monitoring"},{"location":"database/#securite","text":"Limitez les dimensions des vecteurs (768 ici) pour \u00e9viter la surcharge m\u00e9moire Utilisez des corpus_id en UUID pour l'isolation et la s\u00e9curit\u00e9","title":"S\u00e9curit\u00e9"},{"location":"database/#6-installation-et-configuration","text":"","title":"6. Installation et configuration"},{"location":"database/#prerequis","text":"PostgreSQL \u2265 14 Extension pgvector install\u00e9e Python \u2265 3.11","title":"Pr\u00e9requis"},{"location":"database/#installation-sur-opensuse-tumbleweed-wsl","text":"# Installer PostgreSQL et les d\u00e9pendances sudo zypper install postgresql14 postgresql14-server postgresql14-devel git gcc # Installer pgvector depuis les sources git clone https://github.com/pgvector/pgvector.git cd pgvector make sudo make install # Initialiser la base de donn\u00e9es PostgreSQL sudo systemctl enable postgresql sudo systemctl start postgresql sudo -u postgres createuser -s $USER createdb vectordb # Configurer le projet Python cd /chemin/vers/clea-api uv pip install -r requirements.txt # Initialiser la base de donn\u00e9es uv python -m vectordb.src.database","title":"Installation sur openSUSE Tumbleweed (WSL)"},{"location":"database/#configuration-minimale-du-fichier-env","text":"DB_USER=votre_utilisateur DB_PASSWORD=votre_mot_de_passe DB_HOST=localhost DB_PORT=5432 DB_NAME=vectordb Source : database.py Derni\u00e8re mise \u00e0 jour : 03 mai 2025","title":"Configuration minimale du fichier .env"},{"location":"main/","text":"Point d'entr\u00e9e principal : main.py Le fichier main.py constitue le d\u00e9marreur de tout le framework Cl\u00e9a-API , initialisant l'environnement, la base de donn\u00e9es, les extensions, puis lan\u00e7ant l'application FastAPI via Uvicorn. 1. Chargement de la configuration Charge les variables d'environnement depuis .env (via python-dotenv ) Configure le logger global Lit les param\u00e8tres PostgreSQL et API (h\u00f4te, port, workers, niveau de log) :contentReference[oaicite:0]{index=0}:contentReference[oaicite:1]{index=1} load_dotenv () POSTGRES_USER = os . getenv ( \"DB_USER\" ) POSTGRES_PASSWORD = os . getenv ( \"DB_PASSWORD\" ) \u2026 API_HOST = os . getenv ( \"API_HOST\" , \"localhost\" ) API_PORT = int ( os . getenv ( \"API_PORT\" , 8080 )) API_WORKERS = int ( os . getenv ( \"API_WORKERS\" , 1 )) API_LOG_LEVEL = os . getenv ( \"API_LOG_LEVEL\" , \"info\" ) ```` --- ## 2. Gestion centralis\u00e9e du logging ### `configure_logging(debug_mode: bool = False) \u2192 None` Configure le syst\u00e8me de journalisation pour toute l 'application: 1. D\u00e9finit le niveau de log en fonction du mode debug ( ` DEBUG ` ou ` INFO ` ) 2. Configure le logger racine avec un format standardis\u00e9 : ``` python % ( asctime ) s - % ( name ) s - % ( levelname ) s - % ( message ) s Ajuste la verbosit\u00e9 des loggers tiers (uvicorn, sqlalchemy) pour r\u00e9duire le bruit Ce syst\u00e8me permet d'utiliser dans tous les modules: from utils import get_logger # Obtenir un logger pr\u00e9fix\u00e9 automatiquement logger = get_logger ( \"mon_module.sous_module\" ) # Utiliser selon le contexte logger . debug ( \"D\u00e9tail technique\" ) # Visible uniquement en mode debug logger . info ( \"Information importante\" ) # Visible en mode normal logger . error ( \"Probl\u00e8me critique\" ) # Toujours visible La centralisation \u00e9vite de red\u00e9finir le niveau dans chaque module. 3. Arguments en ligne de commande L'application accepte plusieurs arguments pour personnaliser son lancement: Argument Description D\u00e9faut --debug Active les logs d\u00e9taill\u00e9s False --port Port d'\u00e9coute du serveur Valeur de .env ou 8080 --host H\u00f4te d'\u00e9coute du serveur Valeur de .env ou localhost --workers Nombre de processus workers Valeur de .env ou 1 Exemple d'utilisation: python main.py --debug --port 9000 --workers 4 4. V\u00e9rification et d\u00e9marrage de PostgreSQL start_postgres() \u2192 bool R\u00e9cup\u00e8re l'utilisateur courant ( get_current_user ). V\u00e9rifie la disponibilit\u00e9 de PostgreSQL ( check_postgres_status ). Si indisponible, affiche des suggestions d'installation pour Linux, notamment OpenSUSE Tumbleweed. Retourne True si PostgreSQL est accessible, False sinon. 5. Initialisation de la base de donn\u00e9es setup_database() \u2192 bool Cr\u00e9e les tables SQLAlchemy ( Base.metadata.create_all ) et l'extension vector . Appelle init_db() pour installer pgvector et valider la cr\u00e9ation des tables. Renvoie True si l'extension et les tables sont correctement cr\u00e9\u00e9es. 6. Cycle de vie de l'application lifespan(app: FastAPI) \u2192 AsyncGenerator[None, None] Gestionnaire asynchrone ex\u00e9cut\u00e9 au d\u00e9marrage et \u00e0 l'arr\u00eat : Au d\u00e9marrage : D\u00e9marre/v\u00e9rifie PostgreSQL via start_postgres . V\u00e9rifie l'existence des tables ( verify_database_tables ), et lance setup_database si n\u00e9cessaire. \u00c0 l'arr\u00eat : Vide le cache de ressources globales. 7. Cr\u00e9ation de l'application FastAPI app = FastAPI ( title = \"Cl\u00e9a API\" , description = \"API pour g\u00e9rer les documents et effectuer des recherches s\u00e9mantiques.\" , version = \"0.1.1\" , docs_url = \"/docs\" , redoc_url = \"/redoc\" , lifespan = lifespan , ) Active Swagger UI (docs) et ReDoc ( /redoc ). Associe le lifespan d\u00e9fini ci-dessus. 8. Middleware et routeurs CORS : autorise toutes les origines, m\u00e9thodes et headers ( CORSMiddleware ). Inclusion des routers : /database \u2192 crud documents & chunks /search \u2192 recherche hybride /index \u2192 gestion des index vectoriels doc_loader \u2192 upload et extraction de documents pipeline \u2192 traitement complet (extract \u2192 segment \u2192 insert) 9. Gestion des erreurs globales StarletteHTTPException \u2192 renvoie { \"error\": detail } RequestValidationError \u2192 renvoie { \"error\": \"Invalid request\", \"details\": [...] } Exception \u2192 capture toute erreur non g\u00e9r\u00e9e et renvoie 500 Internal server error . 10. Endpoints de sant\u00e9 @app . get ( \"/\" ) async def root (): return { \"message\" : \"Cl\u00e9a API is running\" } \u2013 V\u00e9rifie simplement que l'API est en ligne. 11. Lancement du serveur Lorsque main.py est ex\u00e9cut\u00e9 directement ( if __name__ == \"__main__\" ), il : Parse les arguments en ligne de commande (debug, host, port, workers) Configure le logging via configure_logging() Cr\u00e9e une configuration Uvicorn optimis\u00e9e (reload, workers, proxy headers, choix auto de loop/http) D\u00e9marre le serveur avec uvicorn.Server(config).run() . python main.py # ou avec options python main.py --debug --port 9000 # ou via uvicorn directement uvicorn main:app --host $API_HOST --port $API_PORT --workers $API_WORKERS Fichier source : main.py Derni\u00e8re mise \u00e0 jour : 04 mai 2025","title":"Introduction"},{"location":"main/#point-dentree-principal-mainpy","text":"Le fichier main.py constitue le d\u00e9marreur de tout le framework Cl\u00e9a-API , initialisant l'environnement, la base de donn\u00e9es, les extensions, puis lan\u00e7ant l'application FastAPI via Uvicorn.","title":"Point d'entr\u00e9e principal : main.py"},{"location":"main/#1-chargement-de-la-configuration","text":"Charge les variables d'environnement depuis .env (via python-dotenv ) Configure le logger global Lit les param\u00e8tres PostgreSQL et API (h\u00f4te, port, workers, niveau de log) :contentReference[oaicite:0]{index=0}:contentReference[oaicite:1]{index=1} load_dotenv () POSTGRES_USER = os . getenv ( \"DB_USER\" ) POSTGRES_PASSWORD = os . getenv ( \"DB_PASSWORD\" ) \u2026 API_HOST = os . getenv ( \"API_HOST\" , \"localhost\" ) API_PORT = int ( os . getenv ( \"API_PORT\" , 8080 )) API_WORKERS = int ( os . getenv ( \"API_WORKERS\" , 1 )) API_LOG_LEVEL = os . getenv ( \"API_LOG_LEVEL\" , \"info\" ) ```` --- ## 2. Gestion centralis\u00e9e du logging ### `configure_logging(debug_mode: bool = False) \u2192 None` Configure le syst\u00e8me de journalisation pour toute l 'application: 1. D\u00e9finit le niveau de log en fonction du mode debug ( ` DEBUG ` ou ` INFO ` ) 2. Configure le logger racine avec un format standardis\u00e9 : ``` python % ( asctime ) s - % ( name ) s - % ( levelname ) s - % ( message ) s Ajuste la verbosit\u00e9 des loggers tiers (uvicorn, sqlalchemy) pour r\u00e9duire le bruit Ce syst\u00e8me permet d'utiliser dans tous les modules: from utils import get_logger # Obtenir un logger pr\u00e9fix\u00e9 automatiquement logger = get_logger ( \"mon_module.sous_module\" ) # Utiliser selon le contexte logger . debug ( \"D\u00e9tail technique\" ) # Visible uniquement en mode debug logger . info ( \"Information importante\" ) # Visible en mode normal logger . error ( \"Probl\u00e8me critique\" ) # Toujours visible La centralisation \u00e9vite de red\u00e9finir le niveau dans chaque module.","title":"1. Chargement de la configuration"},{"location":"main/#3-arguments-en-ligne-de-commande","text":"L'application accepte plusieurs arguments pour personnaliser son lancement: Argument Description D\u00e9faut --debug Active les logs d\u00e9taill\u00e9s False --port Port d'\u00e9coute du serveur Valeur de .env ou 8080 --host H\u00f4te d'\u00e9coute du serveur Valeur de .env ou localhost --workers Nombre de processus workers Valeur de .env ou 1 Exemple d'utilisation: python main.py --debug --port 9000 --workers 4","title":"3. Arguments en ligne de commande"},{"location":"main/#4-verification-et-demarrage-de-postgresql","text":"","title":"4. V\u00e9rification et d\u00e9marrage de PostgreSQL"},{"location":"main/#start_postgres-bool","text":"R\u00e9cup\u00e8re l'utilisateur courant ( get_current_user ). V\u00e9rifie la disponibilit\u00e9 de PostgreSQL ( check_postgres_status ). Si indisponible, affiche des suggestions d'installation pour Linux, notamment OpenSUSE Tumbleweed. Retourne True si PostgreSQL est accessible, False sinon.","title":"start_postgres() \u2192 bool"},{"location":"main/#5-initialisation-de-la-base-de-donnees","text":"","title":"5. Initialisation de la base de donn\u00e9es"},{"location":"main/#setup_database-bool","text":"Cr\u00e9e les tables SQLAlchemy ( Base.metadata.create_all ) et l'extension vector . Appelle init_db() pour installer pgvector et valider la cr\u00e9ation des tables. Renvoie True si l'extension et les tables sont correctement cr\u00e9\u00e9es.","title":"setup_database() \u2192 bool"},{"location":"main/#6-cycle-de-vie-de-lapplication","text":"","title":"6. Cycle de vie de l'application"},{"location":"main/#lifespanapp-fastapi-asyncgeneratornone-none","text":"Gestionnaire asynchrone ex\u00e9cut\u00e9 au d\u00e9marrage et \u00e0 l'arr\u00eat : Au d\u00e9marrage : D\u00e9marre/v\u00e9rifie PostgreSQL via start_postgres . V\u00e9rifie l'existence des tables ( verify_database_tables ), et lance setup_database si n\u00e9cessaire. \u00c0 l'arr\u00eat : Vide le cache de ressources globales.","title":"lifespan(app: FastAPI) \u2192 AsyncGenerator[None, None]"},{"location":"main/#7-creation-de-lapplication-fastapi","text":"app = FastAPI ( title = \"Cl\u00e9a API\" , description = \"API pour g\u00e9rer les documents et effectuer des recherches s\u00e9mantiques.\" , version = \"0.1.1\" , docs_url = \"/docs\" , redoc_url = \"/redoc\" , lifespan = lifespan , ) Active Swagger UI (docs) et ReDoc ( /redoc ). Associe le lifespan d\u00e9fini ci-dessus.","title":"7. Cr\u00e9ation de l'application FastAPI"},{"location":"main/#8-middleware-et-routeurs","text":"CORS : autorise toutes les origines, m\u00e9thodes et headers ( CORSMiddleware ). Inclusion des routers : /database \u2192 crud documents & chunks /search \u2192 recherche hybride /index \u2192 gestion des index vectoriels doc_loader \u2192 upload et extraction de documents pipeline \u2192 traitement complet (extract \u2192 segment \u2192 insert)","title":"8. Middleware et routeurs"},{"location":"main/#9-gestion-des-erreurs-globales","text":"StarletteHTTPException \u2192 renvoie { \"error\": detail } RequestValidationError \u2192 renvoie { \"error\": \"Invalid request\", \"details\": [...] } Exception \u2192 capture toute erreur non g\u00e9r\u00e9e et renvoie 500 Internal server error .","title":"9. Gestion des erreurs globales"},{"location":"main/#10-endpoints-de-sante","text":"@app . get ( \"/\" ) async def root (): return { \"message\" : \"Cl\u00e9a API is running\" } \u2013 V\u00e9rifie simplement que l'API est en ligne.","title":"10. Endpoints de sant\u00e9"},{"location":"main/#11-lancement-du-serveur","text":"Lorsque main.py est ex\u00e9cut\u00e9 directement ( if __name__ == \"__main__\" ), il : Parse les arguments en ligne de commande (debug, host, port, workers) Configure le logging via configure_logging() Cr\u00e9e une configuration Uvicorn optimis\u00e9e (reload, workers, proxy headers, choix auto de loop/http) D\u00e9marre le serveur avec uvicorn.Server(config).run() . python main.py # ou avec options python main.py --debug --port 9000 # ou via uvicorn directement uvicorn main:app --host $API_HOST --port $API_PORT --workers $API_WORKERS Fichier source : main.py Derni\u00e8re mise \u00e0 jour : 04 mai 2025","title":"11. Lancement du serveur"},{"location":"schemas/","text":"Schemas Pydantic de Clea-API Les sch\u00e9mas Pydantic d\u00e9finissent la forme des donn\u00e9es \u00e9chang\u00e9es avec l\u2019API (requ\u00eates et r\u00e9ponses). Tous les mod\u00e8les utilisent la configuration CamelConfig pour accepter et produire du camelCase . DocumentCreate Payload minimal pour cr\u00e9er un document (sans contenu). Champ Type Requis Alias Description title string Oui title Titre du document. theme string Oui theme Th\u00e8me du document. document_type string Oui documentType Type du document (PDF, TXT, etc.). publish_date date Oui publishDate Date de publication (YYYY-MM-DD). corpus_id string Non corpusId UUID du corpus (optionnel). type DocumentCreate = { title : string ; theme : string ; documentType : string ; publishDate : string ; // \"2025-05-01\" corpusId? : string ; } ```` --- ## ChunkCreate ** Payload ** pour cr\u00e9er un chunk ( fragment de texte et m\u00e9tadonn\u00e9es hi\u00e9rarchiques ). | Champ | Type | Requis | Alias | Description | | ----------------- | --------- | ------ | ---------------- | ------------------------------------------------------------ | | `id` | `number` | Non | `id` | Identifiant temporaire ( uniquement pour hi\u00e9rarchie interne ). | | `content` | `string` | Oui | `content` | Contenu textuel du chunk . | | `start_char` | `integer` | Oui | `startChar` | Position de d\u00e9but dans le texte source ( >= 0 ). | | `end_char` | `integer` | Oui | `endChar` | Position de fin dans le texte source ( > `startChar` ). | | `hierarchy_level` | `integer` | Oui | `hierarchyLevel` | Niveau hi\u00e9rarchique ( 0 \u2013 3 ). | | `parent_chunk_id` | `integer` | Non | `parentChunkId` | ID du chunk parent ( ou `null` ). | ```ts type ChunkCreate = { id?: number; content: string; startChar: number; endChar: number; hierarchyLevel: 0 | 1 | 2 | 3; parentChunkId?: number | null; } DocumentWithChunks Payload complet pour POST /database/documents . Champ Type Requis Alias Description document DocumentCreate Oui document M\u00e9tadonn\u00e9es du document. chunks ChunkCreate[] Oui chunks Liste des chunks \u00e0 ins\u00e9rer. type DocumentWithChunks = { document : DocumentCreate ; chunks : ChunkCreate []; } DocumentResponse R\u00e9ponse standard pour toutes les op\u00e9rations CRUD sur les documents. Champ Type Requis Alias Description id integer Oui id Identifiant du document. title string Oui title Titre du document. theme string Oui theme Th\u00e8me du document. document_type string Oui documentType Type du document. publish_date date Oui publishDate Date de publication. corpus_id string Non corpusId UUID du corpus. chunk_count integer Oui chunkCount Nombre de chunks associ\u00e9s (\u2265 0). index_needed boolean Oui indexNeeded true si un nouvel index est requis. type DocumentResponse = { id : number ; title : string ; theme : string ; documentType : string ; publishDate : string ; corpusId? : string ; chunkCount : number ; indexNeeded : boolean ; } DocumentUpdate Payload pour PUT /database/documents/{id} (mise \u00e0 jour). Champ Type Requis Alias Description id integer Oui id Identifiant du document \u00e0 mettre \u00e0 jour. title string Non title Nouveau titre. theme string Non theme Nouveau th\u00e8me. document_type string Non documentType Nouveau type de document. publish_date date Non publishDate Nouvelle date de publication. corpus_id string Non corpusId Nouvel UUID de corpus. type DocumentUpdate = { id : number ; title? : string ; theme? : string ; documentType? : string ; publishDate? : string ; corpusId? : string ; } UpdateWithChunks Payload pour PUT /database/documents/{id} avec ajout de chunks. Champ Type Requis Alias Description document DocumentUpdate Oui document M\u00e9tadonn\u00e9es \u00e0 mettre \u00e0 jour. new_chunks ChunkCreate[] Non newChunks Liste de nouveaux chunks \u00e0 ajouter. type UpdateWithChunks = { document : DocumentUpdate ; newChunks? : ChunkCreate []; } HierarchicalContext Parents (niveaux 0\u20132) renvoy\u00e9s quand hierarchical=true en recherche. Champ Type Requis Alias Description level_0 object or null Non level0 Chunk de section (lvl 0). level_1 object or null Non level1 Chunk de sous-section (lvl 1). level_2 object or null Non level2 Chunk de paragraphe (lvl 2). ChunkResult Un chunk renvoy\u00e9 par POST /search/hybrid_search . Champ Type Requis Alias Description chunk_id integer Oui chunkId Identifiant du chunk. document_id integer Oui documentId ID du document parent. title string Oui title Titre du document parent. content string Oui content Contenu du chunk. theme string Oui theme Th\u00e8me du document. document_type string Oui documentType Type du document. publish_date date Oui publishDate Date de publication. score number Oui score Score de similarit\u00e9 (distance ou score). hierarchy_level integer Oui hierarchyLevel Niveau hi\u00e9rarchique (0\u20133). context HierarchicalContext or null Non context Contexte parent (facultatif). SearchRequest Param\u00e8tres pour la recherche hybride. Combine la requ\u00eate textuelle avec des filtres de m\u00e9tadonn\u00e9es optionnels. Champ Type Requis Alias Description query string Oui query Requ\u00eate en langage naturel. top_k integer Non topK Nombre de r\u00e9sultats \u00e0 retourner (d\u00e9faut 10). theme string Non theme Filtre par th\u00e8me. document_type string Non documentType Filtre par type de document. start_date date Non startDate Date de d\u00e9but. end_date date Non endDate Date de fin. corpus_id integer Non corpusId ID du corpus. hierarchy_level integer Non hierarchyLevel Niveau hi\u00e9rarchique (0\u20132). hierarchical boolean Non hierarchical R\u00e9cup\u00e9rer le contexte hi\u00e9rarchique. filter_by_relevance boolean Non filterByRelevance Filtrer les r\u00e9sultats sous le seuil de pertinence. normalize_scores boolean Non normalizeScores Normaliser les scores entre 0 et 1. SearchResponse R\u00e9ponse \u00e0 une requ\u00eate de recherche. Contient les r\u00e9sultats tri\u00e9s par pertinence avec m\u00e9tadonn\u00e9es et \u00e9valuation de confiance. Champ Type Requis Alias Description query string Oui query Requ\u00eate originale. top_k integer Oui topK Nombre de r\u00e9sultats demand\u00e9s. total_results integer Oui totalResults Nombre total de r\u00e9sultats trouv\u00e9s. results ChunkResult[] Oui results R\u00e9sultats de la recherche. confidence ConfidenceMetrics Non confidence M\u00e9triques de confiance sur les r\u00e9sultats. normalized boolean Non normalized Indique si les scores sont normalis\u00e9s (0\u20131). message string Non message Message informatif sur les r\u00e9sultats. ConfidenceMetrics M\u00e9triques de confiance et statistiques pour \u00e9valuer la pertinence des r\u00e9sultats. Champ Type Requis Alias Description level float Oui level Niveau de confiance entre 0 et 1. message string Oui message Message explicatif sur la qualit\u00e9 des r\u00e9sultats. stats object Oui stats Statistiques sur les scores. Exemple de stats : { \"min\" : -5.89 , \"max\" : 1.23 , \"avg\" : -0.45 , \"median\" : -0.12 } IndexStatus Statut d\u2019indexation d\u2019un corpus. Champ Type Requis Alias Description corpus_id string Oui corpusId UUID du corpus interrog\u00e9. index_exists boolean Oui indexExists true si l\u2019index physique existe en base. config_exists boolean Oui configExists true si la config SQLAlchemy ( IndexConfig ) existe. is_indexed boolean Oui isIndexed true si la config est marqu\u00e9e index\u00e9e. index_type string Non indexType Type d\u2019index ( ivfflat , hnsw ). chunk_count integer Oui chunkCount Nombre total de chunks dans le corpus. indexed_chunks integer Oui indexedChunks Nombre de chunks effectivement index\u00e9s. last_indexed date Non lastIndexed Date du dernier index (ou null ). Tous les mod\u00e8les se trouvent dans vectordb/src/schemas.py Derni\u00e8re mise \u00e0 jour : 03 mai 2025 .","title":"Sch\u00e9mas"},{"location":"schemas/#schemas-pydantic-de-clea-api","text":"Les sch\u00e9mas Pydantic d\u00e9finissent la forme des donn\u00e9es \u00e9chang\u00e9es avec l\u2019API (requ\u00eates et r\u00e9ponses). Tous les mod\u00e8les utilisent la configuration CamelConfig pour accepter et produire du camelCase .","title":"Schemas Pydantic de Clea-API"},{"location":"schemas/#documentcreate","text":"Payload minimal pour cr\u00e9er un document (sans contenu). Champ Type Requis Alias Description title string Oui title Titre du document. theme string Oui theme Th\u00e8me du document. document_type string Oui documentType Type du document (PDF, TXT, etc.). publish_date date Oui publishDate Date de publication (YYYY-MM-DD). corpus_id string Non corpusId UUID du corpus (optionnel). type DocumentCreate = { title : string ; theme : string ; documentType : string ; publishDate : string ; // \"2025-05-01\" corpusId? : string ; } ```` --- ## ChunkCreate ** Payload ** pour cr\u00e9er un chunk ( fragment de texte et m\u00e9tadonn\u00e9es hi\u00e9rarchiques ). | Champ | Type | Requis | Alias | Description | | ----------------- | --------- | ------ | ---------------- | ------------------------------------------------------------ | | `id` | `number` | Non | `id` | Identifiant temporaire ( uniquement pour hi\u00e9rarchie interne ). | | `content` | `string` | Oui | `content` | Contenu textuel du chunk . | | `start_char` | `integer` | Oui | `startChar` | Position de d\u00e9but dans le texte source ( >= 0 ). | | `end_char` | `integer` | Oui | `endChar` | Position de fin dans le texte source ( > `startChar` ). | | `hierarchy_level` | `integer` | Oui | `hierarchyLevel` | Niveau hi\u00e9rarchique ( 0 \u2013 3 ). | | `parent_chunk_id` | `integer` | Non | `parentChunkId` | ID du chunk parent ( ou `null` ). | ```ts type ChunkCreate = { id?: number; content: string; startChar: number; endChar: number; hierarchyLevel: 0 | 1 | 2 | 3; parentChunkId?: number | null; }","title":"DocumentCreate"},{"location":"schemas/#documentwithchunks","text":"Payload complet pour POST /database/documents . Champ Type Requis Alias Description document DocumentCreate Oui document M\u00e9tadonn\u00e9es du document. chunks ChunkCreate[] Oui chunks Liste des chunks \u00e0 ins\u00e9rer. type DocumentWithChunks = { document : DocumentCreate ; chunks : ChunkCreate []; }","title":"DocumentWithChunks"},{"location":"schemas/#documentresponse","text":"R\u00e9ponse standard pour toutes les op\u00e9rations CRUD sur les documents. Champ Type Requis Alias Description id integer Oui id Identifiant du document. title string Oui title Titre du document. theme string Oui theme Th\u00e8me du document. document_type string Oui documentType Type du document. publish_date date Oui publishDate Date de publication. corpus_id string Non corpusId UUID du corpus. chunk_count integer Oui chunkCount Nombre de chunks associ\u00e9s (\u2265 0). index_needed boolean Oui indexNeeded true si un nouvel index est requis. type DocumentResponse = { id : number ; title : string ; theme : string ; documentType : string ; publishDate : string ; corpusId? : string ; chunkCount : number ; indexNeeded : boolean ; }","title":"DocumentResponse"},{"location":"schemas/#documentupdate","text":"Payload pour PUT /database/documents/{id} (mise \u00e0 jour). Champ Type Requis Alias Description id integer Oui id Identifiant du document \u00e0 mettre \u00e0 jour. title string Non title Nouveau titre. theme string Non theme Nouveau th\u00e8me. document_type string Non documentType Nouveau type de document. publish_date date Non publishDate Nouvelle date de publication. corpus_id string Non corpusId Nouvel UUID de corpus. type DocumentUpdate = { id : number ; title? : string ; theme? : string ; documentType? : string ; publishDate? : string ; corpusId? : string ; }","title":"DocumentUpdate"},{"location":"schemas/#updatewithchunks","text":"Payload pour PUT /database/documents/{id} avec ajout de chunks. Champ Type Requis Alias Description document DocumentUpdate Oui document M\u00e9tadonn\u00e9es \u00e0 mettre \u00e0 jour. new_chunks ChunkCreate[] Non newChunks Liste de nouveaux chunks \u00e0 ajouter. type UpdateWithChunks = { document : DocumentUpdate ; newChunks? : ChunkCreate []; }","title":"UpdateWithChunks"},{"location":"schemas/#hierarchicalcontext","text":"Parents (niveaux 0\u20132) renvoy\u00e9s quand hierarchical=true en recherche. Champ Type Requis Alias Description level_0 object or null Non level0 Chunk de section (lvl 0). level_1 object or null Non level1 Chunk de sous-section (lvl 1). level_2 object or null Non level2 Chunk de paragraphe (lvl 2).","title":"HierarchicalContext"},{"location":"schemas/#chunkresult","text":"Un chunk renvoy\u00e9 par POST /search/hybrid_search . Champ Type Requis Alias Description chunk_id integer Oui chunkId Identifiant du chunk. document_id integer Oui documentId ID du document parent. title string Oui title Titre du document parent. content string Oui content Contenu du chunk. theme string Oui theme Th\u00e8me du document. document_type string Oui documentType Type du document. publish_date date Oui publishDate Date de publication. score number Oui score Score de similarit\u00e9 (distance ou score). hierarchy_level integer Oui hierarchyLevel Niveau hi\u00e9rarchique (0\u20133). context HierarchicalContext or null Non context Contexte parent (facultatif).","title":"ChunkResult"},{"location":"schemas/#searchrequest","text":"Param\u00e8tres pour la recherche hybride. Combine la requ\u00eate textuelle avec des filtres de m\u00e9tadonn\u00e9es optionnels. Champ Type Requis Alias Description query string Oui query Requ\u00eate en langage naturel. top_k integer Non topK Nombre de r\u00e9sultats \u00e0 retourner (d\u00e9faut 10). theme string Non theme Filtre par th\u00e8me. document_type string Non documentType Filtre par type de document. start_date date Non startDate Date de d\u00e9but. end_date date Non endDate Date de fin. corpus_id integer Non corpusId ID du corpus. hierarchy_level integer Non hierarchyLevel Niveau hi\u00e9rarchique (0\u20132). hierarchical boolean Non hierarchical R\u00e9cup\u00e9rer le contexte hi\u00e9rarchique. filter_by_relevance boolean Non filterByRelevance Filtrer les r\u00e9sultats sous le seuil de pertinence. normalize_scores boolean Non normalizeScores Normaliser les scores entre 0 et 1.","title":"SearchRequest"},{"location":"schemas/#searchresponse","text":"R\u00e9ponse \u00e0 une requ\u00eate de recherche. Contient les r\u00e9sultats tri\u00e9s par pertinence avec m\u00e9tadonn\u00e9es et \u00e9valuation de confiance. Champ Type Requis Alias Description query string Oui query Requ\u00eate originale. top_k integer Oui topK Nombre de r\u00e9sultats demand\u00e9s. total_results integer Oui totalResults Nombre total de r\u00e9sultats trouv\u00e9s. results ChunkResult[] Oui results R\u00e9sultats de la recherche. confidence ConfidenceMetrics Non confidence M\u00e9triques de confiance sur les r\u00e9sultats. normalized boolean Non normalized Indique si les scores sont normalis\u00e9s (0\u20131). message string Non message Message informatif sur les r\u00e9sultats.","title":"SearchResponse"},{"location":"schemas/#confidencemetrics","text":"M\u00e9triques de confiance et statistiques pour \u00e9valuer la pertinence des r\u00e9sultats. Champ Type Requis Alias Description level float Oui level Niveau de confiance entre 0 et 1. message string Oui message Message explicatif sur la qualit\u00e9 des r\u00e9sultats. stats object Oui stats Statistiques sur les scores. Exemple de stats : { \"min\" : -5.89 , \"max\" : 1.23 , \"avg\" : -0.45 , \"median\" : -0.12 }","title":"ConfidenceMetrics"},{"location":"schemas/#indexstatus","text":"Statut d\u2019indexation d\u2019un corpus. Champ Type Requis Alias Description corpus_id string Oui corpusId UUID du corpus interrog\u00e9. index_exists boolean Oui indexExists true si l\u2019index physique existe en base. config_exists boolean Oui configExists true si la config SQLAlchemy ( IndexConfig ) existe. is_indexed boolean Oui isIndexed true si la config est marqu\u00e9e index\u00e9e. index_type string Non indexType Type d\u2019index ( ivfflat , hnsw ). chunk_count integer Oui chunkCount Nombre total de chunks dans le corpus. indexed_chunks integer Oui indexedChunks Nombre de chunks effectivement index\u00e9s. last_indexed date Non lastIndexed Date du dernier index (ou null ). Tous les mod\u00e8les se trouvent dans vectordb/src/schemas.py Derni\u00e8re mise \u00e0 jour : 03 mai 2025 .","title":"IndexStatus"},{"location":"Integration/askai/rag_js/","text":"Int\u00e9gration JavaScript / TypeScript \u2014 Ask-AI (RAG) Ce guide montre comment interroger le service RAG (Retrieve-and-Generate) via les endpoints /ask et /models Les exemples utilisent fetch natif (Node 18+) ou navigateur moderne. Pr\u00e9-requis npm install cross-fetch # polyfill fetch pour Node <18 npm install -D typescript # pour typer ```` D\u00e9finissez l\u2019URL : ``` ts export const CLEA_API = process.env.CLEA_API ?? \"http://localhost:8080\" ; Types TypeScript /** Payload pour /ask */ export interface AskRequest { query : string ; filters? : Record < string , any > ; theme? : string ; modelName? : string ; // ex: \"Qwen3-0.6B\" stream? : boolean ; promptType? : string ; // \"standard\" | \"summary\" | \"comparison\" enableThinking? : boolean ; } /** R\u00e9ponse standard non-stream\u00e9e */ export interface AskResponse { response : string ; context : any []; // tableau de chunks/metadata (voir vectordb SearchResponse) thinking? : string ; // pr\u00e9sent si enableThinking=true } /** Liste des mod\u00e8les disponibles */ export interface ModelsResponse { models : string []; } 1. R\u00e9cup\u00e9rer la liste des mod\u00e8les export async function getModels () : Promise < string [] > { const res = await fetch ( ` ${ CLEA_API } /ask/models` ); if ( ! res . ok ) throw new Error ( `Failed to fetch models: ${ await res . text () } ` ); const data : ModelsResponse = await res . json (); return data . models ; } /* Exemple */ const models = await getModels (); console . log ( \"Mod\u00e8les disponibles :\" , models ); 2. Poser une question (non-streaming) export async function askAI ( req : AskRequest ) : Promise < AskResponse > { const res = await fetch ( ` ${ CLEA_API } /ask` , { method : \"POST\" , headers : { \"Content-Type\" : \"application/json\" }, body : JSON.stringify ( req ), }); if ( ! res . ok ) throw new Error ( `ASK failed: ${ await res . text () } ` ); return res . json (); } /* Exemple */ const answer = await askAI ({ query : \"Qu'est-ce qu'un RAG system ?\" , theme : \"Informatique\" , modelName : \"Qwen3-0.6B\" , stream : false , promptType : \"standard\" , }); console . log ( \"R\u00e9ponse :\" , answer . response ); console . table ( answer . context ); \ud83d\udcdc Log r\u00e9el (extrait) { \"response\" : \"Un syst\u00e8me RAG combine la recherche documentaire\u2026\" , \"context\" : [ { \"chunkId\" : 5 , \"documentId\" : 3 , \"score\" : 0.12 , \u2026 }, \u2026 ] } 3. Poser une question (streaming SSE) export function askAIStream ( req : AskRequest , onData : ( chunk : any ) => void ) { // NOTE : vous pouvez utiliser EventSource si expos\u00e9 en SSE, // ou fetch + ReadableStream pour Node 18+. const evtSource = new EventSource ( ` ${ CLEA_API } /ask? ${ new URLSearchParams ({ stream : \"true\" } )}` , { withCredentials : true }); evtSource . onmessage = ( e ) => { if ( e . data === \"[DONE]\" ) { evtSource . close (); } else { onData ( JSON . parse ( e . data )); } }; } /* Exemple */ askAIStream ({ query : \"Explique RAG\" , stream : true }, ( chunk ) => { console . log ( \"\u2192\" , chunk ); }); Bonnes pratiques Timeout : mettez en place un timeout front-end pour les streams longue dur\u00e9e. Retry : impl\u00e9mentez un back-off sur 502 / 503 . Throttling : prot\u00e9gez le service contre trop de requ\u00eates simultan\u00e9es.","title":"RAG"},{"location":"Integration/askai/rag_js/#integration-javascript-typescript-ask-ai-rag","text":"Ce guide montre comment interroger le service RAG (Retrieve-and-Generate) via les endpoints /ask et /models Les exemples utilisent fetch natif (Node 18+) ou navigateur moderne.","title":"Int\u00e9gration JavaScript / TypeScript \u2014 Ask-AI (RAG)"},{"location":"Integration/askai/rag_js/#pre-requis","text":"npm install cross-fetch # polyfill fetch pour Node <18 npm install -D typescript # pour typer ```` D\u00e9finissez l\u2019URL : ``` ts export const CLEA_API = process.env.CLEA_API ?? \"http://localhost:8080\" ;","title":"Pr\u00e9-requis"},{"location":"Integration/askai/rag_js/#types-typescript","text":"/** Payload pour /ask */ export interface AskRequest { query : string ; filters? : Record < string , any > ; theme? : string ; modelName? : string ; // ex: \"Qwen3-0.6B\" stream? : boolean ; promptType? : string ; // \"standard\" | \"summary\" | \"comparison\" enableThinking? : boolean ; } /** R\u00e9ponse standard non-stream\u00e9e */ export interface AskResponse { response : string ; context : any []; // tableau de chunks/metadata (voir vectordb SearchResponse) thinking? : string ; // pr\u00e9sent si enableThinking=true } /** Liste des mod\u00e8les disponibles */ export interface ModelsResponse { models : string []; }","title":"Types TypeScript"},{"location":"Integration/askai/rag_js/#1-recuperer-la-liste-des-modeles","text":"export async function getModels () : Promise < string [] > { const res = await fetch ( ` ${ CLEA_API } /ask/models` ); if ( ! res . ok ) throw new Error ( `Failed to fetch models: ${ await res . text () } ` ); const data : ModelsResponse = await res . json (); return data . models ; } /* Exemple */ const models = await getModels (); console . log ( \"Mod\u00e8les disponibles :\" , models );","title":"1. R\u00e9cup\u00e9rer la liste des mod\u00e8les"},{"location":"Integration/askai/rag_js/#2-poser-une-question-non-streaming","text":"export async function askAI ( req : AskRequest ) : Promise < AskResponse > { const res = await fetch ( ` ${ CLEA_API } /ask` , { method : \"POST\" , headers : { \"Content-Type\" : \"application/json\" }, body : JSON.stringify ( req ), }); if ( ! res . ok ) throw new Error ( `ASK failed: ${ await res . text () } ` ); return res . json (); } /* Exemple */ const answer = await askAI ({ query : \"Qu'est-ce qu'un RAG system ?\" , theme : \"Informatique\" , modelName : \"Qwen3-0.6B\" , stream : false , promptType : \"standard\" , }); console . log ( \"R\u00e9ponse :\" , answer . response ); console . table ( answer . context ); \ud83d\udcdc Log r\u00e9el (extrait) { \"response\" : \"Un syst\u00e8me RAG combine la recherche documentaire\u2026\" , \"context\" : [ { \"chunkId\" : 5 , \"documentId\" : 3 , \"score\" : 0.12 , \u2026 }, \u2026 ] }","title":"2. Poser une question (non-streaming)"},{"location":"Integration/askai/rag_js/#3-poser-une-question-streaming-sse","text":"export function askAIStream ( req : AskRequest , onData : ( chunk : any ) => void ) { // NOTE : vous pouvez utiliser EventSource si expos\u00e9 en SSE, // ou fetch + ReadableStream pour Node 18+. const evtSource = new EventSource ( ` ${ CLEA_API } /ask? ${ new URLSearchParams ({ stream : \"true\" } )}` , { withCredentials : true }); evtSource . onmessage = ( e ) => { if ( e . data === \"[DONE]\" ) { evtSource . close (); } else { onData ( JSON . parse ( e . data )); } }; } /* Exemple */ askAIStream ({ query : \"Explique RAG\" , stream : true }, ( chunk ) => { console . log ( \"\u2192\" , chunk ); });","title":"3. Poser une question (streaming SSE)"},{"location":"Integration/askai/rag_js/#bonnes-pratiques","text":"Timeout : mettez en place un timeout front-end pour les streams longue dur\u00e9e. Retry : impl\u00e9mentez un back-off sur 502 / 503 . Throttling : prot\u00e9gez le service contre trop de requ\u00eates simultan\u00e9es.","title":"Bonnes pratiques"},{"location":"Integration/doc_loader/doc_loader_js/","text":"Int\u00e9gration JavaScript / TypeScript \u2014 Doc Loader Ce guide montre comment appeler l\u2019endpoint /doc_loader/upload-file pour uploader un fichier et r\u00e9cup\u00e9rer ses chunks extraits Les exemples utilisent fetch natif (Node 18+) ou navigateur moderne. Pour Node < 18, installez un polyfill : npm install cross-fetch Pr\u00e9-requis npm install cross-fetch # polyfill fetch pour Node <18 npm install -D typescript # si vous souhaitez typer ```` D\u00e9finissez l\u2019URL de l\u2019API : ``` ts export const CLEA_API = process.env.CLEA_API ?? \"http://localhost:8080\" ; Types TypeScript /** Chunk brut extrait d\u2019un document */ export interface ExtractedDocument { title : string ; content : string ; theme : string ; documentType : string ; publishDate : string ; // ISO (YYYY-MM-DD) embedding? : string | null ; } Uploader un fichier et r\u00e9cup\u00e9rer les chunks export async function uploadFile ( file : File , maxLength = 1000 , theme = \"Th\u00e8me g\u00e9n\u00e9rique\" ) : Promise < ExtractedDocument [] > { // Pr\u00e9parer le formulaire const form = new FormData (); form . append ( \"file\" , file ); form . append ( \"max_length\" , maxLength . toString ()); form . append ( \"theme\" , theme ); // Appeler l'endpoint const res = await fetch ( ` ${ CLEA_API } /doc_loader/upload-file` , { method : \"POST\" , body : form , }); if ( ! res . ok ) { throw new Error ( `Upload failed: ${ await res . text () } ` ); } return res . json (); } Exemple rapide const input = document . querySelector < HTMLInputElement > ( \"#fileInput\" ) ! ; const file = input . files ! [ 0 ]; const docs = await uploadFile ( file , 2000 , \"Finance\" ); console . log ( \"Chunks re\u00e7us :\" , docs . length ); docs . forEach (( d , i ) => console . log ( `# ${ i + 1 } ` , d . title , d . content . slice ( 0 , 50 ) + \"\u2026\" ) ); \ud83d\udcdc Log r\u00e9el (extrait) [ { \"title\" : \"demo.txt\" , \"content\" : \"Ligne 1\\nLigne 2\\n\u2026\" , \"theme\" : \"Finance\" , \"documentType\" : \"TXT\" , \"publishDate\" : \"2025-05-01\" , \"embedding\" : null }, { \"title\" : \"demo.txt (part 2)\" , \"content\" : \"Suite du document\u2026\" , \"theme\" : \"Finance\" , \"documentType\" : \"TXT\" , \"publishDate\" : \"2025-05-01\" , \"embedding\" : null } ] Bonnes pratiques Taille des chunks : ajustez max_length selon la granularit\u00e9 souhait\u00e9e. Nettoyage : supprimez les fichiers temporaires c\u00f4t\u00e9 client si n\u00e9cessaire. Gestion des erreurs : surveillez les codes 4xx / 5xx et affichez res.statusText ou await res.text() .","title":"Extraction de documents"},{"location":"Integration/doc_loader/doc_loader_js/#integration-javascript-typescript-doc-loader","text":"Ce guide montre comment appeler l\u2019endpoint /doc_loader/upload-file pour uploader un fichier et r\u00e9cup\u00e9rer ses chunks extraits Les exemples utilisent fetch natif (Node 18+) ou navigateur moderne. Pour Node < 18, installez un polyfill : npm install cross-fetch","title":"Int\u00e9gration JavaScript / TypeScript \u2014 Doc Loader"},{"location":"Integration/doc_loader/doc_loader_js/#pre-requis","text":"npm install cross-fetch # polyfill fetch pour Node <18 npm install -D typescript # si vous souhaitez typer ```` D\u00e9finissez l\u2019URL de l\u2019API : ``` ts export const CLEA_API = process.env.CLEA_API ?? \"http://localhost:8080\" ;","title":"Pr\u00e9-requis"},{"location":"Integration/doc_loader/doc_loader_js/#types-typescript","text":"/** Chunk brut extrait d\u2019un document */ export interface ExtractedDocument { title : string ; content : string ; theme : string ; documentType : string ; publishDate : string ; // ISO (YYYY-MM-DD) embedding? : string | null ; }","title":"Types TypeScript"},{"location":"Integration/doc_loader/doc_loader_js/#uploader-un-fichier-et-recuperer-les-chunks","text":"export async function uploadFile ( file : File , maxLength = 1000 , theme = \"Th\u00e8me g\u00e9n\u00e9rique\" ) : Promise < ExtractedDocument [] > { // Pr\u00e9parer le formulaire const form = new FormData (); form . append ( \"file\" , file ); form . append ( \"max_length\" , maxLength . toString ()); form . append ( \"theme\" , theme ); // Appeler l'endpoint const res = await fetch ( ` ${ CLEA_API } /doc_loader/upload-file` , { method : \"POST\" , body : form , }); if ( ! res . ok ) { throw new Error ( `Upload failed: ${ await res . text () } ` ); } return res . json (); }","title":"Uploader un fichier et r\u00e9cup\u00e9rer les chunks"},{"location":"Integration/doc_loader/doc_loader_js/#exemple-rapide","text":"const input = document . querySelector < HTMLInputElement > ( \"#fileInput\" ) ! ; const file = input . files ! [ 0 ]; const docs = await uploadFile ( file , 2000 , \"Finance\" ); console . log ( \"Chunks re\u00e7us :\" , docs . length ); docs . forEach (( d , i ) => console . log ( `# ${ i + 1 } ` , d . title , d . content . slice ( 0 , 50 ) + \"\u2026\" ) ); \ud83d\udcdc Log r\u00e9el (extrait) [ { \"title\" : \"demo.txt\" , \"content\" : \"Ligne 1\\nLigne 2\\n\u2026\" , \"theme\" : \"Finance\" , \"documentType\" : \"TXT\" , \"publishDate\" : \"2025-05-01\" , \"embedding\" : null }, { \"title\" : \"demo.txt (part 2)\" , \"content\" : \"Suite du document\u2026\" , \"theme\" : \"Finance\" , \"documentType\" : \"TXT\" , \"publishDate\" : \"2025-05-01\" , \"embedding\" : null } ]","title":"Exemple rapide"},{"location":"Integration/doc_loader/doc_loader_js/#bonnes-pratiques","text":"Taille des chunks : ajustez max_length selon la granularit\u00e9 souhait\u00e9e. Nettoyage : supprimez les fichiers temporaires c\u00f4t\u00e9 client si n\u00e9cessaire. Gestion des erreurs : surveillez les codes 4xx / 5xx et affichez res.statusText ou await res.text() .","title":"Bonnes pratiques"},{"location":"Integration/pipeline/pipeline_js/","text":"Int\u00e9gration JavaScript / TypeScript \u2014 Doc Loader Ce guide montre comment appeler l\u2019endpoint /doc_loader/upload-file pour uploader un fichier et r\u00e9cup\u00e9rer ses chunks extraits Les exemples utilisent fetch natif (Node 18+) ou navigateur moderne. Pour Node < 18, installez un polyfill : npm install cross-fetch Pr\u00e9-requis npm install cross-fetch # polyfill fetch pour Node <18 npm install -D typescript # si vous souhaitez typer ```` D\u00e9finissez l\u2019URL de l\u2019API : ``` ts export const CLEA_API = process.env.CLEA_API ?? \"http://localhost:8080\" ; Types TypeScript /** Chunk brut extrait d\u2019un document */ export interface ExtractedDocument { title : string ; content : string ; theme : string ; documentType : string ; publishDate : string ; // ISO (YYYY-MM-DD) embedding? : string | null ; } Uploader un fichier et r\u00e9cup\u00e9rer les chunks export async function uploadFile ( file : File , maxLength = 1000 , theme = \"Th\u00e8me g\u00e9n\u00e9rique\" ) : Promise < ExtractedDocument [] > { // Pr\u00e9parer le formulaire const form = new FormData (); form . append ( \"file\" , file ); form . append ( \"max_length\" , maxLength . toString ()); form . append ( \"theme\" , theme ); // Appeler l'endpoint const res = await fetch ( ` ${ CLEA_API } /doc_loader/upload-file` , { method : \"POST\" , body : form , }); if ( ! res . ok ) { throw new Error ( `Upload failed: ${ await res . text () } ` ); } return res . json (); } Exemple rapide const input = document . querySelector < HTMLInputElement > ( \"#fileInput\" ) ! ; const file = input . files ! [ 0 ]; const docs = await uploadFile ( file , 2000 , \"Finance\" ); console . log ( \"Chunks re\u00e7us :\" , docs . length ); docs . forEach (( d , i ) => console . log ( `# ${ i + 1 } ` , d . title , d . content . slice ( 0 , 50 ) + \"\u2026\" ) ); \ud83d\udcdc Log r\u00e9el (extrait) [ { \"title\" : \"demo.txt\" , \"content\" : \"Ligne 1\\nLigne 2\\n\u2026\" , \"theme\" : \"Finance\" , \"documentType\" : \"TXT\" , \"publishDate\" : \"2025-05-01\" , \"embedding\" : null }, { \"title\" : \"demo.txt (part 2)\" , \"content\" : \"Suite du document\u2026\" , \"theme\" : \"Finance\" , \"documentType\" : \"TXT\" , \"publishDate\" : \"2025-05-01\" , \"embedding\" : null } ] Bonnes pratiques Taille des chunks : ajustez max_length selon la granularit\u00e9 souhait\u00e9e. Nettoyage : supprimez les fichiers temporaires c\u00f4t\u00e9 client si n\u00e9cessaire. Gestion des erreurs : surveillez les codes 4xx / 5xx et affichez res.statusText ou await res.text() .","title":"Pipeline end-to-end"},{"location":"Integration/pipeline/pipeline_js/#integration-javascript-typescript-doc-loader","text":"Ce guide montre comment appeler l\u2019endpoint /doc_loader/upload-file pour uploader un fichier et r\u00e9cup\u00e9rer ses chunks extraits Les exemples utilisent fetch natif (Node 18+) ou navigateur moderne. Pour Node < 18, installez un polyfill : npm install cross-fetch","title":"Int\u00e9gration JavaScript / TypeScript \u2014 Doc Loader"},{"location":"Integration/pipeline/pipeline_js/#pre-requis","text":"npm install cross-fetch # polyfill fetch pour Node <18 npm install -D typescript # si vous souhaitez typer ```` D\u00e9finissez l\u2019URL de l\u2019API : ``` ts export const CLEA_API = process.env.CLEA_API ?? \"http://localhost:8080\" ;","title":"Pr\u00e9-requis"},{"location":"Integration/pipeline/pipeline_js/#types-typescript","text":"/** Chunk brut extrait d\u2019un document */ export interface ExtractedDocument { title : string ; content : string ; theme : string ; documentType : string ; publishDate : string ; // ISO (YYYY-MM-DD) embedding? : string | null ; }","title":"Types TypeScript"},{"location":"Integration/pipeline/pipeline_js/#uploader-un-fichier-et-recuperer-les-chunks","text":"export async function uploadFile ( file : File , maxLength = 1000 , theme = \"Th\u00e8me g\u00e9n\u00e9rique\" ) : Promise < ExtractedDocument [] > { // Pr\u00e9parer le formulaire const form = new FormData (); form . append ( \"file\" , file ); form . append ( \"max_length\" , maxLength . toString ()); form . append ( \"theme\" , theme ); // Appeler l'endpoint const res = await fetch ( ` ${ CLEA_API } /doc_loader/upload-file` , { method : \"POST\" , body : form , }); if ( ! res . ok ) { throw new Error ( `Upload failed: ${ await res . text () } ` ); } return res . json (); }","title":"Uploader un fichier et r\u00e9cup\u00e9rer les chunks"},{"location":"Integration/pipeline/pipeline_js/#exemple-rapide","text":"const input = document . querySelector < HTMLInputElement > ( \"#fileInput\" ) ! ; const file = input . files ! [ 0 ]; const docs = await uploadFile ( file , 2000 , \"Finance\" ); console . log ( \"Chunks re\u00e7us :\" , docs . length ); docs . forEach (( d , i ) => console . log ( `# ${ i + 1 } ` , d . title , d . content . slice ( 0 , 50 ) + \"\u2026\" ) ); \ud83d\udcdc Log r\u00e9el (extrait) [ { \"title\" : \"demo.txt\" , \"content\" : \"Ligne 1\\nLigne 2\\n\u2026\" , \"theme\" : \"Finance\" , \"documentType\" : \"TXT\" , \"publishDate\" : \"2025-05-01\" , \"embedding\" : null }, { \"title\" : \"demo.txt (part 2)\" , \"content\" : \"Suite du document\u2026\" , \"theme\" : \"Finance\" , \"documentType\" : \"TXT\" , \"publishDate\" : \"2025-05-01\" , \"embedding\" : null } ]","title":"Exemple rapide"},{"location":"Integration/pipeline/pipeline_js/#bonnes-pratiques","text":"Taille des chunks : ajustez max_length selon la granularit\u00e9 souhait\u00e9e. Nettoyage : supprimez les fichiers temporaires c\u00f4t\u00e9 client si n\u00e9cessaire. Gestion des erreurs : surveillez les codes 4xx / 5xx et affichez res.statusText ou await res.text() .","title":"Bonnes pratiques"},{"location":"Integration/stats/stats_computer_js/","text":"Biblioth\u00e8que de calcul de statistiques ( stats_computer ) Ce module centralise les calculs de m\u00e9triques pour l\u2019interface d\u2019administration de Cl\u00e9a-API. Il fournit : Des schemas Pydantic pour la validation et la s\u00e9rialisation des r\u00e9sultats. Une classe StatsComputer pour l\u2019ex\u00e9cution des calculs. 1. Schemas Pydantic Tous les mod\u00e8les utilisent la configuration CamelConfig qui g\u00e9n\u00e8re/transcode automatiquement les cl\u00e9s snake_case \u2194 camelCase 1.1. DocumentStats Repr\u00e9sente les m\u00e9triques sur les documents index\u00e9s. Attribut Type Description totalCount int Nombre total de documents byTheme Dict[str,int] R\u00e9partition des documents par th\u00e8me byType Dict[str,int] R\u00e9partition des documents par type recentlyAdded int Nombre de documents ajout\u00e9s au cours des 30 derniers jours percentChange float \u00c9volution (%) du nombre ajouts par rapport au total 1.2. SearchStats M\u00e9triques sur l\u2019historique des recherches. Attribut Type Description totalCount int Nombre total de recherches lastMonthCount int Nombre de recherches au cours du dernier mois percentChange float \u00c9volution (%) entre ce mois et le mois pr\u00e9c\u00e9dent topQueries List[Dict[str,Any]] Liste des requ\u00eates les plus populaires {query, count} 1.3. SystemStats Vue d\u2019ensemble de la qualit\u00e9 et de l\u2019\u00e9tat du syst\u00e8me. Attribut Type Description satisfaction float % de recherches jug\u00e9es satisfaisantes (confiance \u2265 0.7) avgConfidence float Confiance moyenne sur les recherches du dernier mois percentChange float \u00c9volution (%) de la confiance indexedCorpora int Nombre de corpus d\u00e9j\u00e0 index\u00e9s totalCorpora int Nombre total de corpus 1.4. DashboardStats Aggr\u00e9gation de DocumentStats , SearchStats et SystemStats . class DashboardStats ( BaseModel ): document_stats : DocumentStats search_stats : SearchStats system_stats : SystemStats ```` --- ## 2. Classe `StatsComputer` Toutes les m\u00e9thodes retournent les objets Pydantic ci - dessus en interrogeant la base de donn\u00e9es via SQLAlchemy . Initialisation et session : ``` python from stats.src.stats_src_compute import StatsComputer stats_computer = StatsComputer () 2.1. compute_document_stats(skip: int = 0, limit: int = 100) \u2192 DocumentStats Calcule : Nombre total de documents (pagin\u00e9). R\u00e9partition par th\u00e8me et type. Nombre de documents ajout\u00e9s dans les 30 derniers jours. Pourcentage d\u2019\u00e9volution. doc_stats : DocumentStats = stats_computer . compute_document_stats ( skip = 0 , limit = 50 ) 2.2. compute_search_stats(skip: int = 0, limit: int = 100) \u2192 SearchStats Calcule : Total des recherches. Recherches du dernier mois. % d\u2019\u00e9volution vs mois pr\u00e9c\u00e9dent. Top 10 des requ\u00eates les plus populaires. search_stats : SearchStats = stats_computer . compute_search_stats ( skip = 0 , limit = 200 ) 2.3. compute_system_stats() \u2192 SystemStats Calcule : Confiance moyenne et satisfaction (confiance \u2265 0.7). % d\u2019\u00e9volution de la confiance. Nombre de corpus index\u00e9s vs total. sys_stats : SystemStats = stats_computer . compute_system_stats () 2.4. compute_all_stats() \u2192 DashboardStats Aggr\u00e8ge les trois calculs ci-dessus : dashboard : DashboardStats = stats_computer . compute_all_stats () 3. Bonnes pratiques Pagination : Jouez sur skip / limit pour ne pas surcharger la m\u00e9moire. Cache : En production, stockez les r\u00e9sultats et rafra\u00eechissez-les p\u00e9riodiquement via un job CRON plut\u00f4t que d\u2019appeler ces m\u00e9thodes \u00e0 chaque requ\u00eate. Surveillance : Loggez les appels et la dur\u00e9e d\u2019ex\u00e9cution ( StatsComputer.logger ) pour d\u00e9tecter les goulets d\u2019\u00e9tranglement.","title":"Statistiques"},{"location":"Integration/stats/stats_computer_js/#bibliotheque-de-calcul-de-statistiques-stats_computer","text":"Ce module centralise les calculs de m\u00e9triques pour l\u2019interface d\u2019administration de Cl\u00e9a-API. Il fournit : Des schemas Pydantic pour la validation et la s\u00e9rialisation des r\u00e9sultats. Une classe StatsComputer pour l\u2019ex\u00e9cution des calculs.","title":"Biblioth\u00e8que de calcul de statistiques (stats_computer)"},{"location":"Integration/stats/stats_computer_js/#1-schemas-pydantic","text":"Tous les mod\u00e8les utilisent la configuration CamelConfig qui g\u00e9n\u00e8re/transcode automatiquement les cl\u00e9s snake_case \u2194 camelCase","title":"1. Schemas Pydantic"},{"location":"Integration/stats/stats_computer_js/#11-documentstats","text":"Repr\u00e9sente les m\u00e9triques sur les documents index\u00e9s. Attribut Type Description totalCount int Nombre total de documents byTheme Dict[str,int] R\u00e9partition des documents par th\u00e8me byType Dict[str,int] R\u00e9partition des documents par type recentlyAdded int Nombre de documents ajout\u00e9s au cours des 30 derniers jours percentChange float \u00c9volution (%) du nombre ajouts par rapport au total","title":"1.1. DocumentStats"},{"location":"Integration/stats/stats_computer_js/#12-searchstats","text":"M\u00e9triques sur l\u2019historique des recherches. Attribut Type Description totalCount int Nombre total de recherches lastMonthCount int Nombre de recherches au cours du dernier mois percentChange float \u00c9volution (%) entre ce mois et le mois pr\u00e9c\u00e9dent topQueries List[Dict[str,Any]] Liste des requ\u00eates les plus populaires {query, count}","title":"1.2. SearchStats"},{"location":"Integration/stats/stats_computer_js/#13-systemstats","text":"Vue d\u2019ensemble de la qualit\u00e9 et de l\u2019\u00e9tat du syst\u00e8me. Attribut Type Description satisfaction float % de recherches jug\u00e9es satisfaisantes (confiance \u2265 0.7) avgConfidence float Confiance moyenne sur les recherches du dernier mois percentChange float \u00c9volution (%) de la confiance indexedCorpora int Nombre de corpus d\u00e9j\u00e0 index\u00e9s totalCorpora int Nombre total de corpus","title":"1.3. SystemStats"},{"location":"Integration/stats/stats_computer_js/#14-dashboardstats","text":"Aggr\u00e9gation de DocumentStats , SearchStats et SystemStats . class DashboardStats ( BaseModel ): document_stats : DocumentStats search_stats : SearchStats system_stats : SystemStats ```` --- ## 2. Classe `StatsComputer` Toutes les m\u00e9thodes retournent les objets Pydantic ci - dessus en interrogeant la base de donn\u00e9es via SQLAlchemy . Initialisation et session : ``` python from stats.src.stats_src_compute import StatsComputer stats_computer = StatsComputer ()","title":"1.4. DashboardStats"},{"location":"Integration/stats/stats_computer_js/#21-compute_document_statsskip-int-0-limit-int-100-documentstats","text":"Calcule : Nombre total de documents (pagin\u00e9). R\u00e9partition par th\u00e8me et type. Nombre de documents ajout\u00e9s dans les 30 derniers jours. Pourcentage d\u2019\u00e9volution. doc_stats : DocumentStats = stats_computer . compute_document_stats ( skip = 0 , limit = 50 )","title":"2.1. compute_document_stats(skip: int = 0, limit: int = 100) \u2192 DocumentStats"},{"location":"Integration/stats/stats_computer_js/#22-compute_search_statsskip-int-0-limit-int-100-searchstats","text":"Calcule : Total des recherches. Recherches du dernier mois. % d\u2019\u00e9volution vs mois pr\u00e9c\u00e9dent. Top 10 des requ\u00eates les plus populaires. search_stats : SearchStats = stats_computer . compute_search_stats ( skip = 0 , limit = 200 )","title":"2.2. compute_search_stats(skip: int = 0, limit: int = 100) \u2192 SearchStats"},{"location":"Integration/stats/stats_computer_js/#23-compute_system_stats-systemstats","text":"Calcule : Confiance moyenne et satisfaction (confiance \u2265 0.7). % d\u2019\u00e9volution de la confiance. Nombre de corpus index\u00e9s vs total. sys_stats : SystemStats = stats_computer . compute_system_stats ()","title":"2.3. compute_system_stats() \u2192 SystemStats"},{"location":"Integration/stats/stats_computer_js/#24-compute_all_stats-dashboardstats","text":"Aggr\u00e8ge les trois calculs ci-dessus : dashboard : DashboardStats = stats_computer . compute_all_stats ()","title":"2.4. compute_all_stats() \u2192 DashboardStats"},{"location":"Integration/stats/stats_computer_js/#3-bonnes-pratiques","text":"Pagination : Jouez sur skip / limit pour ne pas surcharger la m\u00e9moire. Cache : En production, stockez les r\u00e9sultats et rafra\u00eechissez-les p\u00e9riodiquement via un job CRON plut\u00f4t que d\u2019appeler ces m\u00e9thodes \u00e0 chaque requ\u00eate. Surveillance : Loggez les appels et la dur\u00e9e d\u2019ex\u00e9cution ( StatsComputer.logger ) pour d\u00e9tecter les goulets d\u2019\u00e9tranglement.","title":"3. Bonnes pratiques"},{"location":"Integration/vectordb/crud_js/","text":"Int\u00e9gration JavaScript / TypeScript \u2013 Vectordb CRUD Ce guide montre comment appeler les endpoints CRUD de Vectordb (/database) depuis un client JavaScript ou TypeScript, en s\u2019appuyant sur les sch\u00e9mas Pydantic et les routes FastAPI 1. Configuration du client import axios from 'axios' ; export const API_URL = process . env . CLEA_API ?? 'http://localhost:8080' ; export const dbClient = axios . create ({ baseURL : ` ${ API_URL } /database` , headers : { 'Content-Type' : 'application/json' }, }); ```` --- ## 2. Types TypeScript ```ts /* sch\u00e9mas HTTP (Pydantic \u2192 TS) */ export interface DocumentCreate { title: string; theme: string; document_type: string; publish_date: string; // ISO (YYYY-MM-DD) corpus_id?: string | null; } export interface ChunkCreate { id?: number; content: string; start_char: number; end_char: number; hierarchy_level: number; parent_chunk_id?: number | null; } export interface DocumentWithChunks { document: DocumentCreate; chunks: ChunkCreate[]; } export interface DocumentResponse { id: number; title: string; theme: string; document_type: string; publish_date: string; corpus_id: string | null; chunk_count: number; index_needed: boolean; } export interface UpdateWithChunks { document: { id: number } & Partial<DocumentCreate>; new_chunks?: ChunkCreate[]; } 3. Ajouter un document + chunks export async function addDocument ( payload : DocumentWithChunks ) : Promise < DocumentResponse > { const res = await dbClient . post ( '/documents' , payload ); if ( ! res . status . toString (). startsWith ( '2' )) { throw new Error ( `Add failed: ${ res . status } ${ res . statusText } ` ); } return res . data as DocumentResponse ; } // Exemple d\u2019usage const payload : DocumentWithChunks = { document : { title : 'Rapport Q2 2025' , theme : 'Finance' , document_type : 'PDF' , publish_date : '2025-06-30' , }, chunks : [ { content : 'R\u00e9sum\u00e9\u2026' , start_char : 0 , end_char : 100 , hierarchy_level : 3 }, { content : 'D\u00e9tails\u2026' , start_char : 101 , end_char : 300 , hierarchy_level : 3 }, ], }; addDocument ( payload ) . then ( doc => console . log ( 'Cr\u00e9\u00e9 ID \u279c' , doc . id , 'avec' , doc . chunk_count , 'chunks' )) . catch ( console . error ); 4. Mettre \u00e0 jour un document export async function updateDocument ( id : number , patch : Partial < DocumentCreate > , newChunks : ChunkCreate [] = [] ) : Promise < DocumentResponse > { const payload : UpdateWithChunks = { document : { id , ... patch }, new_chunks : newChunks.length ? newChunks : undefined , }; const res = await dbClient . put ( `/documents/ ${ id } ` , payload ); if ( ! res . status . toString (). startsWith ( '2' )) { throw new Error ( `Update failed: ${ res . status } ` ); } return res . data as DocumentResponse ; } // Exemple d\u2019usage updateDocument ( 1 , { title : 'Titre mis \u00e0 jour' }, [ { content : 'Nouveau chunk' , start_char : 300 , end_char : 350 , hierarchy_level : 3 }, ]) . then ( doc => console . log ( 'Mis \u00e0 jour \u279c' , doc )) . catch ( console . error ); 5. Supprimer un document ou ses chunks // Supprimer tout le document export async function deleteDocument ( id : number ) : Promise < void > { const res = await dbClient . delete ( `/documents/ ${ id } ` ); if ( res . status !== 200 ) throw new Error ( `Delete doc failed: ${ res . status } ` ); } // Supprimer des chunks sp\u00e9cifiques export async function deleteChunks ( documentId : number , chunkIds : number [] ) : Promise < { document_id : number ; chunks_deleted : number ; remaining_chunks : number } > { const params = chunkIds . map ( id => `chunk_ids= ${ id } ` ). join ( '&' ); const res = await dbClient . delete ( `/documents/ ${ documentId } /chunks? ${ params } ` ); if ( res . status !== 200 ) throw new Error ( `Delete chunks failed: ${ res . status } ` ); return res . data ; } // Exemple d\u2019usage deleteChunks ( 2 , [ 5 , 6 ]) . then ( info => console . log ( 'Chunks supprim\u00e9s \u279c' , info . chunks_deleted )) . catch ( console . error ); 6. Lister & r\u00e9cup\u00e9rer // Lister avec pagination et filtres export async function listDocuments ( filters : { theme? : string ; document_type? : string ; corpus_id? : string }, skip = 0 , limit = 50 ) : Promise < DocumentResponse [] > { const res = await dbClient . get ( '/documents' , { params : { ... filters , skip , limit } }); return res . data as DocumentResponse []; } // R\u00e9cup\u00e9rer un document par ID export async function getDocument ( id : number ) : Promise < DocumentResponse > { const res = await dbClient . get ( `/documents/ ${ id } ` ); return res . data as DocumentResponse ; } // R\u00e9cup\u00e9rer les chunks d\u2019un document export async function getDocumentChunks ( documentId : number , params : { hierarchyLevel? : number ; parentChunkId? : number ; skip? : number ; limit? : number } = {} ) : Promise < ChunkCreate [] > { const res = await dbClient . get ( `/documents/ ${ documentId } /chunks` , { params : { ... params } }); return res . data as ChunkCreate []; } // Exemple d\u2019usage listDocuments ({ theme : 'Finance' }, 0 , 10 ) . then ( docs => docs . forEach ( d => console . log ( d . id , d . title ))) . catch ( console . error ); Pour plus de d\u00e9tails sur les sch\u00e9mas et les r\u00e9ponses, voir src/schemas.py et api/database_endpoint.py Guide g\u00e9n\u00e9r\u00e9 automatiquement \u2014 05 mai 2025","title":"Op\u00e9rations CRUD"},{"location":"Integration/vectordb/crud_js/#integration-javascript-typescript-vectordb-crud","text":"Ce guide montre comment appeler les endpoints CRUD de Vectordb (/database) depuis un client JavaScript ou TypeScript, en s\u2019appuyant sur les sch\u00e9mas Pydantic et les routes FastAPI","title":"Int\u00e9gration JavaScript / TypeScript \u2013 Vectordb CRUD"},{"location":"Integration/vectordb/crud_js/#1-configuration-du-client","text":"import axios from 'axios' ; export const API_URL = process . env . CLEA_API ?? 'http://localhost:8080' ; export const dbClient = axios . create ({ baseURL : ` ${ API_URL } /database` , headers : { 'Content-Type' : 'application/json' }, }); ```` --- ## 2. Types TypeScript ```ts /* sch\u00e9mas HTTP (Pydantic \u2192 TS) */ export interface DocumentCreate { title: string; theme: string; document_type: string; publish_date: string; // ISO (YYYY-MM-DD) corpus_id?: string | null; } export interface ChunkCreate { id?: number; content: string; start_char: number; end_char: number; hierarchy_level: number; parent_chunk_id?: number | null; } export interface DocumentWithChunks { document: DocumentCreate; chunks: ChunkCreate[]; } export interface DocumentResponse { id: number; title: string; theme: string; document_type: string; publish_date: string; corpus_id: string | null; chunk_count: number; index_needed: boolean; } export interface UpdateWithChunks { document: { id: number } & Partial<DocumentCreate>; new_chunks?: ChunkCreate[]; }","title":"1. Configuration du client"},{"location":"Integration/vectordb/crud_js/#3-ajouter-un-document-chunks","text":"export async function addDocument ( payload : DocumentWithChunks ) : Promise < DocumentResponse > { const res = await dbClient . post ( '/documents' , payload ); if ( ! res . status . toString (). startsWith ( '2' )) { throw new Error ( `Add failed: ${ res . status } ${ res . statusText } ` ); } return res . data as DocumentResponse ; } // Exemple d\u2019usage const payload : DocumentWithChunks = { document : { title : 'Rapport Q2 2025' , theme : 'Finance' , document_type : 'PDF' , publish_date : '2025-06-30' , }, chunks : [ { content : 'R\u00e9sum\u00e9\u2026' , start_char : 0 , end_char : 100 , hierarchy_level : 3 }, { content : 'D\u00e9tails\u2026' , start_char : 101 , end_char : 300 , hierarchy_level : 3 }, ], }; addDocument ( payload ) . then ( doc => console . log ( 'Cr\u00e9\u00e9 ID \u279c' , doc . id , 'avec' , doc . chunk_count , 'chunks' )) . catch ( console . error );","title":"3. Ajouter un document + chunks"},{"location":"Integration/vectordb/crud_js/#4-mettre-a-jour-un-document","text":"export async function updateDocument ( id : number , patch : Partial < DocumentCreate > , newChunks : ChunkCreate [] = [] ) : Promise < DocumentResponse > { const payload : UpdateWithChunks = { document : { id , ... patch }, new_chunks : newChunks.length ? newChunks : undefined , }; const res = await dbClient . put ( `/documents/ ${ id } ` , payload ); if ( ! res . status . toString (). startsWith ( '2' )) { throw new Error ( `Update failed: ${ res . status } ` ); } return res . data as DocumentResponse ; } // Exemple d\u2019usage updateDocument ( 1 , { title : 'Titre mis \u00e0 jour' }, [ { content : 'Nouveau chunk' , start_char : 300 , end_char : 350 , hierarchy_level : 3 }, ]) . then ( doc => console . log ( 'Mis \u00e0 jour \u279c' , doc )) . catch ( console . error );","title":"4. Mettre \u00e0 jour un document"},{"location":"Integration/vectordb/crud_js/#5-supprimer-un-document-ou-ses-chunks","text":"// Supprimer tout le document export async function deleteDocument ( id : number ) : Promise < void > { const res = await dbClient . delete ( `/documents/ ${ id } ` ); if ( res . status !== 200 ) throw new Error ( `Delete doc failed: ${ res . status } ` ); } // Supprimer des chunks sp\u00e9cifiques export async function deleteChunks ( documentId : number , chunkIds : number [] ) : Promise < { document_id : number ; chunks_deleted : number ; remaining_chunks : number } > { const params = chunkIds . map ( id => `chunk_ids= ${ id } ` ). join ( '&' ); const res = await dbClient . delete ( `/documents/ ${ documentId } /chunks? ${ params } ` ); if ( res . status !== 200 ) throw new Error ( `Delete chunks failed: ${ res . status } ` ); return res . data ; } // Exemple d\u2019usage deleteChunks ( 2 , [ 5 , 6 ]) . then ( info => console . log ( 'Chunks supprim\u00e9s \u279c' , info . chunks_deleted )) . catch ( console . error );","title":"5. Supprimer un document ou ses chunks"},{"location":"Integration/vectordb/crud_js/#6-lister-recuperer","text":"// Lister avec pagination et filtres export async function listDocuments ( filters : { theme? : string ; document_type? : string ; corpus_id? : string }, skip = 0 , limit = 50 ) : Promise < DocumentResponse [] > { const res = await dbClient . get ( '/documents' , { params : { ... filters , skip , limit } }); return res . data as DocumentResponse []; } // R\u00e9cup\u00e9rer un document par ID export async function getDocument ( id : number ) : Promise < DocumentResponse > { const res = await dbClient . get ( `/documents/ ${ id } ` ); return res . data as DocumentResponse ; } // R\u00e9cup\u00e9rer les chunks d\u2019un document export async function getDocumentChunks ( documentId : number , params : { hierarchyLevel? : number ; parentChunkId? : number ; skip? : number ; limit? : number } = {} ) : Promise < ChunkCreate [] > { const res = await dbClient . get ( `/documents/ ${ documentId } /chunks` , { params : { ... params } }); return res . data as ChunkCreate []; } // Exemple d\u2019usage listDocuments ({ theme : 'Finance' }, 0 , 10 ) . then ( docs => docs . forEach ( d => console . log ( d . id , d . title ))) . catch ( console . error ); Pour plus de d\u00e9tails sur les sch\u00e9mas et les r\u00e9ponses, voir src/schemas.py et api/database_endpoint.py Guide g\u00e9n\u00e9r\u00e9 automatiquement \u2014 05 mai 2025","title":"6. Lister &amp; r\u00e9cup\u00e9rer"},{"location":"Integration/vectordb/index_js/","text":"Int\u00e9gration JavaScript / TypeScript \u2013 Gestion des index vectoriels Ce guide explique comment g\u00e9rer les index IVFFLAT / HNSW de Vectordb depuis JavaScript ou TypeScript, en appelant directement les endpoints FastAPI 1. Configuration du client import axios from 'axios' ; export const API_URL = process . env . CLEA_API ?? 'http://localhost:8080' ; export const idxClient = axios . create ({ baseURL : ` ${ API_URL } /index` , headers : { 'Content-Type' : 'application/json' }, }); ```` --- ## 2. Types TypeScript ```ts export interface IndexSimpleResult { status: 'ok' | 'error'; message: string; } export interface IndexStatus { corpusId: string; indexExists: boolean; configExists: boolean; isIndexed: boolean; indexType: string; chunkCount: number; indexedChunks: number; lastIndexed: string | null; } export interface AllIndexesStatus { [corpusId: string]: IndexStatus; } 3. Cr\u00e9er un index pour un corpus export async function createIndex ( corpusId : string ) : Promise < IndexSimpleResult > { const res = await idxClient . post ( `/create-index/ ${ corpusId } ` ); if ( ! res . status . toString (). startsWith ( '2' )) { throw new Error ( `Create index failed: ${ res . status } ` ); } return res . data as IndexSimpleResult ; } // Exemple d\u2019usage createIndex ( 'ae12f3e4-5678-90ab-cdef-1234567890ab' ) . then ( r => console . log ( 'Index cr\u00e9\u00e9:' , r . message )) . catch ( console . error ); 4. Supprimer un index export async function dropIndex ( corpusId : string ) : Promise < IndexSimpleResult > { const res = await idxClient . delete ( `/drop-index/ ${ corpusId } ` ); if ( ! res . status . toString (). startsWith ( '2' )) { throw new Error ( `Drop index failed: ${ res . status } ` ); } return res . data as IndexSimpleResult ; } // Exemple d\u2019usage dropIndex ( 'ae12f3e4-5678-90ab-cdef-1234567890ab' ) . then ( r => console . log ( 'Index supprim\u00e9:' , r . message )) . catch ( console . error ); 5. V\u00e9rifier l\u2019\u00e9tat d\u2019un index export async function getIndexStatus ( corpusId : string ) : Promise < IndexStatus > { const res = await idxClient . get < IndexStatus > ( `/index-status/ ${ corpusId } ` ); return res . data ; } // Exemple d\u2019usage getIndexStatus ( 'ae12f3e4-5678-90ab-cdef-1234567890ab' ) . then ( status => { console . log ( `Index exists: ${ status . isIndexed } , type: ${ status . indexType } ` ); }) . catch ( console . error ); 6. V\u00e9rifier tous les index export async function getAllIndexes () : Promise < AllIndexesStatus > { const res = await idxClient . get ( '/indexes' ); return res . data as AllIndexesStatus ; } // Exemple d\u2019usage getAllIndexes () . then ( all => console . table ( all )) . catch ( console . error ); Pour plus de d\u00e9tails, voir les routes d\u00e9finies dans api/index_endpoint.py et les DTO Pydantic dans src/schemas.py Guide g\u00e9n\u00e9r\u00e9 automatiquement \u2014 05 mai 2025","title":"Indexation"},{"location":"Integration/vectordb/index_js/#integration-javascript-typescript-gestion-des-index-vectoriels","text":"Ce guide explique comment g\u00e9rer les index IVFFLAT / HNSW de Vectordb depuis JavaScript ou TypeScript, en appelant directement les endpoints FastAPI","title":"Int\u00e9gration JavaScript / TypeScript \u2013 Gestion des index vectoriels"},{"location":"Integration/vectordb/index_js/#1-configuration-du-client","text":"import axios from 'axios' ; export const API_URL = process . env . CLEA_API ?? 'http://localhost:8080' ; export const idxClient = axios . create ({ baseURL : ` ${ API_URL } /index` , headers : { 'Content-Type' : 'application/json' }, }); ```` --- ## 2. Types TypeScript ```ts export interface IndexSimpleResult { status: 'ok' | 'error'; message: string; } export interface IndexStatus { corpusId: string; indexExists: boolean; configExists: boolean; isIndexed: boolean; indexType: string; chunkCount: number; indexedChunks: number; lastIndexed: string | null; } export interface AllIndexesStatus { [corpusId: string]: IndexStatus; }","title":"1. Configuration du client"},{"location":"Integration/vectordb/index_js/#3-creer-un-index-pour-un-corpus","text":"export async function createIndex ( corpusId : string ) : Promise < IndexSimpleResult > { const res = await idxClient . post ( `/create-index/ ${ corpusId } ` ); if ( ! res . status . toString (). startsWith ( '2' )) { throw new Error ( `Create index failed: ${ res . status } ` ); } return res . data as IndexSimpleResult ; } // Exemple d\u2019usage createIndex ( 'ae12f3e4-5678-90ab-cdef-1234567890ab' ) . then ( r => console . log ( 'Index cr\u00e9\u00e9:' , r . message )) . catch ( console . error );","title":"3. Cr\u00e9er un index pour un corpus"},{"location":"Integration/vectordb/index_js/#4-supprimer-un-index","text":"export async function dropIndex ( corpusId : string ) : Promise < IndexSimpleResult > { const res = await idxClient . delete ( `/drop-index/ ${ corpusId } ` ); if ( ! res . status . toString (). startsWith ( '2' )) { throw new Error ( `Drop index failed: ${ res . status } ` ); } return res . data as IndexSimpleResult ; } // Exemple d\u2019usage dropIndex ( 'ae12f3e4-5678-90ab-cdef-1234567890ab' ) . then ( r => console . log ( 'Index supprim\u00e9:' , r . message )) . catch ( console . error );","title":"4. Supprimer un index"},{"location":"Integration/vectordb/index_js/#5-verifier-letat-dun-index","text":"export async function getIndexStatus ( corpusId : string ) : Promise < IndexStatus > { const res = await idxClient . get < IndexStatus > ( `/index-status/ ${ corpusId } ` ); return res . data ; } // Exemple d\u2019usage getIndexStatus ( 'ae12f3e4-5678-90ab-cdef-1234567890ab' ) . then ( status => { console . log ( `Index exists: ${ status . isIndexed } , type: ${ status . indexType } ` ); }) . catch ( console . error );","title":"5. V\u00e9rifier l\u2019\u00e9tat d\u2019un index"},{"location":"Integration/vectordb/index_js/#6-verifier-tous-les-index","text":"export async function getAllIndexes () : Promise < AllIndexesStatus > { const res = await idxClient . get ( '/indexes' ); return res . data as AllIndexesStatus ; } // Exemple d\u2019usage getAllIndexes () . then ( all => console . table ( all )) . catch ( console . error ); Pour plus de d\u00e9tails, voir les routes d\u00e9finies dans api/index_endpoint.py et les DTO Pydantic dans src/schemas.py Guide g\u00e9n\u00e9r\u00e9 automatiquement \u2014 05 mai 2025","title":"6. V\u00e9rifier tous les index"},{"location":"Integration/vectordb/search_js/","text":"Int\u00e9gration JavaScript du moteur de recherche Cette documentation explique comment int\u00e9grer le moteur de recherche hybride de Cl\u00e9a-API dans une application JavaScript/TypeScript. Configuration du client import axios from 'axios' ; const API_URL = 'http://localhost:8080' ; // Ou votre URL de d\u00e9ploiement const cleaClient = axios . create ({ baseURL : API_URL , headers : { 'Content-Type' : 'application/json' , }, }); Effectuer une recherche simple async function searchDocuments ( query , topK = 10 ) { try { const response = await cleaClient . post ( '/search/hybrid_search' , { query , topK , }); return response . data ; } catch ( error ) { console . error ( 'Erreur lors de la recherche:' , error ); throw error ; } } // Utilisation searchDocuments ( 'analyse des risques' ) . then ( results => { console . log ( ` ${ results . totalResults } r\u00e9sultats trouv\u00e9s` ); results . results . forEach ( chunk => { console . log ( `[ ${ chunk . score . toFixed ( 2 ) } ] ${ chunk . title } : ${ chunk . content . slice ( 0 , 100 ) } ...` ); }); }); Recherche avanc\u00e9e avec confiance et normalisation async function advancedSearch ( query , options = {}) { const defaultOptions = { topK : 10 , theme : null , documentType : null , startDate : null , endDate : null , corpusId : null , hierarchical : false , filterByRelevance : true , normalizeScores : true , }; const searchParams = { query , ... defaultOptions , ... options , }; try { const response = await cleaClient . post ( '/search/hybrid_search' , searchParams ); return response . data ; } catch ( error ) { console . error ( 'Erreur lors de la recherche avanc\u00e9e:' , error ); throw error ; } } // Utilisation avec \u00e9valuation de la confiance advancedSearch ( 'bonnes pratiques d\u00e9veloppement durable' , { theme : 'RSE' , filterByRelevance : true , normalizeScores : true , }) . then ( results => { // Analyser le niveau de confiance const { confidence } = results ; console . log ( `Confiance: ${ confidence . level . toFixed ( 2 ) } - ${ confidence . message } ` ); // Afficher les statistiques console . log ( `Stats: min= ${ confidence . stats . min . toFixed ( 2 ) } , max= ${ confidence . stats . max . toFixed ( 2 ) } ` ); // G\u00e9rer le cas o\u00f9 la requ\u00eate est hors domaine if ( confidence . level < 0.3 ) { console . log ( '\u26a0\ufe0f La requ\u00eate semble \u00eatre hors du domaine de connaissance' ); // Afficher un message \u00e0 l'utilisateur } // Afficher les r\u00e9sultats s'il y en a if ( results . results . length > 0 ) { results . results . forEach ( chunk => { console . log ( `[ ${ chunk . score . toFixed ( 2 ) } ] ${ chunk . title } ` ); }); } else { console . log ( 'Aucun r\u00e9sultat pertinent trouv\u00e9' ); } }); Composant React d'exemple import React , { useState , useEffect } from 'react' ; import axios from 'axios' ; const API_URL = 'http://localhost:8080' ; function SearchComponent () { const [ query , setQuery ] = useState ( '' ); const [ results , setResults ] = useState ([]); const [ confidence , setConfidence ] = useState ( null ); const [ isSearching , setIsSearching ] = useState ( false ); const [ error , setError ] = useState ( null ); const handleSearch = async () => { if ( ! query . trim ()) return ; setIsSearching ( true ); setError ( null ); try { const response = await axios . post ( ` ${ API_URL } /search/hybrid_search` , { query : query . trim (), topK : 10 , filterByRelevance : true , normalizeScores : true }); setResults ( response . data . results ); setConfidence ( response . data . confidence ); } catch ( err ) { setError ( 'Erreur lors de la recherche' ); console . error ( err ); } finally { setIsSearching ( false ); } }; return ( < div className = \"search-container\" > < div className = \"search-bar\" > < input type = \"text\" value = { query } onChange = {( e ) => setQuery ( e . target . value )} placeholder = \"Rechercher...\" /> < button onClick = { handleSearch } disabled = { isSearching }> { isSearching ? 'Recherche...' : 'Rechercher' } </ button > </ div > { confidence && ( < div className = { `confidence-meter confidence- ${ Math . floor ( confidence . level * 10 ) } ` }> < div className = \"confidence-label\" > Pertinence : {( confidence . level * 100 ). toFixed ( 0 )} % </ div > < div className = \"confidence-message\" >{ confidence . message }</ div > </ div > )} { error && < div className = \"error-message\" >{ error }</ div >} < div className = \"results-list\" > { results . length === 0 && ! isSearching && query && ( < p > Aucun r\u00e9sultat pertinent trouv\u00e9 .</ p > )} { results . map (( result ) => ( < div key = { result . chunkId } className = \"result-item\" > < div className = \"result-header\" > < h3 >{ result . title }</ h3 > < span className = \"score\" >{( result . score * 100 ). toFixed ( 0 )} % pertinent </ span > </ div > < div className = \"result-content\" >{ result . content }</ div > < div className = \"result-meta\" > Th\u00e8me : { result . theme } | Type : { result . documentType } | Date : { new Date ( result . publishDate ). toLocaleDateString ()} </ div > </ div > ))} </ div > </ div > ); } export default SearchComponent ;","title":"Recherche"},{"location":"Integration/vectordb/search_js/#integration-javascript-du-moteur-de-recherche","text":"Cette documentation explique comment int\u00e9grer le moteur de recherche hybride de Cl\u00e9a-API dans une application JavaScript/TypeScript.","title":"Int\u00e9gration JavaScript du moteur de recherche"},{"location":"Integration/vectordb/search_js/#configuration-du-client","text":"import axios from 'axios' ; const API_URL = 'http://localhost:8080' ; // Ou votre URL de d\u00e9ploiement const cleaClient = axios . create ({ baseURL : API_URL , headers : { 'Content-Type' : 'application/json' , }, });","title":"Configuration du client"},{"location":"Integration/vectordb/search_js/#effectuer-une-recherche-simple","text":"async function searchDocuments ( query , topK = 10 ) { try { const response = await cleaClient . post ( '/search/hybrid_search' , { query , topK , }); return response . data ; } catch ( error ) { console . error ( 'Erreur lors de la recherche:' , error ); throw error ; } } // Utilisation searchDocuments ( 'analyse des risques' ) . then ( results => { console . log ( ` ${ results . totalResults } r\u00e9sultats trouv\u00e9s` ); results . results . forEach ( chunk => { console . log ( `[ ${ chunk . score . toFixed ( 2 ) } ] ${ chunk . title } : ${ chunk . content . slice ( 0 , 100 ) } ...` ); }); });","title":"Effectuer une recherche simple"},{"location":"Integration/vectordb/search_js/#recherche-avancee-avec-confiance-et-normalisation","text":"async function advancedSearch ( query , options = {}) { const defaultOptions = { topK : 10 , theme : null , documentType : null , startDate : null , endDate : null , corpusId : null , hierarchical : false , filterByRelevance : true , normalizeScores : true , }; const searchParams = { query , ... defaultOptions , ... options , }; try { const response = await cleaClient . post ( '/search/hybrid_search' , searchParams ); return response . data ; } catch ( error ) { console . error ( 'Erreur lors de la recherche avanc\u00e9e:' , error ); throw error ; } } // Utilisation avec \u00e9valuation de la confiance advancedSearch ( 'bonnes pratiques d\u00e9veloppement durable' , { theme : 'RSE' , filterByRelevance : true , normalizeScores : true , }) . then ( results => { // Analyser le niveau de confiance const { confidence } = results ; console . log ( `Confiance: ${ confidence . level . toFixed ( 2 ) } - ${ confidence . message } ` ); // Afficher les statistiques console . log ( `Stats: min= ${ confidence . stats . min . toFixed ( 2 ) } , max= ${ confidence . stats . max . toFixed ( 2 ) } ` ); // G\u00e9rer le cas o\u00f9 la requ\u00eate est hors domaine if ( confidence . level < 0.3 ) { console . log ( '\u26a0\ufe0f La requ\u00eate semble \u00eatre hors du domaine de connaissance' ); // Afficher un message \u00e0 l'utilisateur } // Afficher les r\u00e9sultats s'il y en a if ( results . results . length > 0 ) { results . results . forEach ( chunk => { console . log ( `[ ${ chunk . score . toFixed ( 2 ) } ] ${ chunk . title } ` ); }); } else { console . log ( 'Aucun r\u00e9sultat pertinent trouv\u00e9' ); } });","title":"Recherche avanc\u00e9e avec confiance et normalisation"},{"location":"Integration/vectordb/search_js/#composant-react-dexemple","text":"import React , { useState , useEffect } from 'react' ; import axios from 'axios' ; const API_URL = 'http://localhost:8080' ; function SearchComponent () { const [ query , setQuery ] = useState ( '' ); const [ results , setResults ] = useState ([]); const [ confidence , setConfidence ] = useState ( null ); const [ isSearching , setIsSearching ] = useState ( false ); const [ error , setError ] = useState ( null ); const handleSearch = async () => { if ( ! query . trim ()) return ; setIsSearching ( true ); setError ( null ); try { const response = await axios . post ( ` ${ API_URL } /search/hybrid_search` , { query : query . trim (), topK : 10 , filterByRelevance : true , normalizeScores : true }); setResults ( response . data . results ); setConfidence ( response . data . confidence ); } catch ( err ) { setError ( 'Erreur lors de la recherche' ); console . error ( err ); } finally { setIsSearching ( false ); } }; return ( < div className = \"search-container\" > < div className = \"search-bar\" > < input type = \"text\" value = { query } onChange = {( e ) => setQuery ( e . target . value )} placeholder = \"Rechercher...\" /> < button onClick = { handleSearch } disabled = { isSearching }> { isSearching ? 'Recherche...' : 'Rechercher' } </ button > </ div > { confidence && ( < div className = { `confidence-meter confidence- ${ Math . floor ( confidence . level * 10 ) } ` }> < div className = \"confidence-label\" > Pertinence : {( confidence . level * 100 ). toFixed ( 0 )} % </ div > < div className = \"confidence-message\" >{ confidence . message }</ div > </ div > )} { error && < div className = \"error-message\" >{ error }</ div >} < div className = \"results-list\" > { results . length === 0 && ! isSearching && query && ( < p > Aucun r\u00e9sultat pertinent trouv\u00e9 .</ p > )} { results . map (( result ) => ( < div key = { result . chunkId } className = \"result-item\" > < div className = \"result-header\" > < h3 >{ result . title }</ h3 > < span className = \"score\" >{( result . score * 100 ). toFixed ( 0 )} % pertinent </ span > </ div > < div className = \"result-content\" >{ result . content }</ div > < div className = \"result-meta\" > Th\u00e8me : { result . theme } | Type : { result . documentType } | Date : { new Date ( result . publishDate ). toLocaleDateString ()} </ div > </ div > ))} </ div > </ div > ); } export default SearchComponent ;","title":"Composant React d'exemple"},{"location":"api/lib/askai/rag_references/","text":"References for RAG RAGProcessor Processeur RAG (Retrieval-Augmented Generation) optimis\u00e9 pour petits LLM. Cette classe orchestre la r\u00e9cup\u00e9ration de documents pertinents et leur utilisation pour g\u00e9n\u00e9rer des r\u00e9ponses avec un petit mod\u00e8le LLM Qwen3. Parameters: model_loader ( ModelLoader ) \u2013 Chargeur de mod\u00e8le LLM. search_engine ( SearchEngine ) \u2013 Moteur de recherche vectorielle. db_session ( Session ) \u2013 Session de base de donn\u00e9es SQLAlchemy. max_tokens_per_doc ( int , default: 300 ) \u2013 Nombre maximum de tokens par document. max_docs ( int , default: 5 ) \u2013 Nombre maximum de documents \u00e0 utiliser. Attributes: model_loader ( ModelLoader ) \u2013 Chargeur de mod\u00e8le LLM. search_engine ( SearchEngine ) \u2013 Moteur de recherche vectorielle. db_session ( Session ) \u2013 Session de base de donn\u00e9es SQLAlchemy. max_tokens_per_doc ( int ) \u2013 Nombre maximum de tokens par document. max_docs ( int ) \u2013 Nombre maximum de documents \u00e0 utiliser. logger ( Logger ) \u2013 Logger pour les messages et diagnostics. __init__ ( model_loader , search_engine , db_session , max_tokens_per_doc = 300 , max_docs = 5 ) Initialise le processeur RAG. Parameters: model_loader ( ModelLoader ) \u2013 Chargeur de mod\u00e8le LLM. search_engine ( SearchEngine ) \u2013 Moteur de recherche vectorielle. db_session ( Session ) \u2013 Session de base de donn\u00e9es SQLAlchemy. max_tokens_per_doc ( int , default: 300 ) \u2013 Nombre maximum de tokens par document. max_docs ( int , default: 5 ) \u2013 Nombre maximum de documents \u00e0 utiliser. format_context ( search_results ) Formate les r\u00e9sultats de recherche en contexte structur\u00e9 pour le LLM. Utilise les r\u00e9sultats d'une requ\u00eate pour cr\u00e9er un contexte format\u00e9 qui sera utilis\u00e9 dans le prompt envoy\u00e9 au mod\u00e8le LLM. Parameters: search_results ( SearchResponse ) \u2013 R\u00e9ponse de recherche contenant les chunks pertinents. Returns: str ( str ) \u2013 Contexte format\u00e9 pr\u00eat \u00e0 \u00eatre inject\u00e9 dans le prompt. get_prompt_template ( query , context , prompt_type = 'standard' , ** kwargs ) Retourne un template de prompt adapt\u00e9 au type de requ\u00eate. Parameters: query ( str ) \u2013 Question de l'utilisateur. context ( str ) \u2013 Contexte documentaire format\u00e9. prompt_type ( str , default: 'standard' ) \u2013 Type de prompt \u00e0 utiliser ('standard', 'summary', 'comparison'). **kwargs \u2013 Param\u00e8tres additionnels sp\u00e9cifiques au type de prompt. Returns: PromptTemplate ( PromptTemplate ) \u2013 Template de prompt configur\u00e9 avec les variables appropri\u00e9es. Raises: ValueError \u2013 Si le type de prompt sp\u00e9cifi\u00e9 n'est pas reconnu. retrieve_and_generate ( query , filters = None , prompt_type = 'standard' , generation_kwargs = None , enable_thinking = None , ** prompt_kwargs ) async R\u00e9cup\u00e8re les documents pertinents et g\u00e9n\u00e8re une r\u00e9ponse. Parameters: query ( str ) \u2013 Question de l'utilisateur. filters ( Dict [ str , Any ] , default: None ) \u2013 Filtres \u00e0 appliquer lors de la recherche. prompt_type ( str , default: 'standard' ) \u2013 Type de prompt \u00e0 utiliser ('standard', 'summary', 'comparison'). generation_kwargs ( Dict [ str , Any ] , default: None ) \u2013 Param\u00e8tres additionnels pour la g\u00e9n\u00e9ration de texte. enable_thinking ( Optional [ bool ] , default: None ) \u2013 Active ou d\u00e9sactive le mode de r\u00e9flexion. Si None, utilise la configuration du mod\u00e8le. **prompt_kwargs \u2013 Param\u00e8tres additionnels pour le template de prompt. Returns: tuple ( tuple ) \u2013 Si enable_thinking=True : (thinking, response, search_results) Si enable_thinking=False : (response, search_results) retrieve_and_generate_stream ( query , filters = None , prompt_type = 'standard' , generation_kwargs = None , enable_thinking = None , ** prompt_kwargs ) async R\u00e9cup\u00e8re les documents pertinents et g\u00e9n\u00e8re une r\u00e9ponse en streaming. Cette m\u00e9thode enrichit la r\u00e9ponse avec les documents utilis\u00e9s pour la g\u00e9n\u00e9ration. Chaque fragment retourn\u00e9 est un dictionnaire identifiant son type et contenu. Parameters: query ( str ) \u2013 Question de l'utilisateur. filters ( Dict [ str , Any ] , default: None ) \u2013 Filtres \u00e0 appliquer lors de la recherche. prompt_type ( str , default: 'standard' ) \u2013 Type de prompt \u00e0 utiliser ('standard', 'summary', 'comparison'). generation_kwargs ( Dict [ str , Any ] , default: None ) \u2013 Param\u00e8tres additionnels pour la g\u00e9n\u00e9ration de texte. enable_thinking ( Optional [ bool ] , default: None ) \u2013 Active ou d\u00e9sactive le mode de r\u00e9flexion. Si None, utilise la configuration du mod\u00e8le. **prompt_kwargs \u2013 Param\u00e8tres additionnels pour le template de prompt. Yields: AsyncGenerator [ Dict [ str , Any ], None] \u2013 Dict[str, Any]: Fragments de la r\u00e9ponse ou m\u00e9tadonn\u00e9es avec leur type : - {\"type\": \"thinking\", \"content\": str} pour les parties de r\u00e9flexion - {\"type\": \"response\", \"content\": str} pour les parties de r\u00e9ponse - {\"type\": \"context\", \"content\": Dict} pour le contexte utilis\u00e9 - {\"type\": \"error\", \"content\": str} en cas d'erreur - {\"type\": \"done\", \"content\": \"\"} \u00e0 la fin du streaming retrieve_documents ( query , filters = None ) async R\u00e9cup\u00e8re les documents pertinents pour une requ\u00eate donn\u00e9e. Effectue une recherche dans la base de donn\u00e9es vectorielle et retourne les r\u00e9sultats format\u00e9s selon le sch\u00e9ma standard de l'application. Parameters: query ( str ) \u2013 Question de l'utilisateur. filters ( Dict [ str , Any ] , default: None ) \u2013 Filtres \u00e0 appliquer lors de la recherche. Returns: SearchResponse ( SearchResponse ) \u2013 R\u00e9ponse contenant les r\u00e9sultats de recherche pertinents.","title":"RAG"},{"location":"api/lib/askai/rag_references/#references-for-rag","text":"","title":"References for RAG"},{"location":"api/lib/askai/rag_references/#askai.src.rag.RAGProcessor","text":"Processeur RAG (Retrieval-Augmented Generation) optimis\u00e9 pour petits LLM. Cette classe orchestre la r\u00e9cup\u00e9ration de documents pertinents et leur utilisation pour g\u00e9n\u00e9rer des r\u00e9ponses avec un petit mod\u00e8le LLM Qwen3. Parameters: model_loader ( ModelLoader ) \u2013 Chargeur de mod\u00e8le LLM. search_engine ( SearchEngine ) \u2013 Moteur de recherche vectorielle. db_session ( Session ) \u2013 Session de base de donn\u00e9es SQLAlchemy. max_tokens_per_doc ( int , default: 300 ) \u2013 Nombre maximum de tokens par document. max_docs ( int , default: 5 ) \u2013 Nombre maximum de documents \u00e0 utiliser. Attributes: model_loader ( ModelLoader ) \u2013 Chargeur de mod\u00e8le LLM. search_engine ( SearchEngine ) \u2013 Moteur de recherche vectorielle. db_session ( Session ) \u2013 Session de base de donn\u00e9es SQLAlchemy. max_tokens_per_doc ( int ) \u2013 Nombre maximum de tokens par document. max_docs ( int ) \u2013 Nombre maximum de documents \u00e0 utiliser. logger ( Logger ) \u2013 Logger pour les messages et diagnostics.","title":"RAGProcessor"},{"location":"api/lib/askai/rag_references/#askai.src.rag.RAGProcessor.__init__","text":"Initialise le processeur RAG. Parameters: model_loader ( ModelLoader ) \u2013 Chargeur de mod\u00e8le LLM. search_engine ( SearchEngine ) \u2013 Moteur de recherche vectorielle. db_session ( Session ) \u2013 Session de base de donn\u00e9es SQLAlchemy. max_tokens_per_doc ( int , default: 300 ) \u2013 Nombre maximum de tokens par document. max_docs ( int , default: 5 ) \u2013 Nombre maximum de documents \u00e0 utiliser.","title":"__init__"},{"location":"api/lib/askai/rag_references/#askai.src.rag.RAGProcessor.format_context","text":"Formate les r\u00e9sultats de recherche en contexte structur\u00e9 pour le LLM. Utilise les r\u00e9sultats d'une requ\u00eate pour cr\u00e9er un contexte format\u00e9 qui sera utilis\u00e9 dans le prompt envoy\u00e9 au mod\u00e8le LLM. Parameters: search_results ( SearchResponse ) \u2013 R\u00e9ponse de recherche contenant les chunks pertinents. Returns: str ( str ) \u2013 Contexte format\u00e9 pr\u00eat \u00e0 \u00eatre inject\u00e9 dans le prompt.","title":"format_context"},{"location":"api/lib/askai/rag_references/#askai.src.rag.RAGProcessor.get_prompt_template","text":"Retourne un template de prompt adapt\u00e9 au type de requ\u00eate. Parameters: query ( str ) \u2013 Question de l'utilisateur. context ( str ) \u2013 Contexte documentaire format\u00e9. prompt_type ( str , default: 'standard' ) \u2013 Type de prompt \u00e0 utiliser ('standard', 'summary', 'comparison'). **kwargs \u2013 Param\u00e8tres additionnels sp\u00e9cifiques au type de prompt. Returns: PromptTemplate ( PromptTemplate ) \u2013 Template de prompt configur\u00e9 avec les variables appropri\u00e9es. Raises: ValueError \u2013 Si le type de prompt sp\u00e9cifi\u00e9 n'est pas reconnu.","title":"get_prompt_template"},{"location":"api/lib/askai/rag_references/#askai.src.rag.RAGProcessor.retrieve_and_generate","text":"R\u00e9cup\u00e8re les documents pertinents et g\u00e9n\u00e8re une r\u00e9ponse. Parameters: query ( str ) \u2013 Question de l'utilisateur. filters ( Dict [ str , Any ] , default: None ) \u2013 Filtres \u00e0 appliquer lors de la recherche. prompt_type ( str , default: 'standard' ) \u2013 Type de prompt \u00e0 utiliser ('standard', 'summary', 'comparison'). generation_kwargs ( Dict [ str , Any ] , default: None ) \u2013 Param\u00e8tres additionnels pour la g\u00e9n\u00e9ration de texte. enable_thinking ( Optional [ bool ] , default: None ) \u2013 Active ou d\u00e9sactive le mode de r\u00e9flexion. Si None, utilise la configuration du mod\u00e8le. **prompt_kwargs \u2013 Param\u00e8tres additionnels pour le template de prompt. Returns: tuple ( tuple ) \u2013 Si enable_thinking=True : (thinking, response, search_results) Si enable_thinking=False : (response, search_results)","title":"retrieve_and_generate"},{"location":"api/lib/askai/rag_references/#askai.src.rag.RAGProcessor.retrieve_and_generate_stream","text":"R\u00e9cup\u00e8re les documents pertinents et g\u00e9n\u00e8re une r\u00e9ponse en streaming. Cette m\u00e9thode enrichit la r\u00e9ponse avec les documents utilis\u00e9s pour la g\u00e9n\u00e9ration. Chaque fragment retourn\u00e9 est un dictionnaire identifiant son type et contenu. Parameters: query ( str ) \u2013 Question de l'utilisateur. filters ( Dict [ str , Any ] , default: None ) \u2013 Filtres \u00e0 appliquer lors de la recherche. prompt_type ( str , default: 'standard' ) \u2013 Type de prompt \u00e0 utiliser ('standard', 'summary', 'comparison'). generation_kwargs ( Dict [ str , Any ] , default: None ) \u2013 Param\u00e8tres additionnels pour la g\u00e9n\u00e9ration de texte. enable_thinking ( Optional [ bool ] , default: None ) \u2013 Active ou d\u00e9sactive le mode de r\u00e9flexion. Si None, utilise la configuration du mod\u00e8le. **prompt_kwargs \u2013 Param\u00e8tres additionnels pour le template de prompt. Yields: AsyncGenerator [ Dict [ str , Any ], None] \u2013 Dict[str, Any]: Fragments de la r\u00e9ponse ou m\u00e9tadonn\u00e9es avec leur type : - {\"type\": \"thinking\", \"content\": str} pour les parties de r\u00e9flexion - {\"type\": \"response\", \"content\": str} pour les parties de r\u00e9ponse - {\"type\": \"context\", \"content\": Dict} pour le contexte utilis\u00e9 - {\"type\": \"error\", \"content\": str} en cas d'erreur - {\"type\": \"done\", \"content\": \"\"} \u00e0 la fin du streaming","title":"retrieve_and_generate_stream"},{"location":"api/lib/askai/rag_references/#askai.src.rag.RAGProcessor.retrieve_documents","text":"R\u00e9cup\u00e8re les documents pertinents pour une requ\u00eate donn\u00e9e. Effectue une recherche dans la base de donn\u00e9es vectorielle et retourne les r\u00e9sultats format\u00e9s selon le sch\u00e9ma standard de l'application. Parameters: query ( str ) \u2013 Question de l'utilisateur. filters ( Dict [ str , Any ] , default: None ) \u2013 Filtres \u00e0 appliquer lors de la recherche. Returns: SearchResponse ( SearchResponse ) \u2013 R\u00e9ponse contenant les r\u00e9sultats de recherche pertinents.","title":"retrieve_documents"},{"location":"api/lib/doc_loader/extractor_references/","text":"References for EXTRACTOR BaseExtractor Bases: ABC Interface abstraite pour tous les extracteurs de documents. Attributes: file_path ( Path ) \u2013 Chemin vers le fichier \u00e0 extraire. __init__ ( file_path ) Initialise un extracteur avec le chemin du fichier. Parameters: file_path ( str ) \u2013 Chemin vers le fichier \u00e0 extraire. extract_one ( * , max_length = 1000 ) abstractmethod Retourne un seul DocumentWithChunks pr\u00eat pour la DB. Parameters: max_length ( int , default: 1000 ) \u2013 Longueur maximale d'un chunk. D\u00e9faut \u00e0 1000. Returns: DocumentWithChunks ( DocumentWithChunks ) \u2013 Document avec ses chunks hi\u00e9rarchiques. Raises: NotImplementedError \u2013 Si la m\u00e9thode n'est pas impl\u00e9ment\u00e9e dans la classe d\u00e9riv\u00e9e. DocxExtractor Bases: BaseExtractor Convertit un fichier .docx en un unique payload DocumentWithChunks pr\u00eat \u00e0 \u00eatre POST\u00e9 sur /database/documents . __init__ ( file_path ) Initialise l'extracteur DOCX avec les m\u00e9tadonn\u00e9es par d\u00e9faut. Parameters: file_path ( str ) \u2013 Chemin vers le fichier DOCX. Raises: ValueError \u2013 Si le fichier DOCX ne peut pas \u00eatre ouvert. extract_one ( max_length = 1000 ) Extrait un seul objet DocumentWithChunks \u00e0 partir du fichier DOCX. Parameters: max_length ( int , default: 1000 ) \u2013 Longueur cible pour les segments finaux. Returns: DocumentWithChunks ( DocumentWithChunks ) \u2013 Objet contenant les chunks et m\u00e9tadonn\u00e9es. HtmlExtractor Bases: BaseExtractor Convertit n\u2019importe quel document HTML en un ou plusieurs payloads DocumentWithChunks pr\u00eats pour l\u2019endpoint POST /database/documents . Le titre du <title> ou du nom de fichier est utilis\u00e9 comme document.title . Tout le texte visible (sans balises) est extrait. __init__ ( file_path ) Initialise l'extracteur HTML avec les m\u00e9tadonn\u00e9es par d\u00e9faut. Parameters: file_path ( str ) \u2013 Chemin vers le fichier HTML. Raises: ValueError \u2013 Si le fichier HTML ne peut pas \u00eatre lu. extract_one ( max_length = 1000 ) Extrait un seul objet DocumentWithChunks \u00e0 partir du fichier HTML. Parameters: max_length ( int , default: 1000 ) \u2013 Longueur cible pour les segments finaux. Returns: DocumentWithChunks ( DocumentWithChunks ) \u2013 Objet contenant les chunks et m\u00e9tadonn\u00e9es. Raises: ValueError \u2013 Si aucun contenu n'est extrait du document HTML. iter_text () Renvoie le texte brut, ligne par ligne (utile pour le stream ). Returns: Iterator [ str ] \u2013 Iterator[str]: Texte brut extrait du document HTML. JsonExtractor Bases: BaseExtractor Extrait des contenus stock\u00e9s dans un fichier JSON . Le fichier doit \u00eatre une liste de dictionnaires : [ {\"title\": \"...\", \"content\": \"...\", \"theme\": \"...\", ...}, ... ] Chaque entr\u00e9e g\u00e9n\u00e8re un DocumentWithChunks pr\u00eat \u00e0 \u00eatre envoy\u00e9 \u00e0 l\u2019endpoint POST /database/documents . __init__ ( file_path ) Initialise l'extracteur JSON avec les m\u00e9tadonn\u00e9es par d\u00e9faut. Parameters: file_path ( str ) \u2013 Chemin vers le fichier JSON. Raises: ValueError \u2013 Si le fichier JSON ne peut pas \u00eatre ouvert ou est vide. PdfExtractor Bases: BaseExtractor Convertit un fichier .pdf en un unique payload DocumentWithChunks , pr\u00eat \u00e0 \u00eatre ins\u00e9r\u00e9 via /database/documents . extract_one ( max_length = 1000 ) Extrait un seul objet DocumentWithChunks \u00e0 partir du fichier PDF. Parameters: max_length ( int , default: 1000 ) \u2013 Longueur cible pour les segments finaux. Returns: DocumentWithChunks ( DocumentWithChunks ) \u2013 Objet contenant les chunks et m\u00e9tadonn\u00e9es. TxtExtractor Bases: BaseExtractor Convertit un fichier .txt en un unique payload DocumentWithChunks , pr\u00eat \u00e0 \u00eatre envoy\u00e9 vers /database/documents . extract_one ( max_length = 1000 ) Extrait un seul objet DocumentWithChunks \u00e0 partir du fichier TXT. Parameters: max_length ( int , default: 1000 ) \u2013 Longueur cible pour les segments finaux. Returns: DocumentWithChunks ( DocumentWithChunks ) \u2013 Objet contenant les chunks et m\u00e9tadonn\u00e9es. Raises: ValueError \u2013 Si le fichier est vide ou si aucune m\u00e9tadonn\u00e9e valide n'est trouv\u00e9e.","title":"Extracteurs"},{"location":"api/lib/doc_loader/extractor_references/#references-for-extractor","text":"","title":"References for EXTRACTOR"},{"location":"api/lib/doc_loader/extractor_references/#doc_loader.src.data_extractor.BaseExtractor","text":"Bases: ABC Interface abstraite pour tous les extracteurs de documents. Attributes: file_path ( Path ) \u2013 Chemin vers le fichier \u00e0 extraire.","title":"BaseExtractor"},{"location":"api/lib/doc_loader/extractor_references/#doc_loader.src.data_extractor.BaseExtractor.__init__","text":"Initialise un extracteur avec le chemin du fichier. Parameters: file_path ( str ) \u2013 Chemin vers le fichier \u00e0 extraire.","title":"__init__"},{"location":"api/lib/doc_loader/extractor_references/#doc_loader.src.data_extractor.BaseExtractor.extract_one","text":"Retourne un seul DocumentWithChunks pr\u00eat pour la DB. Parameters: max_length ( int , default: 1000 ) \u2013 Longueur maximale d'un chunk. D\u00e9faut \u00e0 1000. Returns: DocumentWithChunks ( DocumentWithChunks ) \u2013 Document avec ses chunks hi\u00e9rarchiques. Raises: NotImplementedError \u2013 Si la m\u00e9thode n'est pas impl\u00e9ment\u00e9e dans la classe d\u00e9riv\u00e9e.","title":"extract_one"},{"location":"api/lib/doc_loader/extractor_references/#doc_loader.src.data_extractor.DocxExtractor","text":"Bases: BaseExtractor Convertit un fichier .docx en un unique payload DocumentWithChunks pr\u00eat \u00e0 \u00eatre POST\u00e9 sur /database/documents .","title":"DocxExtractor"},{"location":"api/lib/doc_loader/extractor_references/#doc_loader.src.data_extractor.DocxExtractor.__init__","text":"Initialise l'extracteur DOCX avec les m\u00e9tadonn\u00e9es par d\u00e9faut. Parameters: file_path ( str ) \u2013 Chemin vers le fichier DOCX. Raises: ValueError \u2013 Si le fichier DOCX ne peut pas \u00eatre ouvert.","title":"__init__"},{"location":"api/lib/doc_loader/extractor_references/#doc_loader.src.data_extractor.DocxExtractor.extract_one","text":"Extrait un seul objet DocumentWithChunks \u00e0 partir du fichier DOCX. Parameters: max_length ( int , default: 1000 ) \u2013 Longueur cible pour les segments finaux. Returns: DocumentWithChunks ( DocumentWithChunks ) \u2013 Objet contenant les chunks et m\u00e9tadonn\u00e9es.","title":"extract_one"},{"location":"api/lib/doc_loader/extractor_references/#doc_loader.src.data_extractor.HtmlExtractor","text":"Bases: BaseExtractor Convertit n\u2019importe quel document HTML en un ou plusieurs payloads DocumentWithChunks pr\u00eats pour l\u2019endpoint POST /database/documents . Le titre du <title> ou du nom de fichier est utilis\u00e9 comme document.title . Tout le texte visible (sans balises) est extrait.","title":"HtmlExtractor"},{"location":"api/lib/doc_loader/extractor_references/#doc_loader.src.data_extractor.HtmlExtractor.__init__","text":"Initialise l'extracteur HTML avec les m\u00e9tadonn\u00e9es par d\u00e9faut. Parameters: file_path ( str ) \u2013 Chemin vers le fichier HTML. Raises: ValueError \u2013 Si le fichier HTML ne peut pas \u00eatre lu.","title":"__init__"},{"location":"api/lib/doc_loader/extractor_references/#doc_loader.src.data_extractor.HtmlExtractor.extract_one","text":"Extrait un seul objet DocumentWithChunks \u00e0 partir du fichier HTML. Parameters: max_length ( int , default: 1000 ) \u2013 Longueur cible pour les segments finaux. Returns: DocumentWithChunks ( DocumentWithChunks ) \u2013 Objet contenant les chunks et m\u00e9tadonn\u00e9es. Raises: ValueError \u2013 Si aucun contenu n'est extrait du document HTML.","title":"extract_one"},{"location":"api/lib/doc_loader/extractor_references/#doc_loader.src.data_extractor.HtmlExtractor.iter_text","text":"Renvoie le texte brut, ligne par ligne (utile pour le stream ). Returns: Iterator [ str ] \u2013 Iterator[str]: Texte brut extrait du document HTML.","title":"iter_text"},{"location":"api/lib/doc_loader/extractor_references/#doc_loader.src.data_extractor.JsonExtractor","text":"Bases: BaseExtractor Extrait des contenus stock\u00e9s dans un fichier JSON . Le fichier doit \u00eatre une liste de dictionnaires : [ {\"title\": \"...\", \"content\": \"...\", \"theme\": \"...\", ...}, ... ] Chaque entr\u00e9e g\u00e9n\u00e8re un DocumentWithChunks pr\u00eat \u00e0 \u00eatre envoy\u00e9 \u00e0 l\u2019endpoint POST /database/documents .","title":"JsonExtractor"},{"location":"api/lib/doc_loader/extractor_references/#doc_loader.src.data_extractor.JsonExtractor.__init__","text":"Initialise l'extracteur JSON avec les m\u00e9tadonn\u00e9es par d\u00e9faut. Parameters: file_path ( str ) \u2013 Chemin vers le fichier JSON. Raises: ValueError \u2013 Si le fichier JSON ne peut pas \u00eatre ouvert ou est vide.","title":"__init__"},{"location":"api/lib/doc_loader/extractor_references/#doc_loader.src.data_extractor.PdfExtractor","text":"Bases: BaseExtractor Convertit un fichier .pdf en un unique payload DocumentWithChunks , pr\u00eat \u00e0 \u00eatre ins\u00e9r\u00e9 via /database/documents .","title":"PdfExtractor"},{"location":"api/lib/doc_loader/extractor_references/#doc_loader.src.data_extractor.PdfExtractor.extract_one","text":"Extrait un seul objet DocumentWithChunks \u00e0 partir du fichier PDF. Parameters: max_length ( int , default: 1000 ) \u2013 Longueur cible pour les segments finaux. Returns: DocumentWithChunks ( DocumentWithChunks ) \u2013 Objet contenant les chunks et m\u00e9tadonn\u00e9es.","title":"extract_one"},{"location":"api/lib/doc_loader/extractor_references/#doc_loader.src.data_extractor.TxtExtractor","text":"Bases: BaseExtractor Convertit un fichier .txt en un unique payload DocumentWithChunks , pr\u00eat \u00e0 \u00eatre envoy\u00e9 vers /database/documents .","title":"TxtExtractor"},{"location":"api/lib/doc_loader/extractor_references/#doc_loader.src.data_extractor.TxtExtractor.extract_one","text":"Extrait un seul objet DocumentWithChunks \u00e0 partir du fichier TXT. Parameters: max_length ( int , default: 1000 ) \u2013 Longueur cible pour les segments finaux. Returns: DocumentWithChunks ( DocumentWithChunks ) \u2013 Objet contenant les chunks et m\u00e9tadonn\u00e9es. Raises: ValueError \u2013 Si le fichier est vide ou si aucune m\u00e9tadonn\u00e9e valide n'est trouv\u00e9e.","title":"extract_one"},{"location":"api/lib/doc_loader/splitter_references/","text":"References for SPLITTER Package splitter pour la segmentation s\u00e9mantique des documents. Fournit les outils pour segmenter des documents textuels en chunks hi\u00e9rarchiques optimis\u00e9s pour la recherche vectorielle. fallback_segmentation_stream ( text , max_length ) Version streaming de la segmentation de secours pour \u00e9conomiser de la m\u00e9moire. Parameters: text ( str ) \u2013 Texte \u00e0 segmenter. max_length ( int ) \u2013 Longueur maximale d'un chunk. Yields: ChunkCreate ( ChunkCreate ) \u2013 Les chunks g\u00e9n\u00e9r\u00e9s un par un. semantic_segmentation_stream ( text , max_length ) G\u00e9n\u00e8re les chunks s\u00e9mantiques d'un document au fil de l'eau. Version optimis\u00e9e pour les grands corpus non structur\u00e9s avec une meilleure extraction des fronti\u00e8res naturelles de texte et davantage de chunks. Parameters: text ( str ) \u2013 Texte \u00e0 segmenter. max_length ( int ) \u2013 Longueur maximale d'un chunk. Yields: ChunkCreate ( ChunkCreate ) \u2013 Un chunk \u00e0 la fois, g\u00e9n\u00e9r\u00e9 dans l'ordre hi\u00e9rarchique.","title":"Segmentation"},{"location":"api/lib/doc_loader/splitter_references/#references-for-splitter","text":"Package splitter pour la segmentation s\u00e9mantique des documents. Fournit les outils pour segmenter des documents textuels en chunks hi\u00e9rarchiques optimis\u00e9s pour la recherche vectorielle.","title":"References for SPLITTER"},{"location":"api/lib/doc_loader/splitter_references/#doc_loader.src.splitter.fallback_segmentation_stream","text":"Version streaming de la segmentation de secours pour \u00e9conomiser de la m\u00e9moire. Parameters: text ( str ) \u2013 Texte \u00e0 segmenter. max_length ( int ) \u2013 Longueur maximale d'un chunk. Yields: ChunkCreate ( ChunkCreate ) \u2013 Les chunks g\u00e9n\u00e9r\u00e9s un par un.","title":"fallback_segmentation_stream"},{"location":"api/lib/doc_loader/splitter_references/#doc_loader.src.splitter.semantic_segmentation_stream","text":"G\u00e9n\u00e8re les chunks s\u00e9mantiques d'un document au fil de l'eau. Version optimis\u00e9e pour les grands corpus non structur\u00e9s avec une meilleure extraction des fronti\u00e8res naturelles de texte et davantage de chunks. Parameters: text ( str ) \u2013 Texte \u00e0 segmenter. max_length ( int ) \u2013 Longueur maximale d'un chunk. Yields: ChunkCreate ( ChunkCreate ) \u2013 Un chunk \u00e0 la fois, g\u00e9n\u00e9r\u00e9 dans l'ordre hi\u00e9rarchique.","title":"semantic_segmentation_stream"},{"location":"api/lib/pipeline/pipeline_references/","text":"References for PIPELINE determine_document_type ( file_path ) D\u00e9termine le type de document \u00e0 partir du chemin du fichier. Parameters: file_path ( str ) \u2013 Chemin complet vers le fichier. Returns: str \u2013 Type de document d\u00e9tect\u00e9 en fonction de l'extension. process_and_store ( file_path , max_length = 500 , overlap = 100 , theme = 'Th\u00e8me g\u00e9n\u00e9rique' , corpus_id = None ) Traite un fichier et l'ins\u00e8re dans la base de donn\u00e9es avec une structure hi\u00e9rarchique. Cette fonction effectue l'extraction du texte, la segmentation hi\u00e9rarchique et l'insertion en base de donn\u00e9es avec gestion des embeddings. Parameters: file_path ( str ) \u2013 Chemin du fichier \u00e0 traiter. max_length ( int , default: 500 ) \u2013 Taille maximale d'un chunk final. Par d\u00e9faut \u00e0 500 caract\u00e8res. overlap ( int , default: 100 ) \u2013 Chevauchement entre les chunks. Par d\u00e9faut \u00e0 100 caract\u00e8res. theme ( Optional [ str ] , default: 'Th\u00e8me g\u00e9n\u00e9rique' ) \u2013 Th\u00e8me \u00e0 appliquer au document. Par d\u00e9faut \"Th\u00e8me g\u00e9n\u00e9rique\". corpus_id ( Optional [ str ] , default: None ) \u2013 Identifiant du corpus (g\u00e9n\u00e9r\u00e9 automatiquement si None). Returns: Dict [ str , Any ] \u2013 Dict[str, Any]: R\u00e9sultat de l'op\u00e9ration avec l'ID du document et les statistiques. Raises: FileNotFoundError \u2013 Si le fichier sp\u00e9cifi\u00e9 n'existe pas. ValueError \u2013 Si aucun contenu n'a pu \u00eatre extrait du document ou si une erreur survient lors de l'insertion en base.","title":"Pipeline"},{"location":"api/lib/pipeline/pipeline_references/#references-for-pipeline","text":"","title":"References for PIPELINE"},{"location":"api/lib/pipeline/pipeline_references/#pipeline.src.pipeline.determine_document_type","text":"D\u00e9termine le type de document \u00e0 partir du chemin du fichier. Parameters: file_path ( str ) \u2013 Chemin complet vers le fichier. Returns: str \u2013 Type de document d\u00e9tect\u00e9 en fonction de l'extension.","title":"determine_document_type"},{"location":"api/lib/pipeline/pipeline_references/#pipeline.src.pipeline.process_and_store","text":"Traite un fichier et l'ins\u00e8re dans la base de donn\u00e9es avec une structure hi\u00e9rarchique. Cette fonction effectue l'extraction du texte, la segmentation hi\u00e9rarchique et l'insertion en base de donn\u00e9es avec gestion des embeddings. Parameters: file_path ( str ) \u2013 Chemin du fichier \u00e0 traiter. max_length ( int , default: 500 ) \u2013 Taille maximale d'un chunk final. Par d\u00e9faut \u00e0 500 caract\u00e8res. overlap ( int , default: 100 ) \u2013 Chevauchement entre les chunks. Par d\u00e9faut \u00e0 100 caract\u00e8res. theme ( Optional [ str ] , default: 'Th\u00e8me g\u00e9n\u00e9rique' ) \u2013 Th\u00e8me \u00e0 appliquer au document. Par d\u00e9faut \"Th\u00e8me g\u00e9n\u00e9rique\". corpus_id ( Optional [ str ] , default: None ) \u2013 Identifiant du corpus (g\u00e9n\u00e9r\u00e9 automatiquement si None). Returns: Dict [ str , Any ] \u2013 Dict[str, Any]: R\u00e9sultat de l'op\u00e9ration avec l'ID du document et les statistiques. Raises: FileNotFoundError \u2013 Si le fichier sp\u00e9cifi\u00e9 n'existe pas. ValueError \u2013 Si aucun contenu n'a pu \u00eatre extrait du document ou si une erreur survient lors de l'insertion en base.","title":"process_and_store"},{"location":"api/lib/stats/stats_computer_references/","text":"References for STATS DashboardStats Bases: BaseModel Corps minimal pour cr\u00e9er un document (hors contenu). DocumentStats Bases: BaseModel Corps pour cr\u00e9er un chunk (texte + m\u00e9ta hi\u00e9rarchiques). SearchStats Bases: BaseModel Payload complet pour POST /database/documents . StatsComputer Calculateur de statistiques pour le tableau de bord. Cette classe centralise les fonctionnalit\u00e9s de calcul des diff\u00e9rentes m\u00e9triques utilis\u00e9es dans le tableau de bord d'administration. __init__ () Initialise le calculateur de statistiques. compute_all_stats () Calcule toutes les statistiques pour le tableau de bord. Cette fonction agr\u00e8ge les r\u00e9sultats des diff\u00e9rentes fonctions de calcul de statistiques pour fournir un objet unique contenant toutes les m\u00e9triques n\u00e9cessaires au tableau de bord. Returns: DashboardStats \u2013 Un objet DashboardStats contenant l'ensemble des statistiques. compute_document_stats ( skip = 0 , limit = 100 ) Calcule les statistiques des documents pr\u00e9sents dans la base de donn\u00e9es. Cette fonction r\u00e9cup\u00e8re les documents de la base de donn\u00e9es et calcule diverses statistiques comme le nombre total, la r\u00e9partition par th\u00e8me et par type, ainsi que les documents r\u00e9cemment ajout\u00e9s et l'\u00e9volution en pourcentage. Parameters: skip \u2013 Nombre de documents \u00e0 ignorer (pour la pagination). limit \u2013 Nombre maximal de documents \u00e0 retourner. Returns: DocumentStats \u2013 Un objet DocumentStats contenant les statistiques calcul\u00e9es. compute_search_stats ( skip = 0 , limit = 100 ) Calcule les statistiques des recherches effectu\u00e9es dans le syst\u00e8me. Cette fonction analyse l'historique des recherches pour fournir des m\u00e9triques comme le nombre total de recherches, l'activit\u00e9 r\u00e9cente et les requ\u00eates les plus populaires. Parameters: skip \u2013 Nombre d'entr\u00e9es \u00e0 ignorer pour la pagination. limit \u2013 Nombre maximal d'entr\u00e9es \u00e0 traiter. Returns: SearchStats \u2013 Un objet SearchStats contenant les statistiques calcul\u00e9es. compute_system_stats () Calcule les statistiques syst\u00e8me globales. Cette fonction analyse les m\u00e9triques de confiance des recherches effectu\u00e9es et l'\u00e9tat des corpus dans le syst\u00e8me pour fournir une vue d'ensemble de la performance et de l'\u00e9tat de l'indexation. Returns: SystemStats \u2013 Un objet SystemStats contenant les m\u00e9triques syst\u00e8me calcul\u00e9es. SystemStats Bases: BaseModel Payload complet pour POST /database/documents .","title":"Statistiques"},{"location":"api/lib/stats/stats_computer_references/#references-for-stats","text":"","title":"References for STATS"},{"location":"api/lib/stats/stats_computer_references/#stats.DashboardStats","text":"Bases: BaseModel Corps minimal pour cr\u00e9er un document (hors contenu).","title":"DashboardStats"},{"location":"api/lib/stats/stats_computer_references/#stats.DocumentStats","text":"Bases: BaseModel Corps pour cr\u00e9er un chunk (texte + m\u00e9ta hi\u00e9rarchiques).","title":"DocumentStats"},{"location":"api/lib/stats/stats_computer_references/#stats.SearchStats","text":"Bases: BaseModel Payload complet pour POST /database/documents .","title":"SearchStats"},{"location":"api/lib/stats/stats_computer_references/#stats.StatsComputer","text":"Calculateur de statistiques pour le tableau de bord. Cette classe centralise les fonctionnalit\u00e9s de calcul des diff\u00e9rentes m\u00e9triques utilis\u00e9es dans le tableau de bord d'administration.","title":"StatsComputer"},{"location":"api/lib/stats/stats_computer_references/#stats.StatsComputer.__init__","text":"Initialise le calculateur de statistiques.","title":"__init__"},{"location":"api/lib/stats/stats_computer_references/#stats.StatsComputer.compute_all_stats","text":"Calcule toutes les statistiques pour le tableau de bord. Cette fonction agr\u00e8ge les r\u00e9sultats des diff\u00e9rentes fonctions de calcul de statistiques pour fournir un objet unique contenant toutes les m\u00e9triques n\u00e9cessaires au tableau de bord. Returns: DashboardStats \u2013 Un objet DashboardStats contenant l'ensemble des statistiques.","title":"compute_all_stats"},{"location":"api/lib/stats/stats_computer_references/#stats.StatsComputer.compute_document_stats","text":"Calcule les statistiques des documents pr\u00e9sents dans la base de donn\u00e9es. Cette fonction r\u00e9cup\u00e8re les documents de la base de donn\u00e9es et calcule diverses statistiques comme le nombre total, la r\u00e9partition par th\u00e8me et par type, ainsi que les documents r\u00e9cemment ajout\u00e9s et l'\u00e9volution en pourcentage. Parameters: skip \u2013 Nombre de documents \u00e0 ignorer (pour la pagination). limit \u2013 Nombre maximal de documents \u00e0 retourner. Returns: DocumentStats \u2013 Un objet DocumentStats contenant les statistiques calcul\u00e9es.","title":"compute_document_stats"},{"location":"api/lib/stats/stats_computer_references/#stats.StatsComputer.compute_search_stats","text":"Calcule les statistiques des recherches effectu\u00e9es dans le syst\u00e8me. Cette fonction analyse l'historique des recherches pour fournir des m\u00e9triques comme le nombre total de recherches, l'activit\u00e9 r\u00e9cente et les requ\u00eates les plus populaires. Parameters: skip \u2013 Nombre d'entr\u00e9es \u00e0 ignorer pour la pagination. limit \u2013 Nombre maximal d'entr\u00e9es \u00e0 traiter. Returns: SearchStats \u2013 Un objet SearchStats contenant les statistiques calcul\u00e9es.","title":"compute_search_stats"},{"location":"api/lib/stats/stats_computer_references/#stats.StatsComputer.compute_system_stats","text":"Calcule les statistiques syst\u00e8me globales. Cette fonction analyse les m\u00e9triques de confiance des recherches effectu\u00e9es et l'\u00e9tat des corpus dans le syst\u00e8me pour fournir une vue d'ensemble de la performance et de l'\u00e9tat de l'indexation. Returns: SystemStats \u2013 Un objet SystemStats contenant les m\u00e9triques syst\u00e8me calcul\u00e9es.","title":"compute_system_stats"},{"location":"api/lib/stats/stats_computer_references/#stats.SystemStats","text":"Bases: BaseModel Payload complet pour POST /database/documents .","title":"SystemStats"},{"location":"api/lib/vectordb/crud_references/","text":"References for CRUD add_document_with_chunks ( db , doc , chunks , batch_size = 10 ) Ajoute un document et ses chunks \u00e0 la base de donn\u00e9es en g\u00e9n\u00e9rant les embeddings en mode lot. Cette fonction ins\u00e8re un document et tous ses chunks associ\u00e9s, g\u00e9n\u00e8re les embeddings par lots pour am\u00e9liorer les performances, et pr\u00e9serve les relations hi\u00e9rarchiques entre les chunks. Parameters: db ( Session ) \u2013 Session SQLAlchemy active. doc ( DocumentCreate ) \u2013 Donn\u00e9es du document \u00e0 cr\u00e9er. chunks ( List [ Dict [ str , Any ]] ) \u2013 Liste des chunks d\u00e9finis par l'utilisateur. batch_size ( int , default: 10 ) \u2013 Nombre de chunks \u00e0 traiter par lot. Returns: Dict [ str , Any ] \u2013 Dict contenant l'ID du document, le nombre de chunks, le corpus_id et si un index doit \u00eatre cr\u00e9\u00e9. delete_document ( document_id ) Supprime un document de la base de donn\u00e9es avec tous ses chunks associ\u00e9s. Cette fonction supprime un document et tous ses chunks. Si le document est le dernier de son corpus, l'index vectoriel associ\u00e9 est \u00e9galement supprim\u00e9. Parameters: document_id ( int ) \u2013 Identifiant du document \u00e0 supprimer. Returns: Dict [ str , Any ] \u2013 Dict contenant le statut de l'op\u00e9ration et les actions r\u00e9alis\u00e9es. delete_document_chunks ( document_id , chunk_ids = None ) Supprime des chunks sp\u00e9cifiques d'un document ou tous les chunks si aucun ID n'est sp\u00e9cifi\u00e9. Cette fonction supprime des chunks et met \u00e0 jour les statistiques d'index du corpus. L'index devient invalide apr\u00e8s suppression de chunks et n\u00e9cessite une reconstruction. Parameters: document_id ( int ) \u2013 ID du document. chunk_ids ( Optional [ List [ int ]] , default: None ) \u2013 Liste des IDs des chunks \u00e0 supprimer (si None, supprime tous les chunks). Returns: Dict [ str , Any ] \u2013 Dict contenant le statut de l'op\u00e9ration et des informations sur l'index. get_documents ( theme = None , document_type = None , corpus_id = None , skip = 0 , limit = 100 , db = None ) Liste l'ensemble des documents de la base de donn\u00e9es avec leur nombre de chunks. Cette fonction permet de r\u00e9cup\u00e9rer un ensemble pagin\u00e9 de documents avec possibilit\u00e9 de filtrage sur diff\u00e9rents crit\u00e8res. Parameters: theme ( Optional [ str ] , default: None ) \u2013 Filtre optionnel pour le th\u00e8me du document. document_type ( Optional [ str ] , default: None ) \u2013 Filtre optionnel pour le type du document. corpus_id ( Optional [ str ] , default: None ) \u2013 Filtre optionnel sur l'identifiant du corpus. skip ( int , default: 0 ) \u2013 Nombre de documents \u00e0 ignorer (pour la pagination). limit ( int , default: 100 ) \u2013 Nombre maximal de documents \u00e0 retourner. db ( Session , default: None ) \u2013 Session de base de donn\u00e9es fournie par d\u00e9pendance. Returns: List [ DocumentResponse ] \u2013 Liste des documents format\u00e9s avec leur nombre de chunks associ\u00e9s. update_document_with_chunks ( document_update , new_chunks = None ) Met \u00e0 jour un document existant et ses chunks dans la base de donn\u00e9es. Parameters: document_update ( DocumentUpdate ) \u2013 Donn\u00e9es de mise \u00e0 jour du document. new_chunks ( Optional [ List [ Dict [ str , Any ]]] , default: None ) \u2013 Nouveaux chunks \u00e0 ajouter ou \u00e0 remplacer (si replace_chunks=True). Returns: Dict [ str , Any ] \u2013 Dict[str, Any]: R\u00e9sultat de l'op\u00e9ration avec les informations mises \u00e0 jour.","title":"CRUD"},{"location":"api/lib/vectordb/crud_references/#references-for-crud","text":"","title":"References for CRUD"},{"location":"api/lib/vectordb/crud_references/#vectordb.src.crud.add_document_with_chunks","text":"Ajoute un document et ses chunks \u00e0 la base de donn\u00e9es en g\u00e9n\u00e9rant les embeddings en mode lot. Cette fonction ins\u00e8re un document et tous ses chunks associ\u00e9s, g\u00e9n\u00e8re les embeddings par lots pour am\u00e9liorer les performances, et pr\u00e9serve les relations hi\u00e9rarchiques entre les chunks. Parameters: db ( Session ) \u2013 Session SQLAlchemy active. doc ( DocumentCreate ) \u2013 Donn\u00e9es du document \u00e0 cr\u00e9er. chunks ( List [ Dict [ str , Any ]] ) \u2013 Liste des chunks d\u00e9finis par l'utilisateur. batch_size ( int , default: 10 ) \u2013 Nombre de chunks \u00e0 traiter par lot. Returns: Dict [ str , Any ] \u2013 Dict contenant l'ID du document, le nombre de chunks, le corpus_id et si un index doit \u00eatre cr\u00e9\u00e9.","title":"add_document_with_chunks"},{"location":"api/lib/vectordb/crud_references/#vectordb.src.crud.delete_document","text":"Supprime un document de la base de donn\u00e9es avec tous ses chunks associ\u00e9s. Cette fonction supprime un document et tous ses chunks. Si le document est le dernier de son corpus, l'index vectoriel associ\u00e9 est \u00e9galement supprim\u00e9. Parameters: document_id ( int ) \u2013 Identifiant du document \u00e0 supprimer. Returns: Dict [ str , Any ] \u2013 Dict contenant le statut de l'op\u00e9ration et les actions r\u00e9alis\u00e9es.","title":"delete_document"},{"location":"api/lib/vectordb/crud_references/#vectordb.src.crud.delete_document_chunks","text":"Supprime des chunks sp\u00e9cifiques d'un document ou tous les chunks si aucun ID n'est sp\u00e9cifi\u00e9. Cette fonction supprime des chunks et met \u00e0 jour les statistiques d'index du corpus. L'index devient invalide apr\u00e8s suppression de chunks et n\u00e9cessite une reconstruction. Parameters: document_id ( int ) \u2013 ID du document. chunk_ids ( Optional [ List [ int ]] , default: None ) \u2013 Liste des IDs des chunks \u00e0 supprimer (si None, supprime tous les chunks). Returns: Dict [ str , Any ] \u2013 Dict contenant le statut de l'op\u00e9ration et des informations sur l'index.","title":"delete_document_chunks"},{"location":"api/lib/vectordb/crud_references/#vectordb.src.crud.get_documents","text":"Liste l'ensemble des documents de la base de donn\u00e9es avec leur nombre de chunks. Cette fonction permet de r\u00e9cup\u00e9rer un ensemble pagin\u00e9 de documents avec possibilit\u00e9 de filtrage sur diff\u00e9rents crit\u00e8res. Parameters: theme ( Optional [ str ] , default: None ) \u2013 Filtre optionnel pour le th\u00e8me du document. document_type ( Optional [ str ] , default: None ) \u2013 Filtre optionnel pour le type du document. corpus_id ( Optional [ str ] , default: None ) \u2013 Filtre optionnel sur l'identifiant du corpus. skip ( int , default: 0 ) \u2013 Nombre de documents \u00e0 ignorer (pour la pagination). limit ( int , default: 100 ) \u2013 Nombre maximal de documents \u00e0 retourner. db ( Session , default: None ) \u2013 Session de base de donn\u00e9es fournie par d\u00e9pendance. Returns: List [ DocumentResponse ] \u2013 Liste des documents format\u00e9s avec leur nombre de chunks associ\u00e9s.","title":"get_documents"},{"location":"api/lib/vectordb/crud_references/#vectordb.src.crud.update_document_with_chunks","text":"Met \u00e0 jour un document existant et ses chunks dans la base de donn\u00e9es. Parameters: document_update ( DocumentUpdate ) \u2013 Donn\u00e9es de mise \u00e0 jour du document. new_chunks ( Optional [ List [ Dict [ str , Any ]]] , default: None ) \u2013 Nouveaux chunks \u00e0 ajouter ou \u00e0 remplacer (si replace_chunks=True). Returns: Dict [ str , Any ] \u2013 Dict[str, Any]: R\u00e9sultat de l'op\u00e9ration avec les informations mises \u00e0 jour.","title":"update_document_with_chunks"},{"location":"api/lib/vectordb/index_references/","text":"References for INDEX Module de gestion des index vectoriels pour pgvector. Ce module fournit des fonctions simples pour cr\u00e9er et g\u00e9rer des index vectoriels sur les chunks de documents. check_all_indexes () V\u00e9rifie l'\u00e9tat de tous les index vectoriels. Returns: dict ( dict ) \u2013 \u00c9tat des index pour tous les corpus. check_index_status ( corpus_id ) V\u00e9rifie l'\u00e9tat de l'index pour un corpus. Parameters: corpus_id ( str ) \u2013 Identifiant UUID du corpus. Returns: dict ( dict ) \u2013 \u00c9tat de l'index et m\u00e9tadonn\u00e9es. create_simple_index ( corpus_id ) Cr\u00e9e un index vectoriel simple pour un corpus donn\u00e9. Cette fonction cr\u00e9e un index IVFFLAT standard pour acc\u00e9l\u00e9rer les recherches vectorielles sur un corpus sp\u00e9cifique. Parameters: corpus_id ( str ) \u2013 Identifiant UUID du corpus \u00e0 indexer. Returns: dict ( dict ) \u2013 R\u00e9sultat de l'op\u00e9ration avec statut et message. drop_index ( corpus_id ) Supprime l'index vectoriel pour un corpus. Parameters: corpus_id ( str ) \u2013 Identifiant UUID du corpus. Returns: dict ( dict ) \u2013 R\u00e9sultat de l'op\u00e9ration.","title":"Indexation"},{"location":"api/lib/vectordb/index_references/#references-for-index","text":"Module de gestion des index vectoriels pour pgvector. Ce module fournit des fonctions simples pour cr\u00e9er et g\u00e9rer des index vectoriels sur les chunks de documents.","title":"References for INDEX"},{"location":"api/lib/vectordb/index_references/#vectordb.src.index_manager.check_all_indexes","text":"V\u00e9rifie l'\u00e9tat de tous les index vectoriels. Returns: dict ( dict ) \u2013 \u00c9tat des index pour tous les corpus.","title":"check_all_indexes"},{"location":"api/lib/vectordb/index_references/#vectordb.src.index_manager.check_index_status","text":"V\u00e9rifie l'\u00e9tat de l'index pour un corpus. Parameters: corpus_id ( str ) \u2013 Identifiant UUID du corpus. Returns: dict ( dict ) \u2013 \u00c9tat de l'index et m\u00e9tadonn\u00e9es.","title":"check_index_status"},{"location":"api/lib/vectordb/index_references/#vectordb.src.index_manager.create_simple_index","text":"Cr\u00e9e un index vectoriel simple pour un corpus donn\u00e9. Cette fonction cr\u00e9e un index IVFFLAT standard pour acc\u00e9l\u00e9rer les recherches vectorielles sur un corpus sp\u00e9cifique. Parameters: corpus_id ( str ) \u2013 Identifiant UUID du corpus \u00e0 indexer. Returns: dict ( dict ) \u2013 R\u00e9sultat de l'op\u00e9ration avec statut et message.","title":"create_simple_index"},{"location":"api/lib/vectordb/index_references/#vectordb.src.index_manager.drop_index","text":"Supprime l'index vectoriel pour un corpus. Parameters: corpus_id ( str ) \u2013 Identifiant UUID du corpus. Returns: dict ( dict ) \u2013 R\u00e9sultat de l'op\u00e9ration.","title":"drop_index"},{"location":"api/lib/vectordb/search_references/","text":"References for SEARCH search.py \u2013 Recherche hybride s\u00e9mantique et par m\u00e9tadonn\u00e9es. Framework de recherche documentaire hybride pour PostgreSQL + pgvector SearchEngine Moteur de recherche hybride (m\u00e9tadonn\u00e9es + vecteur) avec rerank optionnel via Cross-Encoder. Utilise \u00e0 la fois une recherche par similarit\u00e9 vectorielle et un rerank avec un mod\u00e8le Cross-Encoder. Inclut une \u00e9valuation de la pertinence des r\u00e9sultats et d\u00e9tection des requ\u00eates hors domaine. __init__ ( min_relevance_threshold =- 0.0 , high_confidence_threshold = 0.0 ) Initialise le moteur de recherche avec son g\u00e9n\u00e9rateur d'embeddings et son ranker. Parameters: min_relevance_threshold ( float , default: -0.0 ) \u2013 Score minimal pour qu'un r\u00e9sultat soit consid\u00e9r\u00e9 pertinent. high_confidence_threshold ( float , default: 0.0 ) \u2013 Score minimal pour une confiance \u00e9lev\u00e9e. evaluate_confidence ( scores ) \u00c9value la confiance globale dans les r\u00e9sultats de recherche. Analyse les scores du ranker pour d\u00e9terminer la pertinence globale des r\u00e9sultats et d\u00e9tecter les requ\u00eates potentiellement hors domaine. Parameters: scores ( List [ float ] ) \u2013 Liste des scores de pertinence. Returns: ConfidenceMetrics \u2013 M\u00e9triques de confiance avec niveau, message explicatif et statistiques. hybrid_search ( db , req ) Ex\u00e9cute une recherche hybride avec \u00e9valuation de confiance. Combine recherche vectorielle et filtrage SQL, puis applique un reranking avec un mod\u00e8le Cross-Encoder et \u00e9value la pertinence des r\u00e9sultats. Parameters: db ( Session ) \u2013 Session SQLAlchemy. req ( SearchRequest ) \u2013 Param\u00e8tres de la recherche. Returns: SearchResponse \u2013 R\u00e9ponse format\u00e9e avec r\u00e9sultats tri\u00e9s, m\u00e9triques de confiance et statistiques. log_search_query ( db , req , response ) Enregistre la recherche effectu\u00e9e dans l'historique. Cette m\u00e9thode persiste les m\u00e9tadonn\u00e9es de la requ\u00eate et de ses r\u00e9sultats pour permettre l'analyse statistique ult\u00e9rieure. Parameters: db ( Session ) \u2013 Session SQLAlchemy active. req ( SearchRequest ) \u2013 Requ\u00eate de recherche soumise. response ( SearchResponse ) \u2013 R\u00e9ponse g\u00e9n\u00e9r\u00e9e contenant les r\u00e9sultats et m\u00e9triques. normalize_scores ( scores ) Normalise les scores en valeurs entre 0 et 1. Parameters: scores ( List [ float ] ) \u2013 Liste des scores bruts du mod\u00e8le. Returns: List [ float ] \u2013 Liste de scores normalis\u00e9s entre 0 et 1.","title":"Recherche"},{"location":"api/lib/vectordb/search_references/#references-for-search","text":"search.py \u2013 Recherche hybride s\u00e9mantique et par m\u00e9tadonn\u00e9es. Framework de recherche documentaire hybride pour PostgreSQL + pgvector","title":"References for SEARCH"},{"location":"api/lib/vectordb/search_references/#vectordb.src.search.SearchEngine","text":"Moteur de recherche hybride (m\u00e9tadonn\u00e9es + vecteur) avec rerank optionnel via Cross-Encoder. Utilise \u00e0 la fois une recherche par similarit\u00e9 vectorielle et un rerank avec un mod\u00e8le Cross-Encoder. Inclut une \u00e9valuation de la pertinence des r\u00e9sultats et d\u00e9tection des requ\u00eates hors domaine.","title":"SearchEngine"},{"location":"api/lib/vectordb/search_references/#vectordb.src.search.SearchEngine.__init__","text":"Initialise le moteur de recherche avec son g\u00e9n\u00e9rateur d'embeddings et son ranker. Parameters: min_relevance_threshold ( float , default: -0.0 ) \u2013 Score minimal pour qu'un r\u00e9sultat soit consid\u00e9r\u00e9 pertinent. high_confidence_threshold ( float , default: 0.0 ) \u2013 Score minimal pour une confiance \u00e9lev\u00e9e.","title":"__init__"},{"location":"api/lib/vectordb/search_references/#vectordb.src.search.SearchEngine.evaluate_confidence","text":"\u00c9value la confiance globale dans les r\u00e9sultats de recherche. Analyse les scores du ranker pour d\u00e9terminer la pertinence globale des r\u00e9sultats et d\u00e9tecter les requ\u00eates potentiellement hors domaine. Parameters: scores ( List [ float ] ) \u2013 Liste des scores de pertinence. Returns: ConfidenceMetrics \u2013 M\u00e9triques de confiance avec niveau, message explicatif et statistiques.","title":"evaluate_confidence"},{"location":"api/lib/vectordb/search_references/#vectordb.src.search.SearchEngine.hybrid_search","text":"Ex\u00e9cute une recherche hybride avec \u00e9valuation de confiance. Combine recherche vectorielle et filtrage SQL, puis applique un reranking avec un mod\u00e8le Cross-Encoder et \u00e9value la pertinence des r\u00e9sultats. Parameters: db ( Session ) \u2013 Session SQLAlchemy. req ( SearchRequest ) \u2013 Param\u00e8tres de la recherche. Returns: SearchResponse \u2013 R\u00e9ponse format\u00e9e avec r\u00e9sultats tri\u00e9s, m\u00e9triques de confiance et statistiques.","title":"hybrid_search"},{"location":"api/lib/vectordb/search_references/#vectordb.src.search.SearchEngine.log_search_query","text":"Enregistre la recherche effectu\u00e9e dans l'historique. Cette m\u00e9thode persiste les m\u00e9tadonn\u00e9es de la requ\u00eate et de ses r\u00e9sultats pour permettre l'analyse statistique ult\u00e9rieure. Parameters: db ( Session ) \u2013 Session SQLAlchemy active. req ( SearchRequest ) \u2013 Requ\u00eate de recherche soumise. response ( SearchResponse ) \u2013 R\u00e9ponse g\u00e9n\u00e9r\u00e9e contenant les r\u00e9sultats et m\u00e9triques.","title":"log_search_query"},{"location":"api/lib/vectordb/search_references/#vectordb.src.search.SearchEngine.normalize_scores","text":"Normalise les scores en valeurs entre 0 et 1. Parameters: scores ( List [ float ] ) \u2013 Liste des scores bruts du mod\u00e8le. Returns: List [ float ] \u2013 Liste de scores normalis\u00e9s entre 0 et 1.","title":"normalize_scores"},{"location":"api/rest/rest_api/","text":"REST API Documentation Cl\u00e9a API VERSION API pour g\u00e9rer les documents et effectuer des recherches s\u00e9mantiques. Database POST /database/documents Ajouter un document avec ses chunks Description Ins\u00e8re le document puis ses chunks dans une transaction unique. Args: payload: Objet contenant les donn\u00e9es du document et ses chunks. db: Session de base de donn\u00e9es inject\u00e9e par d\u00e9pendance. Returns: DocumentResponse: Document cr\u00e9\u00e9 avec le nombre de chunks associ\u00e9s. Raises: HTTPException: Si une erreur survient pendant l'insertion. Request body application/json { \"document\" : { \"title\" : \"string\" , \"theme\" : \"string\" , \"documentType\" : \"string\" , \"publishDate\" : \"2022-04-13\" , \"corpusId\" : null }, \"chunks\" : [ { \"id\" : null , \"content\" : \"string\" , \"startChar\" : 0 , \"endChar\" : 0 , \"hierarchyLevel\" : 0 , \"parentChunkId\" : null } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the request body { \"properties\" : { \"document\" : { \"$ref\" : \"#/components/schemas/DocumentCreate\" }, \"chunks\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ChunkCreate\" }, \"type\" : \"array\" , \"title\" : \"Chunks\" } }, \"type\" : \"object\" , \"required\" : [ \"document\" , \"chunks\" ], \"title\" : \"DocumentWithChunks\" , \"description\" : \"Payload complet pour `POST /database/documents`.\" } Response 200 OK application/json { \"id\" : 0 , \"title\" : \"string\" , \"theme\" : \"string\" , \"documentType\" : \"string\" , \"publishDate\" : \"2022-04-13\" , \"corpusId\" : null , \"chunkCount\" : 0 , \"indexNeeded\" : true } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"id\" : { \"type\" : \"integer\" , \"title\" : \"Id\" }, \"title\" : { \"type\" : \"string\" , \"title\" : \"Title\" }, \"theme\" : { \"type\" : \"string\" , \"title\" : \"Theme\" }, \"documentType\" : { \"type\" : \"string\" , \"title\" : \"Documenttype\" }, \"publishDate\" : { \"type\" : \"string\" , \"format\" : \"date\" , \"title\" : \"Publishdate\" }, \"corpusId\" : { \"anyOf\" : [ { \"type\" : \"string\" }, { \"type\" : \"null\" } ], \"title\" : \"Corpusid\" }, \"chunkCount\" : { \"type\" : \"integer\" , \"minimum\" : 0 , \"title\" : \"Chunkcount\" }, \"indexNeeded\" : { \"type\" : \"boolean\" , \"title\" : \"Indexneeded\" , \"default\" : false } }, \"type\" : \"object\" , \"required\" : [ \"id\" , \"title\" , \"theme\" , \"documentType\" , \"publishDate\" , \"chunkCount\" ], \"title\" : \"DocumentResponse\" , \"description\" : \"R\u00e9ponse standard lorsqu\u2019un document est renvoy\u00e9 c\u00f4t\u00e9 API.\\n\\nAttributs:\\n id (int): Identifiant du document.\\n title (str): Titre du document.\\n theme (str): Th\u00e8me du document.\\n document_type (str): Type de document.\\n publish_date (date): Date de publication.\\n corpus_id (Optional[str]): Identifiant du corpus.\\n chunk_count (int): Nombre de chunks associ\u00e9s (>= 0).\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" } GET /database/documents Lister les documents Description Liste l'ensemble des documents de la base de donn\u00e9es avec leur nombre de chunks. Cette fonction permet de r\u00e9cup\u00e9rer un ensemble pagin\u00e9 de documents avec possibilit\u00e9 de filtrage sur diff\u00e9rents crit\u00e8res. Args: theme: Filtre optionnel pour le th\u00e8me du document. document_type: Filtre optionnel pour le type du document. corpus_id: Filtre optionnel sur l'identifiant du corpus. skip: Nombre de documents \u00e0 ignorer (pour la pagination). limit: Nombre maximal de documents \u00e0 retourner. db: Session de base de donn\u00e9es fournie par d\u00e9pendance. Returns: Liste des documents format\u00e9s avec leur nombre de chunks associ\u00e9s. Input parameters Parameter In Type Default Nullable Description corpusId query None No documentType query None No limit query integer 100 No skip query integer 0 No theme query None No Response 200 OK application/json [ { \"id\" : 0 , \"title\" : \"string\" , \"theme\" : \"string\" , \"documentType\" : \"string\" , \"publishDate\" : \"2022-04-13\" , \"corpusId\" : null , \"chunkCount\" : 0 , \"indexNeeded\" : true } ] \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"type\" : \"array\" , \"items\" : { \"$ref\" : \"#/components/schemas/DocumentResponse\" }, \"title\" : \"Response List Documents Database Documents Get\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" } PUT /database/documents/ {document_id} Mettre \u00e0 jour un document (et/ou ajouter des chunks) Description Met \u00e0 jour les m\u00e9tadonn\u00e9es d'un document et peut ajouter de nouveaux chunks. Cette fonction permet de modifier les informations d'un document existant et d'y ajouter de nouveaux fragments de texte (chunks) en une seule op\u00e9ration. Args: payload: Objet de mise \u00e0 jour contenant le document et \u00e9ventuellement des nouveaux chunks. document_id: Identifiant num\u00e9rique du document \u00e0 mettre \u00e0 jour (\u2265 1). db: Session de base de donn\u00e9es inject\u00e9e par d\u00e9pendance. Returns: DocumentResponse: Document mis \u00e0 jour avec le nombre total de chunks associ\u00e9s. Raises: HTTPException: - 404: Si le document n'existe pas dans la base Input parameters Parameter In Type Default Nullable Description document_id path integer No Identifiant du document \u00e0 mettre \u00e0 jour (>= 1). Request body application/json { \"document\" : { \"id\" : 0 , \"title\" : null , \"theme\" : null , \"documentType\" : null , \"publishDate\" : null , \"corpusId\" : null }, \"newChunks\" : null } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the request body { \"properties\" : { \"document\" : { \"$ref\" : \"#/components/schemas/DocumentUpdate\" }, \"newChunks\" : { \"anyOf\" : [ { \"items\" : { \"$ref\" : \"#/components/schemas/ChunkCreate\" }, \"type\" : \"array\" }, { \"type\" : \"null\" } ], \"title\" : \"Newchunks\" } }, \"type\" : \"object\" , \"required\" : [ \"document\" ], \"title\" : \"UpdateWithChunks\" , \"description\" : \"Payload de mise-\u00e0-jour :\\n\\n* `document` \u2192 DTO `DocumentUpdate`\\n* `newChunks` \u2192 \u00e9ventuelle liste de nouveaux chunks \u00e0 ajouter\" } Response 200 OK application/json { \"id\" : 0 , \"title\" : \"string\" , \"theme\" : \"string\" , \"documentType\" : \"string\" , \"publishDate\" : \"2022-04-13\" , \"corpusId\" : null , \"chunkCount\" : 0 , \"indexNeeded\" : true } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"id\" : { \"type\" : \"integer\" , \"title\" : \"Id\" }, \"title\" : { \"type\" : \"string\" , \"title\" : \"Title\" }, \"theme\" : { \"type\" : \"string\" , \"title\" : \"Theme\" }, \"documentType\" : { \"type\" : \"string\" , \"title\" : \"Documenttype\" }, \"publishDate\" : { \"type\" : \"string\" , \"format\" : \"date\" , \"title\" : \"Publishdate\" }, \"corpusId\" : { \"anyOf\" : [ { \"type\" : \"string\" }, { \"type\" : \"null\" } ], \"title\" : \"Corpusid\" }, \"chunkCount\" : { \"type\" : \"integer\" , \"minimum\" : 0 , \"title\" : \"Chunkcount\" }, \"indexNeeded\" : { \"type\" : \"boolean\" , \"title\" : \"Indexneeded\" , \"default\" : false } }, \"type\" : \"object\" , \"required\" : [ \"id\" , \"title\" , \"theme\" , \"documentType\" , \"publishDate\" , \"chunkCount\" ], \"title\" : \"DocumentResponse\" , \"description\" : \"R\u00e9ponse standard lorsqu\u2019un document est renvoy\u00e9 c\u00f4t\u00e9 API.\\n\\nAttributs:\\n id (int): Identifiant du document.\\n title (str): Titre du document.\\n theme (str): Th\u00e8me du document.\\n document_type (str): Type de document.\\n publish_date (date): Date de publication.\\n corpus_id (Optional[str]): Identifiant du corpus.\\n chunk_count (int): Nombre de chunks associ\u00e9s (>= 0).\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" } DELETE /database/documents/ {document_id} Supprimer un document et tous ses chunks Input parameters Parameter In Type Default Nullable Description document_id path integer No Response 200 OK application/json Schema of the response body { \"type\" : \"object\" , \"additionalProperties\" : true , \"title\" : \"Response Remove Document Database Documents Document Id Delete\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" } GET /database/documents/ {document_id} R\u00e9cup\u00e9rer un document Description R\u00e9cup\u00e8re un document \u00e0 partir de son identifiant et le formate en DocumentResponse. Args: document_id (int): Identifiant du document \u00e0 r\u00e9cup\u00e9rer. db (Session): Session de base de donn\u00e9es fournie par d\u00e9pendance. Returns: DocumentResponse: Document format\u00e9 avec le nombre de chunks associ\u00e9s. Raises: HTTPException: Si le document n'existe pas dans la base. Input parameters Parameter In Type Default Nullable Description document_id path integer No Response 200 OK application/json { \"id\" : 0 , \"title\" : \"string\" , \"theme\" : \"string\" , \"documentType\" : \"string\" , \"publishDate\" : \"2022-04-13\" , \"corpusId\" : null , \"chunkCount\" : 0 , \"indexNeeded\" : true } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"id\" : { \"type\" : \"integer\" , \"title\" : \"Id\" }, \"title\" : { \"type\" : \"string\" , \"title\" : \"Title\" }, \"theme\" : { \"type\" : \"string\" , \"title\" : \"Theme\" }, \"documentType\" : { \"type\" : \"string\" , \"title\" : \"Documenttype\" }, \"publishDate\" : { \"type\" : \"string\" , \"format\" : \"date\" , \"title\" : \"Publishdate\" }, \"corpusId\" : { \"anyOf\" : [ { \"type\" : \"string\" }, { \"type\" : \"null\" } ], \"title\" : \"Corpusid\" }, \"chunkCount\" : { \"type\" : \"integer\" , \"minimum\" : 0 , \"title\" : \"Chunkcount\" }, \"indexNeeded\" : { \"type\" : \"boolean\" , \"title\" : \"Indexneeded\" , \"default\" : false } }, \"type\" : \"object\" , \"required\" : [ \"id\" , \"title\" , \"theme\" , \"documentType\" , \"publishDate\" , \"chunkCount\" ], \"title\" : \"DocumentResponse\" , \"description\" : \"R\u00e9ponse standard lorsqu\u2019un document est renvoy\u00e9 c\u00f4t\u00e9 API.\\n\\nAttributs:\\n id (int): Identifiant du document.\\n title (str): Titre du document.\\n theme (str): Th\u00e8me du document.\\n document_type (str): Type de document.\\n publish_date (date): Date de publication.\\n corpus_id (Optional[str]): Identifiant du corpus.\\n chunk_count (int): Nombre de chunks associ\u00e9s (>= 0).\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" } DELETE /database/documents/ {document_id} /chunks Supprimer des chunks d'un document Input parameters Parameter In Type Default Nullable Description chunk_ids query None No IDs \u00e0 supprimer ; vide \u21d2 tous les chunks du document document_id path integer No Response 200 OK application/json Schema of the response body { \"type\" : \"object\" , \"additionalProperties\" : true , \"title\" : \"Response Remove Chunks Database Documents Document Id Chunks Delete\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" } GET /database/documents/ {document_id} /chunks R\u00e9cup\u00e9rer les chunks d'un document Input parameters Parameter In Type Default Nullable Description document_id path integer No hierarchyLevel query None No limit query integer 100 No parentChunkId query None No skip query integer 0 No Response 200 OK application/json [ null ] \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"type\" : \"array\" , \"items\" : {}, \"title\" : \"Response Get Chunks Database Documents Document Id Chunks Get\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" } Search POST /search/hybrid_search Recherche hybride (vecteur + filtres) Description Retourne les top k chunks les plus pertinents avec \u00e9valuation de confiance. Le moteur combine : Filtres SQL (theme, document_type , dates, corpus_id ) Index vectoriel pgvector (IVFFLAT ou HNSW) Rerank Cross-Encoder sur un sous-ensemble \u00e9largi ( top k \u00d7 3 ) \u00c9valuation de confiance analyse la pertinence globale des r\u00e9sultats Filtrage de pertinence \u00e9limine les r\u00e9sultats sous le seuil minimal Normalisation des scores facilite l'interpr\u00e9tation (0-1) Les r\u00e9sultats incluent des m\u00e9triques de confiance qui permettent d'identifier les requ\u00eates hors du domaine de connaissances. Request body application/json { \"query\" : \"string\" , \"topK\" : 0 , \"theme\" : null , \"documentType\" : null , \"startDate\" : null , \"endDate\" : null , \"corpusId\" : null , \"hierarchyLevel\" : null , \"hierarchical\" : true , \"filterByRelevance\" : true , \"normalizeScores\" : true } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the request body { \"properties\" : { \"query\" : { \"type\" : \"string\" , \"title\" : \"Query\" , \"description\" : \"Requ\u00eate en langage naturel\" }, \"topK\" : { \"type\" : \"integer\" , \"title\" : \"Topk\" , \"description\" : \"Nombre de r\u00e9sultats \u00e0 retourner\" , \"default\" : 10 }, \"theme\" : { \"anyOf\" : [ { \"type\" : \"string\" }, { \"type\" : \"null\" } ], \"title\" : \"Theme\" , \"description\" : \"Filtre par th\u00e8me\" }, \"documentType\" : { \"anyOf\" : [ { \"type\" : \"string\" }, { \"type\" : \"null\" } ], \"title\" : \"Documenttype\" , \"description\" : \"Filtre par type de document\" }, \"startDate\" : { \"anyOf\" : [ { \"type\" : \"string\" , \"format\" : \"date-time\" }, { \"type\" : \"null\" } ], \"title\" : \"Startdate\" , \"description\" : \"Date de d\u00e9but\" }, \"endDate\" : { \"anyOf\" : [ { \"type\" : \"string\" , \"format\" : \"date-time\" }, { \"type\" : \"null\" } ], \"title\" : \"Enddate\" , \"description\" : \"Date de fin\" }, \"corpusId\" : { \"anyOf\" : [ { \"type\" : \"integer\" }, { \"type\" : \"null\" } ], \"title\" : \"Corpusid\" , \"description\" : \"ID du corpus\" }, \"hierarchyLevel\" : { \"anyOf\" : [ { \"type\" : \"integer\" }, { \"type\" : \"null\" } ], \"title\" : \"Hierarchylevel\" , \"description\" : \"Niveau hi\u00e9rarchique (0-2)\" }, \"hierarchical\" : { \"type\" : \"boolean\" , \"title\" : \"Hierarchical\" , \"description\" : \"R\u00e9cup\u00e9rer le contexte hi\u00e9rarchique\" , \"default\" : false }, \"filterByRelevance\" : { \"type\" : \"boolean\" , \"title\" : \"Filterbyrelevance\" , \"description\" : \"Filtrer les r\u00e9sultats sous le seuil de pertinence\" , \"default\" : false }, \"normalizeScores\" : { \"type\" : \"boolean\" , \"title\" : \"Normalizescores\" , \"description\" : \"Normaliser les scores entre 0 et 1\" , \"default\" : false } }, \"type\" : \"object\" , \"required\" : [ \"query\" ], \"title\" : \"SearchRequest\" , \"description\" : \"Param\u00e8tres pour la recherche hybride.\\n\\nCombine la requ\u00eate textuelle avec des filtres de m\u00e9tadonn\u00e9es optionnels.\" } Response 200 OK application/json { \"query\" : \"string\" , \"topK\" : 0 , \"totalResults\" : 0 , \"results\" : [ { \"chunkId\" : 0 , \"documentId\" : 0 , \"title\" : \"string\" , \"content\" : \"string\" , \"theme\" : \"string\" , \"documentType\" : \"string\" , \"publishDate\" : \"2022-04-13\" , \"score\" : 10.12 , \"hierarchyLevel\" : 0 , \"context\" : null } ], \"confidence\" : null , \"normalized\" : true , \"message\" : null } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"query\" : { \"type\" : \"string\" , \"title\" : \"Query\" , \"description\" : \"Requ\u00eate originale\" }, \"topK\" : { \"type\" : \"integer\" , \"title\" : \"Topk\" , \"description\" : \"Nombre de r\u00e9sultats demand\u00e9s\" }, \"totalResults\" : { \"type\" : \"integer\" , \"title\" : \"Totalresults\" , \"description\" : \"Nombre total de r\u00e9sultats trouv\u00e9s\" }, \"results\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ChunkResult\" }, \"type\" : \"array\" , \"title\" : \"Results\" , \"description\" : \"R\u00e9sultats de la recherche\" }, \"confidence\" : { \"anyOf\" : [ { \"$ref\" : \"#/components/schemas/ConfidenceMetrics\" }, { \"type\" : \"null\" } ], \"description\" : \"M\u00e9triques de confiance sur les r\u00e9sultats\" }, \"normalized\" : { \"type\" : \"boolean\" , \"title\" : \"Normalized\" , \"description\" : \"Indique si les scores sont normalis\u00e9s (0-1)\" , \"default\" : false }, \"message\" : { \"anyOf\" : [ { \"type\" : \"string\" }, { \"type\" : \"null\" } ], \"title\" : \"Message\" , \"description\" : \"Message informatif sur les r\u00e9sultats\" } }, \"type\" : \"object\" , \"required\" : [ \"query\" , \"topK\" , \"totalResults\" , \"results\" ], \"title\" : \"SearchResponse\" , \"description\" : \"R\u00e9ponse \u00e0 une requ\u00eate de recherche.\\n\\nContient les r\u00e9sultats tri\u00e9s par pertinence avec m\u00e9tadonn\u00e9es et \u00e9valuation de confiance.\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" } Index POST /index/create-index/ {corpus_id} Cr\u00e9er un index vectoriel pour un corpus Description Cr\u00e9e un index vectoriel pour un corpus sp\u00e9cifique. Cette fonction cr\u00e9e un index IVFFLAT simple pour acc\u00e9l\u00e9rer les recherches vectorielles sur le corpus sp\u00e9cifi\u00e9. Args: corpus_id: Identifiant UUID du corpus \u00e0 indexer. Returns: dict: R\u00e9sultat de l'op\u00e9ration avec statut et message. Raises: HTTPException: Si une erreur survient lors de la cr\u00e9ation de l'index. Input parameters Parameter In Type Default Nullable Description corpus_id path string No Identifiant UUID du corpus Response 200 OK application/json Schema of the response body { \"type\" : \"object\" , \"additionalProperties\" : true , \"title\" : \"Response Create Corpus Index Index Create Index Corpus Id Post\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" } DELETE /index/drop-index/ {corpus_id} Supprimer l'index vectoriel d'un corpus Description Supprime l'index vectoriel pour un corpus sp\u00e9cifique. Args: corpus_id: Identifiant UUID du corpus. Returns: dict: R\u00e9sultat de l'op\u00e9ration. Raises: HTTPException: Si une erreur survient lors de la suppression de l'index. Input parameters Parameter In Type Default Nullable Description corpus_id path string No Identifiant UUID du corpus Response 200 OK application/json Schema of the response body { \"type\" : \"object\" , \"additionalProperties\" : true , \"title\" : \"Response Remove Corpus Index Index Drop Index Corpus Id Delete\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" } GET /index/index-status/ {corpus_id} V\u00e9rifier l'\u00e9tat de l'index pour un corpus Description V\u00e9rifie l'\u00e9tat de l'index pour un corpus sp\u00e9cifique. Args: corpus_id: Identifiant UUID du corpus. Returns: IndexStatus: \u00c9tat de l'index et m\u00e9tadonn\u00e9es. Raises: HTTPException: Si une erreur survient lors de la v\u00e9rification de l'\u00e9tat. Input parameters Parameter In Type Default Nullable Description corpus_id path string No Identifiant UUID du corpus Response 200 OK application/json { \"corpusId\" : null , \"indexExists\" : true , \"configExists\" : true , \"isIndexed\" : true , \"indexType\" : null , \"chunkCount\" : 0 , \"indexedChunks\" : 0 , \"lastIndexed\" : null } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"corpusId\" : { \"anyOf\" : [ { \"type\" : \"string\" }, { \"type\" : \"null\" } ], \"title\" : \"Corpusid\" }, \"indexExists\" : { \"type\" : \"boolean\" , \"title\" : \"Indexexists\" }, \"configExists\" : { \"type\" : \"boolean\" , \"title\" : \"Configexists\" }, \"isIndexed\" : { \"type\" : \"boolean\" , \"title\" : \"Isindexed\" }, \"indexType\" : { \"anyOf\" : [ { \"type\" : \"string\" }, { \"type\" : \"null\" } ], \"title\" : \"Indextype\" }, \"chunkCount\" : { \"type\" : \"integer\" , \"title\" : \"Chunkcount\" }, \"indexedChunks\" : { \"type\" : \"integer\" , \"title\" : \"Indexedchunks\" }, \"lastIndexed\" : { \"anyOf\" : [ { \"type\" : \"string\" , \"format\" : \"date\" }, { \"type\" : \"null\" } ], \"title\" : \"Lastindexed\" } }, \"type\" : \"object\" , \"required\" : [ \"corpusId\" , \"indexExists\" , \"configExists\" , \"isIndexed\" , \"indexType\" , \"chunkCount\" , \"indexedChunks\" , \"lastIndexed\" ], \"title\" : \"IndexStatus\" , \"description\" : \"Statut de l'indexation d'un document.\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" } GET /index/indexes V\u00e9rifier l'\u00e9tat de tous les index vectoriels Description V\u00e9rifie l'\u00e9tat de tous les index vectoriels. Returns: dict: \u00c9tat des index pour tous les corpus. Response 200 OK application/json Schema of the response body { \"additionalProperties\" : true , \"type\" : \"object\" , \"title\" : \"Response Get All Indexes Index Indexes Get\" } POST /index/cleanup-indexes Nettoyer les index vectoriels orphelins Description Nettoie les index vectoriels orphelins. Cette route identifie et supprime les configurations d'index et vues mat\u00e9rialis\u00e9es qui ne sont plus associ\u00e9es \u00e0 des corpus existants dans la base de donn\u00e9es. Returns: dict: R\u00e9sultat de l'op\u00e9ration avec statistiques. Response 200 OK application/json Schema of the response body { \"additionalProperties\" : true , \"type\" : \"object\" , \"title\" : \"Response Cleanup Orphaned Indexes Index Cleanup Indexes Post\" } DocLoader POST /doc_loader/upload-file Uploader un fichier et le traiter Description Uploade un fichier, l'extrait et le divise en chunks. Args: file (UploadFile): Fichier upload\u00e9 par l'utilisateur. max_length (int): Taille maximale d'un chunk. Par d\u00e9faut 1000. theme (str): Th\u00e8me du document. Par d\u00e9faut \"Th\u00e8me g\u00e9n\u00e9rique\". Returns: List[DocumentWithChunks]: Liste des documents extraits. Raises: HTTPException: Si une erreur survient lors du traitement ou si aucun contenu n'est extrait. Input parameters Parameter In Type Default Nullable Description max_length query integer 1000 No Taille maximale d'un chunk theme query string Th\u00e8me g\u00e9n\u00e9rique No Th\u00e8me du document Request body multipart/form-data { \"file\" : \"TG9yZW0gaXBzdW0gZG9sb3Igc2l0IGFtZXQ=\" } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the request body { \"properties\" : { \"file\" : { \"type\" : \"string\" , \"format\" : \"binary\" , \"title\" : \"File\" , \"description\" : \"Fichier \u00e0 traiter\" } }, \"type\" : \"object\" , \"required\" : [ \"file\" ], \"title\" : \"Body_upload_and_process_file_doc_loader_upload_file_post\" } Response 200 OK application/json { \"document\" : { \"title\" : \"string\" , \"theme\" : \"string\" , \"documentType\" : \"string\" , \"publishDate\" : \"2022-04-13\" , \"corpusId\" : null }, \"chunks\" : [ { \"id\" : null , \"content\" : \"string\" , \"startChar\" : 0 , \"endChar\" : 0 , \"hierarchyLevel\" : 0 , \"parentChunkId\" : null } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"document\" : { \"$ref\" : \"#/components/schemas/DocumentCreate\" }, \"chunks\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ChunkCreate\" }, \"type\" : \"array\" , \"title\" : \"Chunks\" } }, \"type\" : \"object\" , \"required\" : [ \"document\" , \"chunks\" ], \"title\" : \"DocumentWithChunks\" , \"description\" : \"Payload complet pour `POST /database/documents`.\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" } Pipeline POST /pipeline/process-and-store Charger un fichier, l'extraire et l'ins\u00e9rer dans la base de donn\u00e9es avec segmentation adaptative Description Charge un fichier, l'extrait avec segmentation hi\u00e9rarchique et l'ins\u00e8re dans la base de donn\u00e9es. Le fichier est temporairement sauvegard\u00e9 sur le disque, trait\u00e9 pour en extraire le contenu textuel, segment\u00e9 selon une approche hi\u00e9rarchique, puis ins\u00e9r\u00e9 dans la base de donn\u00e9es avec g\u00e9n\u00e9ration automatique d'embeddings. Args: file: Fichier upload\u00e9 par l'utilisateur. max_length: Taille maximale d'un chunk final. overlap: Chevauchement entre les chunks. theme: Th\u00e8me \u00e0 appliquer au document. corpus_id: Identifiant du corpus (g\u00e9n\u00e9r\u00e9 si non sp\u00e9cifi\u00e9). Returns: Dict: R\u00e9sultats de l'op\u00e9ration avec l'ID du document et les statistiques de segmentation. Raises: HTTPException: Si une erreur survient pendant le traitement du document. Input parameters Parameter In Type Default Nullable Description corpus_id query None No Identifiant du corpus (g\u00e9n\u00e9r\u00e9 si non sp\u00e9cifi\u00e9) max_length query integer 500 No Taille maximale d'un chunk overlap query integer 100 No Chevauchement entre les chunks theme query string Th\u00e8me g\u00e9n\u00e9rique No Th\u00e8me du document Request body multipart/form-data { \"file\" : \"TG9yZW0gaXBzdW0gZG9sb3Igc2l0IGFtZXQ=\" } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the request body { \"properties\" : { \"file\" : { \"type\" : \"string\" , \"format\" : \"binary\" , \"title\" : \"File\" , \"description\" : \"Fichier \u00e0 traiter\" } }, \"type\" : \"object\" , \"required\" : [ \"file\" ], \"title\" : \"Body_process_and_store_endpoint_pipeline_process_and_store_post\" } Response 200 OK application/json Schema of the response body { \"type\" : \"object\" , \"additionalProperties\" : true , \"title\" : \"Response Process And Store Endpoint Pipeline Process And Store Post\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" } POST /pipeline/process-and-store-async Traiter un fichier en arri\u00e8re-plan et l'ins\u00e9rer dans la base de donn\u00e9es Description Traite un fichier en arri\u00e8re-plan et l'ins\u00e8re dans la base de donn\u00e9es. Similaire \u00e0 process-and-store mais s'ex\u00e9cute de mani\u00e8re asynchrone pour les fichiers volumineux. Le client re\u00e7oit imm\u00e9diatement une r\u00e9ponse avec un identifiant de t\u00e2che pendant que le traitement se poursuit en arri\u00e8re-plan. Args: background_tasks: Gestionnaire de t\u00e2ches en arri\u00e8re-plan de FastAPI. file: Fichier upload\u00e9 par l'utilisateur. max_length: Taille maximale d'un chunk final. overlap: Chevauchement entre les chunks. theme: Th\u00e8me \u00e0 appliquer au document. corpus_id: Identifiant du corpus (g\u00e9n\u00e9r\u00e9 si non sp\u00e9cifi\u00e9). Returns: Dict: Informations sur la t\u00e2che en arri\u00e8re-plan cr\u00e9\u00e9e. Input parameters Parameter In Type Default Nullable Description corpus_id query None No Identifiant du corpus (g\u00e9n\u00e9r\u00e9 si non sp\u00e9cifi\u00e9) max_length query integer 500 No Taille maximale d'un chunk overlap query integer 100 No Chevauchement entre les chunks theme query string Th\u00e8me g\u00e9n\u00e9rique No Th\u00e8me du document Request body multipart/form-data { \"file\" : \"TG9yZW0gaXBzdW0gZG9sb3Igc2l0IGFtZXQ=\" } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the request body { \"properties\" : { \"file\" : { \"type\" : \"string\" , \"format\" : \"binary\" , \"title\" : \"File\" , \"description\" : \"Fichier \u00e0 traiter\" } }, \"type\" : \"object\" , \"required\" : [ \"file\" ], \"title\" : \"Body_process_and_store_async_endpoint_pipeline_process_and_store_async_post\" } Response 200 OK application/json Schema of the response body { \"type\" : \"object\" , \"additionalProperties\" : true , \"title\" : \"Response Process And Store Async Endpoint Pipeline Process And Store Async Post\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" } AskAI POST /askai/ask Ask Ai Description Endpoint pour poser une question et obtenir une r\u00e9ponse bas\u00e9e sur les documents. Cette fonction interroge la base documentaire avec la question fournie, r\u00e9cup\u00e8re les documents pertinents et utilise un mod\u00e8le LLM pour g\u00e9n\u00e9rer une r\u00e9ponse contextuelle. Args: request: Param\u00e8tres de la requ\u00eate (query, filters, etc.). db: Session de base de donn\u00e9es SQLAlchemy. search_engine: Instance du moteur de recherche vectorielle. Returns: Dict[str, Any] ou StreamingResponse: R\u00e9ponse g\u00e9n\u00e9r\u00e9e avec contexte. Raises: HTTPException: Si une erreur survient lors du traitement. Request body application/json { \"query\" : \"string\" , \"filters\" : null , \"theme\" : null , \"modelName\" : null , \"stream\" : true , \"promptType\" : \"string\" , \"enableThinking\" : true } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the request body { \"properties\" : { \"query\" : { \"type\" : \"string\" , \"title\" : \"Query\" , \"description\" : \"Question \u00e0 poser au syst\u00e8me\" }, \"filters\" : { \"anyOf\" : [ { \"additionalProperties\" : true , \"type\" : \"object\" }, { \"type\" : \"null\" } ], \"title\" : \"Filters\" , \"description\" : \"Filtres pour la recherche\" }, \"theme\" : { \"anyOf\" : [ { \"type\" : \"string\" }, { \"type\" : \"null\" } ], \"title\" : \"Theme\" , \"description\" : \"Th\u00e8me pour filtrer les documents\" }, \"modelName\" : { \"anyOf\" : [ { \"type\" : \"string\" }, { \"type\" : \"null\" } ], \"title\" : \"Modelname\" , \"description\" : \"Mod\u00e8le \u00e0 utiliser\" , \"default\" : \"Qwen3-0.6B\" }, \"stream\" : { \"type\" : \"boolean\" , \"title\" : \"Stream\" , \"description\" : \"R\u00e9ponse en streaming\" , \"default\" : false }, \"promptType\" : { \"type\" : \"string\" , \"title\" : \"Prompttype\" , \"description\" : \"Type de prompt (standard, summary, comparison)\" , \"default\" : \"standard\" }, \"enableThinking\" : { \"type\" : \"boolean\" , \"title\" : \"Enablethinking\" , \"description\" : \"Activer le mode 'think' pour le mod\u00e8le\" , \"default\" : false } }, \"type\" : \"object\" , \"required\" : [ \"query\" ], \"title\" : \"AskRequest\" , \"description\" : \"Requ\u00eate pour poser une question au syst\u00e8me RAG.\\n\\nArgs:\\n query: Question \u00e0 poser au syst\u00e8me.\\n filters: Filtres pour la recherche documentaire.\\n theme: Th\u00e8me pour filtrer les documents.\\n model_name: Nom du mod\u00e8le \u00e0 utiliser.\\n stream: Indique si la r\u00e9ponse doit \u00eatre stream\u00e9e.\\n prompt_type: Type de prompt \u00e0 utiliser.\\n\\nAttributes:\\n query (str): Question ou requ\u00eate de l'utilisateur.\\n filters (Optional[Dict[str, Any]]): Filtres pour la recherche documentaire.\\n theme (Optional[str]): Th\u00e8me pour filtrer les documents.\\n model_name (Optional[str]): Nom du mod\u00e8le \u00e0 utiliser.\\n stream (bool): Indique si la r\u00e9ponse doit \u00eatre stream\u00e9e.\\n prompt_type (str): Type de prompt \u00e0 utiliser ('standard', 'summary', 'comparison').\" } Response 200 OK application/json Schema of the response body Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" } GET /askai/models Get Models Description Endpoint pour r\u00e9cup\u00e9rer la liste des mod\u00e8les disponibles. Returns: List[str]: Liste des noms de mod\u00e8les disponibles. Response 200 OK application/json Schema of the response body Stats GET /stats/documents Statistiques sur les documents Description R\u00e9cup\u00e8re les statistiques sur les documents pr\u00e9sents dans la base de donn\u00e9es. Cette route calcule diff\u00e9rentes m\u00e9triques sur les documents index\u00e9s, notamment la distribution par th\u00e8me, par type de document, ainsi que les tendances r\u00e9centes d'ajout de documents. Args: skip: Nombre d'\u00e9l\u00e9ments \u00e0 ignorer pour la pagination. limit: Nombre maximal d'\u00e9l\u00e9ments \u00e0 retourner. db: Session de base de donn\u00e9es fournie par d\u00e9pendance. Returns: DocumentStats: Objet contenant les statistiques sur les documents. Raises: HTTPException: Si une erreur survient lors du calcul des statistiques. Input parameters Parameter In Type Default Nullable Description limit query integer 100 No Nombre maximal d'\u00e9l\u00e9ments \u00e0 retourner skip query integer 0 No Nombre d'\u00e9l\u00e9ments \u00e0 ignorer Response 200 OK application/json { \"totalCount\" : 0 , \"byTheme\" : {}, \"byType\" : {}, \"recentlyAdded\" : 0 , \"percentChange\" : 10.12 } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"totalCount\" : { \"type\" : \"integer\" , \"title\" : \"Totalcount\" }, \"byTheme\" : { \"additionalProperties\" : { \"type\" : \"integer\" }, \"type\" : \"object\" , \"title\" : \"Bytheme\" }, \"byType\" : { \"additionalProperties\" : { \"type\" : \"integer\" }, \"type\" : \"object\" , \"title\" : \"Bytype\" }, \"recentlyAdded\" : { \"type\" : \"integer\" , \"title\" : \"Recentlyadded\" }, \"percentChange\" : { \"type\" : \"number\" , \"title\" : \"Percentchange\" } }, \"type\" : \"object\" , \"required\" : [ \"totalCount\" , \"byTheme\" , \"byType\" , \"recentlyAdded\" , \"percentChange\" ], \"title\" : \"DocumentStats\" , \"description\" : \"Corps pour cr\u00e9er un chunk (texte + m\u00e9ta hi\u00e9rarchiques).\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" } GET /stats/searches Statistiques sur les recherches effectu\u00e9es Description R\u00e9cup\u00e8re les statistiques sur les recherches effectu\u00e9es dans le syst\u00e8me. Cette route analyse l'historique des recherches pour fournir des m\u00e9triques comme le nombre total de recherches, l'activit\u00e9 r\u00e9cente et les requ\u00eates les plus populaires. Args: skip: Nombre d'\u00e9l\u00e9ments \u00e0 ignorer pour la pagination. limit: Nombre maximal d'\u00e9l\u00e9ments \u00e0 retourner. db: Session de base de donn\u00e9es fournie par d\u00e9pendance. Returns: SearchStats: Objet contenant les statistiques sur les recherches. Raises: HTTPException: Si une erreur survient lors du calcul des statistiques. Input parameters Parameter In Type Default Nullable Description limit query integer 100 No Nombre maximal d'\u00e9l\u00e9ments \u00e0 retourner skip query integer 0 No Nombre d'\u00e9l\u00e9ments \u00e0 ignorer Response 200 OK application/json { \"totalCount\" : 0 , \"lastMonthCount\" : 0 , \"percentChange\" : 10.12 , \"topQueries\" : [ {} ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"totalCount\" : { \"type\" : \"integer\" , \"title\" : \"Totalcount\" }, \"lastMonthCount\" : { \"type\" : \"integer\" , \"title\" : \"Lastmonthcount\" }, \"percentChange\" : { \"type\" : \"number\" , \"title\" : \"Percentchange\" }, \"topQueries\" : { \"items\" : { \"additionalProperties\" : true , \"type\" : \"object\" }, \"type\" : \"array\" , \"title\" : \"Topqueries\" } }, \"type\" : \"object\" , \"required\" : [ \"totalCount\" , \"lastMonthCount\" , \"percentChange\" , \"topQueries\" ], \"title\" : \"SearchStats\" , \"description\" : \"Payload complet pour `POST /database/documents`.\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" } GET /stats/system Statistiques syst\u00e8me globales Description R\u00e9cup\u00e8re les statistiques syst\u00e8me globales. Cette route analyse les m\u00e9triques de confiance des recherches effectu\u00e9es et l'\u00e9tat des corpus dans le syst\u00e8me pour fournir une vue d'ensemble de la performance et de l'\u00e9tat de l'indexation. Args: db: Session de base de donn\u00e9es fournie par d\u00e9pendance. Returns: SystemStats: Objet contenant les m\u00e9triques syst\u00e8me. Raises: HTTPException: Si une erreur survient lors du calcul des statistiques. Response 200 OK application/json { \"satisfaction\" : 10.12 , \"avgConfidence\" : 10.12 , \"percentChange\" : 10.12 , \"indexedCorpora\" : 0 , \"totalCorpora\" : 0 } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"satisfaction\" : { \"type\" : \"number\" , \"title\" : \"Satisfaction\" }, \"avgConfidence\" : { \"type\" : \"number\" , \"title\" : \"Avgconfidence\" }, \"percentChange\" : { \"type\" : \"number\" , \"title\" : \"Percentchange\" }, \"indexedCorpora\" : { \"type\" : \"integer\" , \"title\" : \"Indexedcorpora\" }, \"totalCorpora\" : { \"type\" : \"integer\" , \"title\" : \"Totalcorpora\" } }, \"type\" : \"object\" , \"required\" : [ \"satisfaction\" , \"avgConfidence\" , \"percentChange\" , \"indexedCorpora\" , \"totalCorpora\" ], \"title\" : \"SystemStats\" , \"description\" : \"Payload complet pour `POST /database/documents`.\" } GET /stats/dashboard Toutes les statistiques pour le tableau de bord Description R\u00e9cup\u00e8re l'ensemble des statistiques pour le tableau de bord. Cette route agr\u00e8ge les r\u00e9sultats des diff\u00e9rentes fonctions de calcul de statistiques pour fournir un objet unique contenant toutes les m\u00e9triques n\u00e9cessaires au tableau de bord d'administration. Args: db: Session de base de donn\u00e9es fournie par d\u00e9pendance. Returns: DashboardStats: Objet contenant l'ensemble des statistiques. Raises: HTTPException: Si une erreur survient lors du calcul des statistiques. Response 200 OK application/json { \"documentStats\" : { \"totalCount\" : 0 , \"byTheme\" : {}, \"byType\" : {}, \"recentlyAdded\" : 0 , \"percentChange\" : 10.12 }, \"searchStats\" : { \"totalCount\" : 0 , \"lastMonthCount\" : 0 , \"percentChange\" : 10.12 , \"topQueries\" : [ {} ] }, \"systemStats\" : { \"satisfaction\" : 10.12 , \"avgConfidence\" : 10.12 , \"percentChange\" : 10.12 , \"indexedCorpora\" : 0 , \"totalCorpora\" : 0 } } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"documentStats\" : { \"$ref\" : \"#/components/schemas/DocumentStats\" }, \"searchStats\" : { \"$ref\" : \"#/components/schemas/SearchStats\" }, \"systemStats\" : { \"$ref\" : \"#/components/schemas/SystemStats\" } }, \"type\" : \"object\" , \"required\" : [ \"documentStats\" , \"searchStats\" , \"systemStats\" ], \"title\" : \"DashboardStats\" , \"description\" : \"Corps minimal pour cr\u00e9er un document (hors contenu).\" } POST /stats/refresh Rafra\u00eechir le cache des statistiques Description Force le rafra\u00eechissement du cache des statistiques. Cette route permet d'invalider les caches potentiels et de forcer un recalcul complet de toutes les m\u00e9triques du syst\u00e8me. Utile apr\u00e8s des op\u00e9rations importantes comme des imports massifs ou des maintenances. Args: db: Session de base de donn\u00e9es fournie par d\u00e9pendance. Returns: dict: R\u00e9sultat de l'op\u00e9ration avec statut et message. Raises: HTTPException: Si une erreur survient lors du rafra\u00eechissement. Response 200 OK application/json Schema of the response body { \"additionalProperties\" : true , \"type\" : \"object\" , \"title\" : \"Response Refresh Stats Cache Stats Refresh Post\" } Endpoints GET / Root Description Endpoint racine pour v\u00e9rifier l'\u00e9tat de l'API. Returns: dict: Message indiquant que l'API est en cours d'ex\u00e9cution. Response 200 OK application/json Schema of the response body Schemas AskRequest Name Type enableThinking boolean filters modelName promptType string query string stream boolean theme Body_process_and_store_async_endpoint_pipeline_process_and_store_async_post Name Type file string ( binary ) Body_process_and_store_endpoint_pipeline_process_and_store_post Name Type file string ( binary ) Body_upload_and_process_file_doc_loader_upload_file_post Name Type file string ( binary ) ChunkCreate Name Type content string endChar integer hierarchyLevel integer id parentChunkId startChar integer ChunkResult Name Type chunkId integer content string context documentId integer documentType string hierarchyLevel integer publishDate string ( date ) score number theme string title string ConfidenceMetrics Name Type level number message string stats DashboardStats Name Type documentStats DocumentStats searchStats SearchStats systemStats SystemStats DocumentCreate Name Type corpusId documentType string publishDate string ( date ) theme string title string DocumentResponse Name Type chunkCount integer corpusId documentType string id integer indexNeeded boolean publishDate string ( date ) theme string title string DocumentStats Name Type byTheme byType percentChange number recentlyAdded integer totalCount integer DocumentUpdate Name Type corpusId documentType id integer publishDate theme title DocumentWithChunks Name Type chunks Array< ChunkCreate > document DocumentCreate HierarchicalContext Name Type level0 level1 level2 HTTPValidationError Name Type detail Array< ValidationError > IndexStatus Name Type chunkCount integer configExists boolean corpusId indexedChunks integer indexExists boolean indexType isIndexed boolean lastIndexed SearchRequest Name Type corpusId documentType endDate filterByRelevance boolean hierarchical boolean hierarchyLevel normalizeScores boolean query string startDate theme topK integer SearchResponse Name Type confidence message normalized boolean query string results Array< ChunkResult > topK integer totalResults integer SearchStats Name Type lastMonthCount integer percentChange number topQueries Array<> totalCount integer SystemStats Name Type avgConfidence number indexedCorpora integer percentChange number satisfaction number totalCorpora integer UpdateWithChunks Name Type document DocumentUpdate newChunks ValidationError Name Type loc Array<> msg string type string","title":"Documentation Swagger"},{"location":"api/rest/rest_api/#rest-api-documentation","text":"","title":"REST API Documentation"},{"location":"api/rest/rest_api/#clea-api-version","text":"API pour g\u00e9rer les documents et effectuer des recherches s\u00e9mantiques.","title":"Cl\u00e9a API VERSION"},{"location":"api/rest/rest_api/#database","text":"","title":"Database"},{"location":"api/rest/rest_api/#post-databasedocuments","text":"Ajouter un document avec ses chunks Description Ins\u00e8re le document puis ses chunks dans une transaction unique. Args: payload: Objet contenant les donn\u00e9es du document et ses chunks. db: Session de base de donn\u00e9es inject\u00e9e par d\u00e9pendance. Returns: DocumentResponse: Document cr\u00e9\u00e9 avec le nombre de chunks associ\u00e9s. Raises: HTTPException: Si une erreur survient pendant l'insertion. Request body application/json { \"document\" : { \"title\" : \"string\" , \"theme\" : \"string\" , \"documentType\" : \"string\" , \"publishDate\" : \"2022-04-13\" , \"corpusId\" : null }, \"chunks\" : [ { \"id\" : null , \"content\" : \"string\" , \"startChar\" : 0 , \"endChar\" : 0 , \"hierarchyLevel\" : 0 , \"parentChunkId\" : null } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the request body { \"properties\" : { \"document\" : { \"$ref\" : \"#/components/schemas/DocumentCreate\" }, \"chunks\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ChunkCreate\" }, \"type\" : \"array\" , \"title\" : \"Chunks\" } }, \"type\" : \"object\" , \"required\" : [ \"document\" , \"chunks\" ], \"title\" : \"DocumentWithChunks\" , \"description\" : \"Payload complet pour `POST /database/documents`.\" } Response 200 OK application/json { \"id\" : 0 , \"title\" : \"string\" , \"theme\" : \"string\" , \"documentType\" : \"string\" , \"publishDate\" : \"2022-04-13\" , \"corpusId\" : null , \"chunkCount\" : 0 , \"indexNeeded\" : true } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"id\" : { \"type\" : \"integer\" , \"title\" : \"Id\" }, \"title\" : { \"type\" : \"string\" , \"title\" : \"Title\" }, \"theme\" : { \"type\" : \"string\" , \"title\" : \"Theme\" }, \"documentType\" : { \"type\" : \"string\" , \"title\" : \"Documenttype\" }, \"publishDate\" : { \"type\" : \"string\" , \"format\" : \"date\" , \"title\" : \"Publishdate\" }, \"corpusId\" : { \"anyOf\" : [ { \"type\" : \"string\" }, { \"type\" : \"null\" } ], \"title\" : \"Corpusid\" }, \"chunkCount\" : { \"type\" : \"integer\" , \"minimum\" : 0 , \"title\" : \"Chunkcount\" }, \"indexNeeded\" : { \"type\" : \"boolean\" , \"title\" : \"Indexneeded\" , \"default\" : false } }, \"type\" : \"object\" , \"required\" : [ \"id\" , \"title\" , \"theme\" , \"documentType\" , \"publishDate\" , \"chunkCount\" ], \"title\" : \"DocumentResponse\" , \"description\" : \"R\u00e9ponse standard lorsqu\u2019un document est renvoy\u00e9 c\u00f4t\u00e9 API.\\n\\nAttributs:\\n id (int): Identifiant du document.\\n title (str): Titre du document.\\n theme (str): Th\u00e8me du document.\\n document_type (str): Type de document.\\n publish_date (date): Date de publication.\\n corpus_id (Optional[str]): Identifiant du corpus.\\n chunk_count (int): Nombre de chunks associ\u00e9s (>= 0).\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" }","title":"POST /database/documents"},{"location":"api/rest/rest_api/#get-databasedocuments","text":"Lister les documents Description Liste l'ensemble des documents de la base de donn\u00e9es avec leur nombre de chunks. Cette fonction permet de r\u00e9cup\u00e9rer un ensemble pagin\u00e9 de documents avec possibilit\u00e9 de filtrage sur diff\u00e9rents crit\u00e8res. Args: theme: Filtre optionnel pour le th\u00e8me du document. document_type: Filtre optionnel pour le type du document. corpus_id: Filtre optionnel sur l'identifiant du corpus. skip: Nombre de documents \u00e0 ignorer (pour la pagination). limit: Nombre maximal de documents \u00e0 retourner. db: Session de base de donn\u00e9es fournie par d\u00e9pendance. Returns: Liste des documents format\u00e9s avec leur nombre de chunks associ\u00e9s. Input parameters Parameter In Type Default Nullable Description corpusId query None No documentType query None No limit query integer 100 No skip query integer 0 No theme query None No Response 200 OK application/json [ { \"id\" : 0 , \"title\" : \"string\" , \"theme\" : \"string\" , \"documentType\" : \"string\" , \"publishDate\" : \"2022-04-13\" , \"corpusId\" : null , \"chunkCount\" : 0 , \"indexNeeded\" : true } ] \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"type\" : \"array\" , \"items\" : { \"$ref\" : \"#/components/schemas/DocumentResponse\" }, \"title\" : \"Response List Documents Database Documents Get\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" }","title":"GET /database/documents"},{"location":"api/rest/rest_api/#put-databasedocumentsdocument_id","text":"Mettre \u00e0 jour un document (et/ou ajouter des chunks) Description Met \u00e0 jour les m\u00e9tadonn\u00e9es d'un document et peut ajouter de nouveaux chunks. Cette fonction permet de modifier les informations d'un document existant et d'y ajouter de nouveaux fragments de texte (chunks) en une seule op\u00e9ration. Args: payload: Objet de mise \u00e0 jour contenant le document et \u00e9ventuellement des nouveaux chunks. document_id: Identifiant num\u00e9rique du document \u00e0 mettre \u00e0 jour (\u2265 1). db: Session de base de donn\u00e9es inject\u00e9e par d\u00e9pendance. Returns: DocumentResponse: Document mis \u00e0 jour avec le nombre total de chunks associ\u00e9s. Raises: HTTPException: - 404: Si le document n'existe pas dans la base Input parameters Parameter In Type Default Nullable Description document_id path integer No Identifiant du document \u00e0 mettre \u00e0 jour (>= 1). Request body application/json { \"document\" : { \"id\" : 0 , \"title\" : null , \"theme\" : null , \"documentType\" : null , \"publishDate\" : null , \"corpusId\" : null }, \"newChunks\" : null } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the request body { \"properties\" : { \"document\" : { \"$ref\" : \"#/components/schemas/DocumentUpdate\" }, \"newChunks\" : { \"anyOf\" : [ { \"items\" : { \"$ref\" : \"#/components/schemas/ChunkCreate\" }, \"type\" : \"array\" }, { \"type\" : \"null\" } ], \"title\" : \"Newchunks\" } }, \"type\" : \"object\" , \"required\" : [ \"document\" ], \"title\" : \"UpdateWithChunks\" , \"description\" : \"Payload de mise-\u00e0-jour :\\n\\n* `document` \u2192 DTO `DocumentUpdate`\\n* `newChunks` \u2192 \u00e9ventuelle liste de nouveaux chunks \u00e0 ajouter\" } Response 200 OK application/json { \"id\" : 0 , \"title\" : \"string\" , \"theme\" : \"string\" , \"documentType\" : \"string\" , \"publishDate\" : \"2022-04-13\" , \"corpusId\" : null , \"chunkCount\" : 0 , \"indexNeeded\" : true } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"id\" : { \"type\" : \"integer\" , \"title\" : \"Id\" }, \"title\" : { \"type\" : \"string\" , \"title\" : \"Title\" }, \"theme\" : { \"type\" : \"string\" , \"title\" : \"Theme\" }, \"documentType\" : { \"type\" : \"string\" , \"title\" : \"Documenttype\" }, \"publishDate\" : { \"type\" : \"string\" , \"format\" : \"date\" , \"title\" : \"Publishdate\" }, \"corpusId\" : { \"anyOf\" : [ { \"type\" : \"string\" }, { \"type\" : \"null\" } ], \"title\" : \"Corpusid\" }, \"chunkCount\" : { \"type\" : \"integer\" , \"minimum\" : 0 , \"title\" : \"Chunkcount\" }, \"indexNeeded\" : { \"type\" : \"boolean\" , \"title\" : \"Indexneeded\" , \"default\" : false } }, \"type\" : \"object\" , \"required\" : [ \"id\" , \"title\" , \"theme\" , \"documentType\" , \"publishDate\" , \"chunkCount\" ], \"title\" : \"DocumentResponse\" , \"description\" : \"R\u00e9ponse standard lorsqu\u2019un document est renvoy\u00e9 c\u00f4t\u00e9 API.\\n\\nAttributs:\\n id (int): Identifiant du document.\\n title (str): Titre du document.\\n theme (str): Th\u00e8me du document.\\n document_type (str): Type de document.\\n publish_date (date): Date de publication.\\n corpus_id (Optional[str]): Identifiant du corpus.\\n chunk_count (int): Nombre de chunks associ\u00e9s (>= 0).\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" }","title":"PUT /database/documents/{document_id}"},{"location":"api/rest/rest_api/#delete-databasedocumentsdocument_id","text":"Supprimer un document et tous ses chunks Input parameters Parameter In Type Default Nullable Description document_id path integer No Response 200 OK application/json Schema of the response body { \"type\" : \"object\" , \"additionalProperties\" : true , \"title\" : \"Response Remove Document Database Documents Document Id Delete\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" }","title":"DELETE /database/documents/{document_id}"},{"location":"api/rest/rest_api/#get-databasedocumentsdocument_id","text":"R\u00e9cup\u00e9rer un document Description R\u00e9cup\u00e8re un document \u00e0 partir de son identifiant et le formate en DocumentResponse. Args: document_id (int): Identifiant du document \u00e0 r\u00e9cup\u00e9rer. db (Session): Session de base de donn\u00e9es fournie par d\u00e9pendance. Returns: DocumentResponse: Document format\u00e9 avec le nombre de chunks associ\u00e9s. Raises: HTTPException: Si le document n'existe pas dans la base. Input parameters Parameter In Type Default Nullable Description document_id path integer No Response 200 OK application/json { \"id\" : 0 , \"title\" : \"string\" , \"theme\" : \"string\" , \"documentType\" : \"string\" , \"publishDate\" : \"2022-04-13\" , \"corpusId\" : null , \"chunkCount\" : 0 , \"indexNeeded\" : true } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"id\" : { \"type\" : \"integer\" , \"title\" : \"Id\" }, \"title\" : { \"type\" : \"string\" , \"title\" : \"Title\" }, \"theme\" : { \"type\" : \"string\" , \"title\" : \"Theme\" }, \"documentType\" : { \"type\" : \"string\" , \"title\" : \"Documenttype\" }, \"publishDate\" : { \"type\" : \"string\" , \"format\" : \"date\" , \"title\" : \"Publishdate\" }, \"corpusId\" : { \"anyOf\" : [ { \"type\" : \"string\" }, { \"type\" : \"null\" } ], \"title\" : \"Corpusid\" }, \"chunkCount\" : { \"type\" : \"integer\" , \"minimum\" : 0 , \"title\" : \"Chunkcount\" }, \"indexNeeded\" : { \"type\" : \"boolean\" , \"title\" : \"Indexneeded\" , \"default\" : false } }, \"type\" : \"object\" , \"required\" : [ \"id\" , \"title\" , \"theme\" , \"documentType\" , \"publishDate\" , \"chunkCount\" ], \"title\" : \"DocumentResponse\" , \"description\" : \"R\u00e9ponse standard lorsqu\u2019un document est renvoy\u00e9 c\u00f4t\u00e9 API.\\n\\nAttributs:\\n id (int): Identifiant du document.\\n title (str): Titre du document.\\n theme (str): Th\u00e8me du document.\\n document_type (str): Type de document.\\n publish_date (date): Date de publication.\\n corpus_id (Optional[str]): Identifiant du corpus.\\n chunk_count (int): Nombre de chunks associ\u00e9s (>= 0).\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" }","title":"GET /database/documents/{document_id}"},{"location":"api/rest/rest_api/#delete-databasedocumentsdocument_idchunks","text":"Supprimer des chunks d'un document Input parameters Parameter In Type Default Nullable Description chunk_ids query None No IDs \u00e0 supprimer ; vide \u21d2 tous les chunks du document document_id path integer No Response 200 OK application/json Schema of the response body { \"type\" : \"object\" , \"additionalProperties\" : true , \"title\" : \"Response Remove Chunks Database Documents Document Id Chunks Delete\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" }","title":"DELETE /database/documents/{document_id}/chunks"},{"location":"api/rest/rest_api/#get-databasedocumentsdocument_idchunks","text":"R\u00e9cup\u00e9rer les chunks d'un document Input parameters Parameter In Type Default Nullable Description document_id path integer No hierarchyLevel query None No limit query integer 100 No parentChunkId query None No skip query integer 0 No Response 200 OK application/json [ null ] \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"type\" : \"array\" , \"items\" : {}, \"title\" : \"Response Get Chunks Database Documents Document Id Chunks Get\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" }","title":"GET /database/documents/{document_id}/chunks"},{"location":"api/rest/rest_api/#search","text":"","title":"Search"},{"location":"api/rest/rest_api/#post-searchhybrid_search","text":"Recherche hybride (vecteur + filtres) Description Retourne les top k chunks les plus pertinents avec \u00e9valuation de confiance. Le moteur combine : Filtres SQL (theme, document_type , dates, corpus_id ) Index vectoriel pgvector (IVFFLAT ou HNSW) Rerank Cross-Encoder sur un sous-ensemble \u00e9largi ( top k \u00d7 3 ) \u00c9valuation de confiance analyse la pertinence globale des r\u00e9sultats Filtrage de pertinence \u00e9limine les r\u00e9sultats sous le seuil minimal Normalisation des scores facilite l'interpr\u00e9tation (0-1) Les r\u00e9sultats incluent des m\u00e9triques de confiance qui permettent d'identifier les requ\u00eates hors du domaine de connaissances. Request body application/json { \"query\" : \"string\" , \"topK\" : 0 , \"theme\" : null , \"documentType\" : null , \"startDate\" : null , \"endDate\" : null , \"corpusId\" : null , \"hierarchyLevel\" : null , \"hierarchical\" : true , \"filterByRelevance\" : true , \"normalizeScores\" : true } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the request body { \"properties\" : { \"query\" : { \"type\" : \"string\" , \"title\" : \"Query\" , \"description\" : \"Requ\u00eate en langage naturel\" }, \"topK\" : { \"type\" : \"integer\" , \"title\" : \"Topk\" , \"description\" : \"Nombre de r\u00e9sultats \u00e0 retourner\" , \"default\" : 10 }, \"theme\" : { \"anyOf\" : [ { \"type\" : \"string\" }, { \"type\" : \"null\" } ], \"title\" : \"Theme\" , \"description\" : \"Filtre par th\u00e8me\" }, \"documentType\" : { \"anyOf\" : [ { \"type\" : \"string\" }, { \"type\" : \"null\" } ], \"title\" : \"Documenttype\" , \"description\" : \"Filtre par type de document\" }, \"startDate\" : { \"anyOf\" : [ { \"type\" : \"string\" , \"format\" : \"date-time\" }, { \"type\" : \"null\" } ], \"title\" : \"Startdate\" , \"description\" : \"Date de d\u00e9but\" }, \"endDate\" : { \"anyOf\" : [ { \"type\" : \"string\" , \"format\" : \"date-time\" }, { \"type\" : \"null\" } ], \"title\" : \"Enddate\" , \"description\" : \"Date de fin\" }, \"corpusId\" : { \"anyOf\" : [ { \"type\" : \"integer\" }, { \"type\" : \"null\" } ], \"title\" : \"Corpusid\" , \"description\" : \"ID du corpus\" }, \"hierarchyLevel\" : { \"anyOf\" : [ { \"type\" : \"integer\" }, { \"type\" : \"null\" } ], \"title\" : \"Hierarchylevel\" , \"description\" : \"Niveau hi\u00e9rarchique (0-2)\" }, \"hierarchical\" : { \"type\" : \"boolean\" , \"title\" : \"Hierarchical\" , \"description\" : \"R\u00e9cup\u00e9rer le contexte hi\u00e9rarchique\" , \"default\" : false }, \"filterByRelevance\" : { \"type\" : \"boolean\" , \"title\" : \"Filterbyrelevance\" , \"description\" : \"Filtrer les r\u00e9sultats sous le seuil de pertinence\" , \"default\" : false }, \"normalizeScores\" : { \"type\" : \"boolean\" , \"title\" : \"Normalizescores\" , \"description\" : \"Normaliser les scores entre 0 et 1\" , \"default\" : false } }, \"type\" : \"object\" , \"required\" : [ \"query\" ], \"title\" : \"SearchRequest\" , \"description\" : \"Param\u00e8tres pour la recherche hybride.\\n\\nCombine la requ\u00eate textuelle avec des filtres de m\u00e9tadonn\u00e9es optionnels.\" } Response 200 OK application/json { \"query\" : \"string\" , \"topK\" : 0 , \"totalResults\" : 0 , \"results\" : [ { \"chunkId\" : 0 , \"documentId\" : 0 , \"title\" : \"string\" , \"content\" : \"string\" , \"theme\" : \"string\" , \"documentType\" : \"string\" , \"publishDate\" : \"2022-04-13\" , \"score\" : 10.12 , \"hierarchyLevel\" : 0 , \"context\" : null } ], \"confidence\" : null , \"normalized\" : true , \"message\" : null } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"query\" : { \"type\" : \"string\" , \"title\" : \"Query\" , \"description\" : \"Requ\u00eate originale\" }, \"topK\" : { \"type\" : \"integer\" , \"title\" : \"Topk\" , \"description\" : \"Nombre de r\u00e9sultats demand\u00e9s\" }, \"totalResults\" : { \"type\" : \"integer\" , \"title\" : \"Totalresults\" , \"description\" : \"Nombre total de r\u00e9sultats trouv\u00e9s\" }, \"results\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ChunkResult\" }, \"type\" : \"array\" , \"title\" : \"Results\" , \"description\" : \"R\u00e9sultats de la recherche\" }, \"confidence\" : { \"anyOf\" : [ { \"$ref\" : \"#/components/schemas/ConfidenceMetrics\" }, { \"type\" : \"null\" } ], \"description\" : \"M\u00e9triques de confiance sur les r\u00e9sultats\" }, \"normalized\" : { \"type\" : \"boolean\" , \"title\" : \"Normalized\" , \"description\" : \"Indique si les scores sont normalis\u00e9s (0-1)\" , \"default\" : false }, \"message\" : { \"anyOf\" : [ { \"type\" : \"string\" }, { \"type\" : \"null\" } ], \"title\" : \"Message\" , \"description\" : \"Message informatif sur les r\u00e9sultats\" } }, \"type\" : \"object\" , \"required\" : [ \"query\" , \"topK\" , \"totalResults\" , \"results\" ], \"title\" : \"SearchResponse\" , \"description\" : \"R\u00e9ponse \u00e0 une requ\u00eate de recherche.\\n\\nContient les r\u00e9sultats tri\u00e9s par pertinence avec m\u00e9tadonn\u00e9es et \u00e9valuation de confiance.\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" }","title":"POST /search/hybrid_search"},{"location":"api/rest/rest_api/#index","text":"","title":"Index"},{"location":"api/rest/rest_api/#post-indexcreate-indexcorpus_id","text":"Cr\u00e9er un index vectoriel pour un corpus Description Cr\u00e9e un index vectoriel pour un corpus sp\u00e9cifique. Cette fonction cr\u00e9e un index IVFFLAT simple pour acc\u00e9l\u00e9rer les recherches vectorielles sur le corpus sp\u00e9cifi\u00e9. Args: corpus_id: Identifiant UUID du corpus \u00e0 indexer. Returns: dict: R\u00e9sultat de l'op\u00e9ration avec statut et message. Raises: HTTPException: Si une erreur survient lors de la cr\u00e9ation de l'index. Input parameters Parameter In Type Default Nullable Description corpus_id path string No Identifiant UUID du corpus Response 200 OK application/json Schema of the response body { \"type\" : \"object\" , \"additionalProperties\" : true , \"title\" : \"Response Create Corpus Index Index Create Index Corpus Id Post\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" }","title":"POST /index/create-index/{corpus_id}"},{"location":"api/rest/rest_api/#delete-indexdrop-indexcorpus_id","text":"Supprimer l'index vectoriel d'un corpus Description Supprime l'index vectoriel pour un corpus sp\u00e9cifique. Args: corpus_id: Identifiant UUID du corpus. Returns: dict: R\u00e9sultat de l'op\u00e9ration. Raises: HTTPException: Si une erreur survient lors de la suppression de l'index. Input parameters Parameter In Type Default Nullable Description corpus_id path string No Identifiant UUID du corpus Response 200 OK application/json Schema of the response body { \"type\" : \"object\" , \"additionalProperties\" : true , \"title\" : \"Response Remove Corpus Index Index Drop Index Corpus Id Delete\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" }","title":"DELETE /index/drop-index/{corpus_id}"},{"location":"api/rest/rest_api/#get-indexindex-statuscorpus_id","text":"V\u00e9rifier l'\u00e9tat de l'index pour un corpus Description V\u00e9rifie l'\u00e9tat de l'index pour un corpus sp\u00e9cifique. Args: corpus_id: Identifiant UUID du corpus. Returns: IndexStatus: \u00c9tat de l'index et m\u00e9tadonn\u00e9es. Raises: HTTPException: Si une erreur survient lors de la v\u00e9rification de l'\u00e9tat. Input parameters Parameter In Type Default Nullable Description corpus_id path string No Identifiant UUID du corpus Response 200 OK application/json { \"corpusId\" : null , \"indexExists\" : true , \"configExists\" : true , \"isIndexed\" : true , \"indexType\" : null , \"chunkCount\" : 0 , \"indexedChunks\" : 0 , \"lastIndexed\" : null } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"corpusId\" : { \"anyOf\" : [ { \"type\" : \"string\" }, { \"type\" : \"null\" } ], \"title\" : \"Corpusid\" }, \"indexExists\" : { \"type\" : \"boolean\" , \"title\" : \"Indexexists\" }, \"configExists\" : { \"type\" : \"boolean\" , \"title\" : \"Configexists\" }, \"isIndexed\" : { \"type\" : \"boolean\" , \"title\" : \"Isindexed\" }, \"indexType\" : { \"anyOf\" : [ { \"type\" : \"string\" }, { \"type\" : \"null\" } ], \"title\" : \"Indextype\" }, \"chunkCount\" : { \"type\" : \"integer\" , \"title\" : \"Chunkcount\" }, \"indexedChunks\" : { \"type\" : \"integer\" , \"title\" : \"Indexedchunks\" }, \"lastIndexed\" : { \"anyOf\" : [ { \"type\" : \"string\" , \"format\" : \"date\" }, { \"type\" : \"null\" } ], \"title\" : \"Lastindexed\" } }, \"type\" : \"object\" , \"required\" : [ \"corpusId\" , \"indexExists\" , \"configExists\" , \"isIndexed\" , \"indexType\" , \"chunkCount\" , \"indexedChunks\" , \"lastIndexed\" ], \"title\" : \"IndexStatus\" , \"description\" : \"Statut de l'indexation d'un document.\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" }","title":"GET /index/index-status/{corpus_id}"},{"location":"api/rest/rest_api/#get-indexindexes","text":"V\u00e9rifier l'\u00e9tat de tous les index vectoriels Description V\u00e9rifie l'\u00e9tat de tous les index vectoriels. Returns: dict: \u00c9tat des index pour tous les corpus. Response 200 OK application/json Schema of the response body { \"additionalProperties\" : true , \"type\" : \"object\" , \"title\" : \"Response Get All Indexes Index Indexes Get\" }","title":"GET /index/indexes"},{"location":"api/rest/rest_api/#post-indexcleanup-indexes","text":"Nettoyer les index vectoriels orphelins Description Nettoie les index vectoriels orphelins. Cette route identifie et supprime les configurations d'index et vues mat\u00e9rialis\u00e9es qui ne sont plus associ\u00e9es \u00e0 des corpus existants dans la base de donn\u00e9es. Returns: dict: R\u00e9sultat de l'op\u00e9ration avec statistiques. Response 200 OK application/json Schema of the response body { \"additionalProperties\" : true , \"type\" : \"object\" , \"title\" : \"Response Cleanup Orphaned Indexes Index Cleanup Indexes Post\" }","title":"POST /index/cleanup-indexes"},{"location":"api/rest/rest_api/#docloader","text":"","title":"DocLoader"},{"location":"api/rest/rest_api/#post-doc_loaderupload-file","text":"Uploader un fichier et le traiter Description Uploade un fichier, l'extrait et le divise en chunks. Args: file (UploadFile): Fichier upload\u00e9 par l'utilisateur. max_length (int): Taille maximale d'un chunk. Par d\u00e9faut 1000. theme (str): Th\u00e8me du document. Par d\u00e9faut \"Th\u00e8me g\u00e9n\u00e9rique\". Returns: List[DocumentWithChunks]: Liste des documents extraits. Raises: HTTPException: Si une erreur survient lors du traitement ou si aucun contenu n'est extrait. Input parameters Parameter In Type Default Nullable Description max_length query integer 1000 No Taille maximale d'un chunk theme query string Th\u00e8me g\u00e9n\u00e9rique No Th\u00e8me du document Request body multipart/form-data { \"file\" : \"TG9yZW0gaXBzdW0gZG9sb3Igc2l0IGFtZXQ=\" } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the request body { \"properties\" : { \"file\" : { \"type\" : \"string\" , \"format\" : \"binary\" , \"title\" : \"File\" , \"description\" : \"Fichier \u00e0 traiter\" } }, \"type\" : \"object\" , \"required\" : [ \"file\" ], \"title\" : \"Body_upload_and_process_file_doc_loader_upload_file_post\" } Response 200 OK application/json { \"document\" : { \"title\" : \"string\" , \"theme\" : \"string\" , \"documentType\" : \"string\" , \"publishDate\" : \"2022-04-13\" , \"corpusId\" : null }, \"chunks\" : [ { \"id\" : null , \"content\" : \"string\" , \"startChar\" : 0 , \"endChar\" : 0 , \"hierarchyLevel\" : 0 , \"parentChunkId\" : null } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"document\" : { \"$ref\" : \"#/components/schemas/DocumentCreate\" }, \"chunks\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ChunkCreate\" }, \"type\" : \"array\" , \"title\" : \"Chunks\" } }, \"type\" : \"object\" , \"required\" : [ \"document\" , \"chunks\" ], \"title\" : \"DocumentWithChunks\" , \"description\" : \"Payload complet pour `POST /database/documents`.\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" }","title":"POST /doc_loader/upload-file"},{"location":"api/rest/rest_api/#pipeline","text":"","title":"Pipeline"},{"location":"api/rest/rest_api/#post-pipelineprocess-and-store","text":"Charger un fichier, l'extraire et l'ins\u00e9rer dans la base de donn\u00e9es avec segmentation adaptative Description Charge un fichier, l'extrait avec segmentation hi\u00e9rarchique et l'ins\u00e8re dans la base de donn\u00e9es. Le fichier est temporairement sauvegard\u00e9 sur le disque, trait\u00e9 pour en extraire le contenu textuel, segment\u00e9 selon une approche hi\u00e9rarchique, puis ins\u00e9r\u00e9 dans la base de donn\u00e9es avec g\u00e9n\u00e9ration automatique d'embeddings. Args: file: Fichier upload\u00e9 par l'utilisateur. max_length: Taille maximale d'un chunk final. overlap: Chevauchement entre les chunks. theme: Th\u00e8me \u00e0 appliquer au document. corpus_id: Identifiant du corpus (g\u00e9n\u00e9r\u00e9 si non sp\u00e9cifi\u00e9). Returns: Dict: R\u00e9sultats de l'op\u00e9ration avec l'ID du document et les statistiques de segmentation. Raises: HTTPException: Si une erreur survient pendant le traitement du document. Input parameters Parameter In Type Default Nullable Description corpus_id query None No Identifiant du corpus (g\u00e9n\u00e9r\u00e9 si non sp\u00e9cifi\u00e9) max_length query integer 500 No Taille maximale d'un chunk overlap query integer 100 No Chevauchement entre les chunks theme query string Th\u00e8me g\u00e9n\u00e9rique No Th\u00e8me du document Request body multipart/form-data { \"file\" : \"TG9yZW0gaXBzdW0gZG9sb3Igc2l0IGFtZXQ=\" } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the request body { \"properties\" : { \"file\" : { \"type\" : \"string\" , \"format\" : \"binary\" , \"title\" : \"File\" , \"description\" : \"Fichier \u00e0 traiter\" } }, \"type\" : \"object\" , \"required\" : [ \"file\" ], \"title\" : \"Body_process_and_store_endpoint_pipeline_process_and_store_post\" } Response 200 OK application/json Schema of the response body { \"type\" : \"object\" , \"additionalProperties\" : true , \"title\" : \"Response Process And Store Endpoint Pipeline Process And Store Post\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" }","title":"POST /pipeline/process-and-store"},{"location":"api/rest/rest_api/#post-pipelineprocess-and-store-async","text":"Traiter un fichier en arri\u00e8re-plan et l'ins\u00e9rer dans la base de donn\u00e9es Description Traite un fichier en arri\u00e8re-plan et l'ins\u00e8re dans la base de donn\u00e9es. Similaire \u00e0 process-and-store mais s'ex\u00e9cute de mani\u00e8re asynchrone pour les fichiers volumineux. Le client re\u00e7oit imm\u00e9diatement une r\u00e9ponse avec un identifiant de t\u00e2che pendant que le traitement se poursuit en arri\u00e8re-plan. Args: background_tasks: Gestionnaire de t\u00e2ches en arri\u00e8re-plan de FastAPI. file: Fichier upload\u00e9 par l'utilisateur. max_length: Taille maximale d'un chunk final. overlap: Chevauchement entre les chunks. theme: Th\u00e8me \u00e0 appliquer au document. corpus_id: Identifiant du corpus (g\u00e9n\u00e9r\u00e9 si non sp\u00e9cifi\u00e9). Returns: Dict: Informations sur la t\u00e2che en arri\u00e8re-plan cr\u00e9\u00e9e. Input parameters Parameter In Type Default Nullable Description corpus_id query None No Identifiant du corpus (g\u00e9n\u00e9r\u00e9 si non sp\u00e9cifi\u00e9) max_length query integer 500 No Taille maximale d'un chunk overlap query integer 100 No Chevauchement entre les chunks theme query string Th\u00e8me g\u00e9n\u00e9rique No Th\u00e8me du document Request body multipart/form-data { \"file\" : \"TG9yZW0gaXBzdW0gZG9sb3Igc2l0IGFtZXQ=\" } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the request body { \"properties\" : { \"file\" : { \"type\" : \"string\" , \"format\" : \"binary\" , \"title\" : \"File\" , \"description\" : \"Fichier \u00e0 traiter\" } }, \"type\" : \"object\" , \"required\" : [ \"file\" ], \"title\" : \"Body_process_and_store_async_endpoint_pipeline_process_and_store_async_post\" } Response 200 OK application/json Schema of the response body { \"type\" : \"object\" , \"additionalProperties\" : true , \"title\" : \"Response Process And Store Async Endpoint Pipeline Process And Store Async Post\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" }","title":"POST /pipeline/process-and-store-async"},{"location":"api/rest/rest_api/#askai","text":"","title":"AskAI"},{"location":"api/rest/rest_api/#post-askaiask","text":"Ask Ai Description Endpoint pour poser une question et obtenir une r\u00e9ponse bas\u00e9e sur les documents. Cette fonction interroge la base documentaire avec la question fournie, r\u00e9cup\u00e8re les documents pertinents et utilise un mod\u00e8le LLM pour g\u00e9n\u00e9rer une r\u00e9ponse contextuelle. Args: request: Param\u00e8tres de la requ\u00eate (query, filters, etc.). db: Session de base de donn\u00e9es SQLAlchemy. search_engine: Instance du moteur de recherche vectorielle. Returns: Dict[str, Any] ou StreamingResponse: R\u00e9ponse g\u00e9n\u00e9r\u00e9e avec contexte. Raises: HTTPException: Si une erreur survient lors du traitement. Request body application/json { \"query\" : \"string\" , \"filters\" : null , \"theme\" : null , \"modelName\" : null , \"stream\" : true , \"promptType\" : \"string\" , \"enableThinking\" : true } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the request body { \"properties\" : { \"query\" : { \"type\" : \"string\" , \"title\" : \"Query\" , \"description\" : \"Question \u00e0 poser au syst\u00e8me\" }, \"filters\" : { \"anyOf\" : [ { \"additionalProperties\" : true , \"type\" : \"object\" }, { \"type\" : \"null\" } ], \"title\" : \"Filters\" , \"description\" : \"Filtres pour la recherche\" }, \"theme\" : { \"anyOf\" : [ { \"type\" : \"string\" }, { \"type\" : \"null\" } ], \"title\" : \"Theme\" , \"description\" : \"Th\u00e8me pour filtrer les documents\" }, \"modelName\" : { \"anyOf\" : [ { \"type\" : \"string\" }, { \"type\" : \"null\" } ], \"title\" : \"Modelname\" , \"description\" : \"Mod\u00e8le \u00e0 utiliser\" , \"default\" : \"Qwen3-0.6B\" }, \"stream\" : { \"type\" : \"boolean\" , \"title\" : \"Stream\" , \"description\" : \"R\u00e9ponse en streaming\" , \"default\" : false }, \"promptType\" : { \"type\" : \"string\" , \"title\" : \"Prompttype\" , \"description\" : \"Type de prompt (standard, summary, comparison)\" , \"default\" : \"standard\" }, \"enableThinking\" : { \"type\" : \"boolean\" , \"title\" : \"Enablethinking\" , \"description\" : \"Activer le mode 'think' pour le mod\u00e8le\" , \"default\" : false } }, \"type\" : \"object\" , \"required\" : [ \"query\" ], \"title\" : \"AskRequest\" , \"description\" : \"Requ\u00eate pour poser une question au syst\u00e8me RAG.\\n\\nArgs:\\n query: Question \u00e0 poser au syst\u00e8me.\\n filters: Filtres pour la recherche documentaire.\\n theme: Th\u00e8me pour filtrer les documents.\\n model_name: Nom du mod\u00e8le \u00e0 utiliser.\\n stream: Indique si la r\u00e9ponse doit \u00eatre stream\u00e9e.\\n prompt_type: Type de prompt \u00e0 utiliser.\\n\\nAttributes:\\n query (str): Question ou requ\u00eate de l'utilisateur.\\n filters (Optional[Dict[str, Any]]): Filtres pour la recherche documentaire.\\n theme (Optional[str]): Th\u00e8me pour filtrer les documents.\\n model_name (Optional[str]): Nom du mod\u00e8le \u00e0 utiliser.\\n stream (bool): Indique si la r\u00e9ponse doit \u00eatre stream\u00e9e.\\n prompt_type (str): Type de prompt \u00e0 utiliser ('standard', 'summary', 'comparison').\" } Response 200 OK application/json Schema of the response body Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" }","title":"POST /askai/ask"},{"location":"api/rest/rest_api/#get-askaimodels","text":"Get Models Description Endpoint pour r\u00e9cup\u00e9rer la liste des mod\u00e8les disponibles. Returns: List[str]: Liste des noms de mod\u00e8les disponibles. Response 200 OK application/json Schema of the response body","title":"GET /askai/models"},{"location":"api/rest/rest_api/#stats","text":"","title":"Stats"},{"location":"api/rest/rest_api/#get-statsdocuments","text":"Statistiques sur les documents Description R\u00e9cup\u00e8re les statistiques sur les documents pr\u00e9sents dans la base de donn\u00e9es. Cette route calcule diff\u00e9rentes m\u00e9triques sur les documents index\u00e9s, notamment la distribution par th\u00e8me, par type de document, ainsi que les tendances r\u00e9centes d'ajout de documents. Args: skip: Nombre d'\u00e9l\u00e9ments \u00e0 ignorer pour la pagination. limit: Nombre maximal d'\u00e9l\u00e9ments \u00e0 retourner. db: Session de base de donn\u00e9es fournie par d\u00e9pendance. Returns: DocumentStats: Objet contenant les statistiques sur les documents. Raises: HTTPException: Si une erreur survient lors du calcul des statistiques. Input parameters Parameter In Type Default Nullable Description limit query integer 100 No Nombre maximal d'\u00e9l\u00e9ments \u00e0 retourner skip query integer 0 No Nombre d'\u00e9l\u00e9ments \u00e0 ignorer Response 200 OK application/json { \"totalCount\" : 0 , \"byTheme\" : {}, \"byType\" : {}, \"recentlyAdded\" : 0 , \"percentChange\" : 10.12 } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"totalCount\" : { \"type\" : \"integer\" , \"title\" : \"Totalcount\" }, \"byTheme\" : { \"additionalProperties\" : { \"type\" : \"integer\" }, \"type\" : \"object\" , \"title\" : \"Bytheme\" }, \"byType\" : { \"additionalProperties\" : { \"type\" : \"integer\" }, \"type\" : \"object\" , \"title\" : \"Bytype\" }, \"recentlyAdded\" : { \"type\" : \"integer\" , \"title\" : \"Recentlyadded\" }, \"percentChange\" : { \"type\" : \"number\" , \"title\" : \"Percentchange\" } }, \"type\" : \"object\" , \"required\" : [ \"totalCount\" , \"byTheme\" , \"byType\" , \"recentlyAdded\" , \"percentChange\" ], \"title\" : \"DocumentStats\" , \"description\" : \"Corps pour cr\u00e9er un chunk (texte + m\u00e9ta hi\u00e9rarchiques).\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" }","title":"GET /stats/documents"},{"location":"api/rest/rest_api/#get-statssearches","text":"Statistiques sur les recherches effectu\u00e9es Description R\u00e9cup\u00e8re les statistiques sur les recherches effectu\u00e9es dans le syst\u00e8me. Cette route analyse l'historique des recherches pour fournir des m\u00e9triques comme le nombre total de recherches, l'activit\u00e9 r\u00e9cente et les requ\u00eates les plus populaires. Args: skip: Nombre d'\u00e9l\u00e9ments \u00e0 ignorer pour la pagination. limit: Nombre maximal d'\u00e9l\u00e9ments \u00e0 retourner. db: Session de base de donn\u00e9es fournie par d\u00e9pendance. Returns: SearchStats: Objet contenant les statistiques sur les recherches. Raises: HTTPException: Si une erreur survient lors du calcul des statistiques. Input parameters Parameter In Type Default Nullable Description limit query integer 100 No Nombre maximal d'\u00e9l\u00e9ments \u00e0 retourner skip query integer 0 No Nombre d'\u00e9l\u00e9ments \u00e0 ignorer Response 200 OK application/json { \"totalCount\" : 0 , \"lastMonthCount\" : 0 , \"percentChange\" : 10.12 , \"topQueries\" : [ {} ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"totalCount\" : { \"type\" : \"integer\" , \"title\" : \"Totalcount\" }, \"lastMonthCount\" : { \"type\" : \"integer\" , \"title\" : \"Lastmonthcount\" }, \"percentChange\" : { \"type\" : \"number\" , \"title\" : \"Percentchange\" }, \"topQueries\" : { \"items\" : { \"additionalProperties\" : true , \"type\" : \"object\" }, \"type\" : \"array\" , \"title\" : \"Topqueries\" } }, \"type\" : \"object\" , \"required\" : [ \"totalCount\" , \"lastMonthCount\" , \"percentChange\" , \"topQueries\" ], \"title\" : \"SearchStats\" , \"description\" : \"Payload complet pour `POST /database/documents`.\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" }","title":"GET /stats/searches"},{"location":"api/rest/rest_api/#get-statssystem","text":"Statistiques syst\u00e8me globales Description R\u00e9cup\u00e8re les statistiques syst\u00e8me globales. Cette route analyse les m\u00e9triques de confiance des recherches effectu\u00e9es et l'\u00e9tat des corpus dans le syst\u00e8me pour fournir une vue d'ensemble de la performance et de l'\u00e9tat de l'indexation. Args: db: Session de base de donn\u00e9es fournie par d\u00e9pendance. Returns: SystemStats: Objet contenant les m\u00e9triques syst\u00e8me. Raises: HTTPException: Si une erreur survient lors du calcul des statistiques. Response 200 OK application/json { \"satisfaction\" : 10.12 , \"avgConfidence\" : 10.12 , \"percentChange\" : 10.12 , \"indexedCorpora\" : 0 , \"totalCorpora\" : 0 } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"satisfaction\" : { \"type\" : \"number\" , \"title\" : \"Satisfaction\" }, \"avgConfidence\" : { \"type\" : \"number\" , \"title\" : \"Avgconfidence\" }, \"percentChange\" : { \"type\" : \"number\" , \"title\" : \"Percentchange\" }, \"indexedCorpora\" : { \"type\" : \"integer\" , \"title\" : \"Indexedcorpora\" }, \"totalCorpora\" : { \"type\" : \"integer\" , \"title\" : \"Totalcorpora\" } }, \"type\" : \"object\" , \"required\" : [ \"satisfaction\" , \"avgConfidence\" , \"percentChange\" , \"indexedCorpora\" , \"totalCorpora\" ], \"title\" : \"SystemStats\" , \"description\" : \"Payload complet pour `POST /database/documents`.\" }","title":"GET /stats/system"},{"location":"api/rest/rest_api/#get-statsdashboard","text":"Toutes les statistiques pour le tableau de bord Description R\u00e9cup\u00e8re l'ensemble des statistiques pour le tableau de bord. Cette route agr\u00e8ge les r\u00e9sultats des diff\u00e9rentes fonctions de calcul de statistiques pour fournir un objet unique contenant toutes les m\u00e9triques n\u00e9cessaires au tableau de bord d'administration. Args: db: Session de base de donn\u00e9es fournie par d\u00e9pendance. Returns: DashboardStats: Objet contenant l'ensemble des statistiques. Raises: HTTPException: Si une erreur survient lors du calcul des statistiques. Response 200 OK application/json { \"documentStats\" : { \"totalCount\" : 0 , \"byTheme\" : {}, \"byType\" : {}, \"recentlyAdded\" : 0 , \"percentChange\" : 10.12 }, \"searchStats\" : { \"totalCount\" : 0 , \"lastMonthCount\" : 0 , \"percentChange\" : 10.12 , \"topQueries\" : [ {} ] }, \"systemStats\" : { \"satisfaction\" : 10.12 , \"avgConfidence\" : 10.12 , \"percentChange\" : 10.12 , \"indexedCorpora\" : 0 , \"totalCorpora\" : 0 } } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"documentStats\" : { \"$ref\" : \"#/components/schemas/DocumentStats\" }, \"searchStats\" : { \"$ref\" : \"#/components/schemas/SearchStats\" }, \"systemStats\" : { \"$ref\" : \"#/components/schemas/SystemStats\" } }, \"type\" : \"object\" , \"required\" : [ \"documentStats\" , \"searchStats\" , \"systemStats\" ], \"title\" : \"DashboardStats\" , \"description\" : \"Corps minimal pour cr\u00e9er un document (hors contenu).\" }","title":"GET /stats/dashboard"},{"location":"api/rest/rest_api/#post-statsrefresh","text":"Rafra\u00eechir le cache des statistiques Description Force le rafra\u00eechissement du cache des statistiques. Cette route permet d'invalider les caches potentiels et de forcer un recalcul complet de toutes les m\u00e9triques du syst\u00e8me. Utile apr\u00e8s des op\u00e9rations importantes comme des imports massifs ou des maintenances. Args: db: Session de base de donn\u00e9es fournie par d\u00e9pendance. Returns: dict: R\u00e9sultat de l'op\u00e9ration avec statut et message. Raises: HTTPException: Si une erreur survient lors du rafra\u00eechissement. Response 200 OK application/json Schema of the response body { \"additionalProperties\" : true , \"type\" : \"object\" , \"title\" : \"Response Refresh Stats Cache Stats Refresh Post\" }","title":"POST /stats/refresh"},{"location":"api/rest/rest_api/#endpoints","text":"","title":"Endpoints"},{"location":"api/rest/rest_api/#get","text":"Root Description Endpoint racine pour v\u00e9rifier l'\u00e9tat de l'API. Returns: dict: Message indiquant que l'API est en cours d'ex\u00e9cution. Response 200 OK application/json Schema of the response body","title":"GET /"},{"location":"api/rest/rest_api/#schemas","text":"","title":"Schemas"},{"location":"api/rest/rest_api/#askrequest","text":"Name Type enableThinking boolean filters modelName promptType string query string stream boolean theme","title":"AskRequest"},{"location":"api/rest/rest_api/#body_process_and_store_async_endpoint_pipeline_process_and_store_async_post","text":"Name Type file string ( binary )","title":"Body_process_and_store_async_endpoint_pipeline_process_and_store_async_post"},{"location":"api/rest/rest_api/#body_process_and_store_endpoint_pipeline_process_and_store_post","text":"Name Type file string ( binary )","title":"Body_process_and_store_endpoint_pipeline_process_and_store_post"},{"location":"api/rest/rest_api/#body_upload_and_process_file_doc_loader_upload_file_post","text":"Name Type file string ( binary )","title":"Body_upload_and_process_file_doc_loader_upload_file_post"},{"location":"api/rest/rest_api/#chunkcreate","text":"Name Type content string endChar integer hierarchyLevel integer id parentChunkId startChar integer","title":"ChunkCreate"},{"location":"api/rest/rest_api/#chunkresult","text":"Name Type chunkId integer content string context documentId integer documentType string hierarchyLevel integer publishDate string ( date ) score number theme string title string","title":"ChunkResult"},{"location":"api/rest/rest_api/#confidencemetrics","text":"Name Type level number message string stats","title":"ConfidenceMetrics"},{"location":"api/rest/rest_api/#dashboardstats","text":"Name Type documentStats DocumentStats searchStats SearchStats systemStats SystemStats","title":"DashboardStats"},{"location":"api/rest/rest_api/#documentcreate","text":"Name Type corpusId documentType string publishDate string ( date ) theme string title string","title":"DocumentCreate"},{"location":"api/rest/rest_api/#documentresponse","text":"Name Type chunkCount integer corpusId documentType string id integer indexNeeded boolean publishDate string ( date ) theme string title string","title":"DocumentResponse"},{"location":"api/rest/rest_api/#documentstats","text":"Name Type byTheme byType percentChange number recentlyAdded integer totalCount integer","title":"DocumentStats"},{"location":"api/rest/rest_api/#documentupdate","text":"Name Type corpusId documentType id integer publishDate theme title","title":"DocumentUpdate"},{"location":"api/rest/rest_api/#documentwithchunks","text":"Name Type chunks Array< ChunkCreate > document DocumentCreate","title":"DocumentWithChunks"},{"location":"api/rest/rest_api/#hierarchicalcontext","text":"Name Type level0 level1 level2","title":"HierarchicalContext"},{"location":"api/rest/rest_api/#httpvalidationerror","text":"Name Type detail Array< ValidationError >","title":"HTTPValidationError"},{"location":"api/rest/rest_api/#indexstatus","text":"Name Type chunkCount integer configExists boolean corpusId indexedChunks integer indexExists boolean indexType isIndexed boolean lastIndexed","title":"IndexStatus"},{"location":"api/rest/rest_api/#searchrequest","text":"Name Type corpusId documentType endDate filterByRelevance boolean hierarchical boolean hierarchyLevel normalizeScores boolean query string startDate theme topK integer","title":"SearchRequest"},{"location":"api/rest/rest_api/#searchresponse","text":"Name Type confidence message normalized boolean query string results Array< ChunkResult > topK integer totalResults integer","title":"SearchResponse"},{"location":"api/rest/rest_api/#searchstats","text":"Name Type lastMonthCount integer percentChange number topQueries Array<> totalCount integer","title":"SearchStats"},{"location":"api/rest/rest_api/#systemstats","text":"Name Type avgConfidence number indexedCorpora integer percentChange number satisfaction number totalCorpora integer","title":"SystemStats"},{"location":"api/rest/rest_api/#updatewithchunks","text":"Name Type document DocumentUpdate newChunks","title":"UpdateWithChunks"},{"location":"api/rest/rest_api/#validationerror","text":"Name Type loc Array<> msg string type string","title":"ValidationError"},{"location":"lib/askai/rag_lib/","text":"Module rag (Retrieval-Augmented Generation) Ce module impl\u00e9mente un processeur RAG optimis\u00e9 pour petits LLM qui combine recherche vectorielle et g\u00e9n\u00e9ration de r\u00e9ponses via Qwen3. Il inclut une gestion avanc\u00e9e du raisonnement et du contexte pour une transparence maximale. Table des mati\u00e8res Installation Mod\u00e8les (schemas) Classe RAGProcessor Constructeur M\u00e9thode format_context M\u00e9thode get_prompt_template M\u00e9thode retrieve_documents M\u00e9thode retrieve_and_generate M\u00e9thode retrieve_and_generate_stream Classe ModelLoader Constructeur M\u00e9thode load M\u00e9thode generate M\u00e9thode generate_with_thinking Classe AsyncStreamedResponse M\u00e9thode generate_stream Mode Thinking & Contexte Exemple d'utilisation Installation # Installer les d\u00e9pendances uv pip install -r requirements.txt # Pr\u00e9requis pour les mod\u00e8les Qwen3 uv pip install protobuf tokenizers> = 0 .13.3 Mod\u00e8les (schemas) Les templates de prompts utilis\u00e9s par le processeur RAG se trouvent dans prompt_schemas.py : PromptTemplate : mod\u00e8le de base pour les templates de prompts. StandardRAGPrompt : prompt optimis\u00e9 pour les requ\u00eates RAG simples. SummaryRAGPrompt : prompt pour les t\u00e2ches de r\u00e9sum\u00e9 de documents. ComparisonRAGPrompt : prompt pour comparer des \u00e9l\u00e9ments \u00e0 partir des documents. Pour plus de d\u00e9tails sur ces sch\u00e9mas, consultez le fichier source prompt_schemas.py. Classe RAGProcessor Constructeur processor = RAGProcessor ( model_loader : ModelLoader , search_engine : SearchEngine , db_session : Session , max_tokens_per_doc : int = 300 , max_docs : int = 5 , ) Initialise : model_loader : chargeur de mod\u00e8le LLM search_engine : moteur de recherche vectorielle db_session : session de base de donn\u00e9es SQLAlchemy max_tokens_per_doc : nombre maximum de tokens par document max_docs : nombre maximum de documents \u00e0 utiliser M\u00e9thode format_context def format_context ( self , search_results : SearchResponse ) -> str : \"\"\"Formate les r\u00e9sultats de recherche en contexte structur\u00e9 pour le LLM. Utilise les r\u00e9sultats d'une requ\u00eate pour cr\u00e9er un contexte format\u00e9 qui sera utilis\u00e9 dans le prompt envoy\u00e9 au mod\u00e8le LLM. Args: search_results: R\u00e9ponse de recherche contenant les chunks pertinents. Returns: str: Contexte format\u00e9 pr\u00eat \u00e0 \u00eatre inject\u00e9 dans le prompt. \"\"\" ... But : formater les r\u00e9sultats de recherche en contexte structur\u00e9 pour le LLM Arguments : search_results : r\u00e9ponse de recherche contenant les chunks pertinents Fonctionnement : It\u00e8re sur chaque r\u00e9sultat et cr\u00e9e une repr\u00e9sentation structur\u00e9e Inclut les m\u00e9tadonn\u00e9es: titre, source, th\u00e8me, date Ajoute le contexte hi\u00e9rarchique si disponible (sections parentes) Indique le score de pertinence Retour : cha\u00eene de caract\u00e8res format\u00e9e pour injecter dans le prompt M\u00e9thode get_prompt_template def get_prompt_template ( self , query : str , context : str , prompt_type : str = \"standard\" , ** kwargs ) -> PromptTemplate : \"\"\"Retourne un template de prompt adapt\u00e9 au type de requ\u00eate. Args: query: Question de l'utilisateur. context: Contexte documentaire format\u00e9. prompt_type: Type de prompt \u00e0 utiliser ('standard', 'summary', 'comparison'). **kwargs: Param\u00e8tres additionnels sp\u00e9cifiques au type de prompt. Returns: PromptTemplate: Template de prompt configur\u00e9 avec les variables appropri\u00e9es. Raises: ValueError: Si le type de prompt sp\u00e9cifi\u00e9 n'est pas reconnu. \"\"\" ... But : cr\u00e9er un template de prompt adapt\u00e9 au type de requ\u00eate Arguments : query : question de l'utilisateur context : contexte documentaire format\u00e9 prompt_type : type de prompt ('standard', 'summary', 'comparison') **kwargs : param\u00e8tres additionnels sp\u00e9cifiques au type de prompt Types disponibles : standard : prompt pour questions/r\u00e9ponses g\u00e9n\u00e9riques summary : prompt pour r\u00e9sum\u00e9 de documents comparison : prompt pour analyse comparative Retour : instance de PromptTemplate configur\u00e9e M\u00e9thode retrieve_documents async def retrieve_documents ( self , query : str , filters : Dict [ str , Any ] = None ) -> SearchResponse : \"\"\"R\u00e9cup\u00e8re les documents pertinents pour une requ\u00eate donn\u00e9e. Effectue une recherche dans la base de donn\u00e9es vectorielle et retourne les r\u00e9sultats format\u00e9s selon le sch\u00e9ma standard de l'application. Args: query: Question de l'utilisateur. filters: Filtres \u00e0 appliquer lors de la recherche. Returns: SearchResponse: R\u00e9ponse contenant les r\u00e9sultats de recherche pertinents. \"\"\" ... But : r\u00e9cup\u00e9rer les documents pertinents pour une requ\u00eate donn\u00e9e Arguments : query : question de l'utilisateur filters : filtres \u00e0 appliquer lors de la recherche Fonctionnement : Construit une requ\u00eate SearchRequest avec les param\u00e8tres fournis Ex\u00e9cute la recherche via search_engine.hybrid_search Journalise le nombre de documents r\u00e9cup\u00e9r\u00e9s Retour : r\u00e9ponse de recherche ( SearchResponse ) contenant les documents pertinents M\u00e9thode retrieve_and_generate async def retrieve_and_generate ( self , query : str , filters : Dict [ str , Any ] = None , prompt_type : str = \"standard\" , generation_kwargs : Dict [ str , Any ] = None , enable_thinking : Optional [ bool ] = None , ** prompt_kwargs ) -> tuple : \"\"\"R\u00e9cup\u00e8re les documents pertinents et g\u00e9n\u00e8re une r\u00e9ponse. Args: query: Question de l'utilisateur. filters: Filtres \u00e0 appliquer lors de la recherche. prompt_type: Type de prompt \u00e0 utiliser ('standard', 'summary', 'comparison'). generation_kwargs: Param\u00e8tres additionnels pour la g\u00e9n\u00e9ration de texte. enable_thinking: Active ou d\u00e9sactive le mode de r\u00e9flexion. Si None, utilise la configuration du mod\u00e8le. **prompt_kwargs: Param\u00e8tres additionnels pour le template de prompt. Returns: tuple: - Si enable_thinking=True : (thinking, response, search_results) - Si enable_thinking=False : (response, search_results) \"\"\" ... But : r\u00e9cup\u00e9rer les documents pertinents et g\u00e9n\u00e9rer une r\u00e9ponse compl\u00e8te Arguments : query : question de l'utilisateur filters : filtres \u00e0 appliquer lors de la recherche prompt_type : type de prompt \u00e0 utiliser generation_kwargs : param\u00e8tres pour la g\u00e9n\u00e9ration de texte enable_thinking : active le mode de r\u00e9flexion du mod\u00e8le **prompt_kwargs : param\u00e8tres additionnels pour le template de prompt Fonctionnement : R\u00e9cup\u00e8re les documents pertinents via retrieve_documents Formate le contexte documentaire avec format_context Cr\u00e9e le prompt avec le template appropri\u00e9 G\u00e9n\u00e8re la r\u00e9ponse avec le mod\u00e8le LLM Retour : Avec enable_thinking=True : tuple (thinking, response, search_results) Avec enable_thinking=False : tuple (response, search_results) M\u00e9thode retrieve_and_generate_stream async def retrieve_and_generate_stream ( self , query : str , filters : Dict [ str , Any ] = None , prompt_type : str = \"standard\" , generation_kwargs : Dict [ str , Any ] = None , enable_thinking : Optional [ bool ] = None , ** prompt_kwargs ) -> AsyncGenerator [ Dict [ str , Any ], None ]: \"\"\"R\u00e9cup\u00e8re les documents pertinents et g\u00e9n\u00e8re une r\u00e9ponse en streaming. Cette m\u00e9thode enrichit la r\u00e9ponse avec les documents utilis\u00e9s pour la g\u00e9n\u00e9ration. Chaque fragment retourn\u00e9 est un dictionnaire identifiant son type et contenu. Args: query: Question de l'utilisateur. filters: Filtres \u00e0 appliquer lors de la recherche. prompt_type: Type de prompt \u00e0 utiliser ('standard', 'summary', 'comparison'). generation_kwargs: Param\u00e8tres additionnels pour la g\u00e9n\u00e9ration de texte. enable_thinking: Active ou d\u00e9sactive le mode de r\u00e9flexion. Si None, utilise la configuration du mod\u00e8le. **prompt_kwargs: Param\u00e8tres additionnels pour le template de prompt. Yields: Dict[str, Any]: Fragments de la r\u00e9ponse ou m\u00e9tadonn\u00e9es avec leur type : - {\"type\": \"thinking\", \"content\": str} pour les parties de r\u00e9flexion - {\"type\": \"response\", \"content\": str} pour les parties de r\u00e9ponse - {\"type\": \"context\", \"content\": Dict} pour le contexte utilis\u00e9 - {\"type\": \"error\", \"content\": str} en cas d'erreur - {\"type\": \"done\", \"content\": \"\"} \u00e0 la fin du streaming \"\"\" ... But : r\u00e9cup\u00e9rer les documents et g\u00e9n\u00e9rer une r\u00e9ponse en streaming (progressive) Arguments : identiques \u00e0 retrieve_and_generate Fonctionnement : R\u00e9cup\u00e8re et formate les documents comme retrieve_and_generate Transmet imm\u00e9diatement le contexte au client ( {\"type\": \"context\", \"content\": search_results.dict()} ) Utilise AsyncStreamedResponse pour g\u00e9n\u00e9rer la r\u00e9ponse progressivement \u00c9met chaque fragment selon son type (r\u00e9ponse, r\u00e9flexion, contexte) Signale la fin du streaming avec un \u00e9v\u00e9nement {\"type\": \"done\"} Retour : g\u00e9n\u00e9rateur asynchrone de fragments typ\u00e9s Classe ModelLoader Constructeur loader = ModelLoader ( model_name : str = \"Qwen/Qwen3-0.6B\" , device : Optional [ str ] = None , load_in_8bit : bool = False , base_path : str = \"askai/models\" , thinking_enabled : bool = True , auto_load : bool = False , test_mode : bool = False , auto_fix : bool = True ) But : g\u00e9rer le chargement et la configuration des mod\u00e8les LLM Arguments : model_name : nom du mod\u00e8le \u00e0 charger (Qwen/Qwen3-0.6B, Qwen/Qwen3-1.7B) device : p\u00e9riph\u00e9rique de calcul ('cpu', 'cuda', 'auto') load_in_8bit : active la quantification 8-bit base_path : chemin vers le r\u00e9pertoire des mod\u00e8les thinking_enabled : active le mode de r\u00e9flexion par d\u00e9faut auto_load : charge automatiquement le mod\u00e8le \u00e0 l'initialisation test_mode : active un mode de simulation sans charger de mod\u00e8le M\u00e9thode load def load ( self ) -> None : \"\"\"Charge le mod\u00e8le et le tokenizer en m\u00e9moire. V\u00e9rifie si le mode test est activ\u00e9, puis charge le tokenizer et le mod\u00e8le depuis HuggingFace ou en local, et configure les options d'optimisation. Raises: RuntimeError: Si le chargement \u00e9choue. \"\"\" ... But : charger le mod\u00e8le et le tokenizer en m\u00e9moire Fonctionnement : V\u00e9rifie si le mode test est activ\u00e9 Charge le tokenizer et le mod\u00e8le depuis HuggingFace ou local Configure les options de quantification et d'optimisation Raises : RuntimeError si le chargement \u00e9choue M\u00e9thode generate def generate ( self , prompt : str , enable_thinking : bool = True , max_new_tokens : int = 2048 , do_sample : bool = True , temperature : float = 0.6 , top_p : float = 0.95 , top_k : int = 20 , ** kwargs ) -> str : \"\"\"G\u00e9n\u00e8re du texte \u00e0 partir d'un prompt donn\u00e9. Args: prompt: Texte d'entr\u00e9e pour la g\u00e9n\u00e9ration. enable_thinking: Active le mode de r\u00e9flexion. max_new_tokens: Nombre maximum de tokens \u00e0 g\u00e9n\u00e9rer. do_sample: Utilise l'\u00e9chantillonnage pour la g\u00e9n\u00e9ration. temperature: Contr\u00f4le la cr\u00e9ativit\u00e9 (plus \u00e9lev\u00e9 = plus al\u00e9atoire). top_p: Filtrage nucleus sampling. top_k: Nombre de tokens consid\u00e9r\u00e9s \u00e0 chaque \u00e9tape. **kwargs: Param\u00e8tres additionnels pour la g\u00e9n\u00e9ration. Returns: str: Texte g\u00e9n\u00e9r\u00e9 par le mod\u00e8le. \"\"\" ... But : g\u00e9n\u00e9rer du texte \u00e0 partir d'un prompt donn\u00e9 Arguments : prompt : texte d'entr\u00e9e pour la g\u00e9n\u00e9ration enable_thinking : active le mode de r\u00e9flexion max_new_tokens : nombre maximum de tokens \u00e0 g\u00e9n\u00e9rer do_sample : utilise l'\u00e9chantillonnage pour la g\u00e9n\u00e9ration temperature : contr\u00f4le la cr\u00e9ativit\u00e9 (plus \u00e9lev\u00e9 = plus al\u00e9atoire) top_p : filtrage nucleus sampling top_k : nombre de tokens consid\u00e9r\u00e9s \u00e0 chaque \u00e9tape Retour : texte g\u00e9n\u00e9r\u00e9 par le mod\u00e8le M\u00e9thode generate_with_thinking def generate_with_thinking ( self , prompt : str , max_new_tokens : int = 2048 , do_sample : bool = True , temperature : float = 0.6 , ** kwargs ) -> Tuple [ str , str ]: \"\"\"G\u00e9n\u00e8re du texte avec s\u00e9paration explicite entre r\u00e9flexion et r\u00e9ponse. Cette m\u00e9thode capture le raisonnement \u00e9tape par \u00e9tape du mod\u00e8le et le s\u00e9pare de la r\u00e9ponse finale gr\u00e2ce \u00e0 des balises sp\u00e9cifiques dans le prompt. Args: prompt: Texte d'entr\u00e9e pour la g\u00e9n\u00e9ration. max_new_tokens: Nombre maximum de tokens \u00e0 g\u00e9n\u00e9rer. do_sample: Utilise l'\u00e9chantillonnage pour la g\u00e9n\u00e9ration. temperature: Contr\u00f4le la cr\u00e9ativit\u00e9 (plus \u00e9lev\u00e9 = plus al\u00e9atoire). **kwargs: Param\u00e8tres additionnels pour la g\u00e9n\u00e9ration. Returns: Tuple[str, str]: Tuple contenant (r\u00e9flexion, r\u00e9ponse). \"\"\" ... But : g\u00e9n\u00e9rer du texte avec s\u00e9paration entre r\u00e9flexion et r\u00e9ponse finale Arguments : similaires \u00e0 generate Fonctionnement : Ajoute des instructions au mod\u00e8le pour s\u00e9parer r\u00e9flexion et r\u00e9ponse Utilise des balises sp\u00e9ciales ( <thinking> , </thinking> , <answer> ) dans le prompt Extrait et s\u00e9pare la partie r\u00e9flexion de la partie r\u00e9ponse Retour : tuple ( thinking , answer ) contenant le raisonnement et la r\u00e9ponse Classe AsyncStreamedResponse Constructeur streamer = AsyncStreamedResponse ( model_loader : ModelLoader , filter_thinking : bool = False ) But : g\u00e9rer les r\u00e9ponses en streaming pour les interactions avec le LLM Arguments : model_loader : chargeur de mod\u00e8le LLM filter_thinking : si True, filtre le contenu de r\u00e9flexion des r\u00e9ponses M\u00e9thode generate_stream async def generate_stream ( self , prompt : str , enable_thinking : bool = True , chunk_size : int = 3 , ** kwargs ) -> AsyncGenerator [ Dict [ str , Any ], None ]: \"\"\"G\u00e9n\u00e8re une r\u00e9ponse en streaming progressif avec diff\u00e9renciation des types. Args: prompt: Texte d'entr\u00e9e pour la g\u00e9n\u00e9ration. enable_thinking: Active le mode de r\u00e9flexion. chunk_size: Nombre de mots par fragment en mode simulation. **kwargs: Param\u00e8tres additionnels pour la g\u00e9n\u00e9ration. Yields: Dict[str, Any]: Fragments typ\u00e9s de la g\u00e9n\u00e9ration: - {\"type\": \"thinking\", \"content\": str} pour les parties de r\u00e9flexion - {\"type\": \"response\", \"content\": str} pour les parties de r\u00e9ponse \"\"\" ... But : g\u00e9n\u00e9rer une r\u00e9ponse en streaming progressif avec diff\u00e9renciation des types Arguments : prompt : texte d'entr\u00e9e pour la g\u00e9n\u00e9ration enable_thinking : active le mode de r\u00e9flexion chunk_size : nombre de mots par fragment en mode simulation **kwargs : param\u00e8tres additionnels pour la g\u00e9n\u00e9ration Fonctionnement : D\u00e9tecte si le mod\u00e8le supporte nativement le streaming Si oui, utilise le streaming natif du mod\u00e8le Sinon, simule le streaming par d\u00e9coupage progressif Identifie et type chaque fragment (r\u00e9flexion ou r\u00e9ponse) Retour : g\u00e9n\u00e9rateur asynchrone de fragments typ\u00e9s Mode Thinking & Contexte Le module RAG impl\u00e9mente une approche transparente qui permet aux utilisateurs d'acc\u00e9der: Au raisonnement complet du mod\u00e8le (mode \"thinking\") Au contexte documentaire utilis\u00e9 pour g\u00e9n\u00e9rer la r\u00e9ponse Raisonnement (Thinking) Le mode thinking permet de visualiser: Le processus de r\u00e9flexion \u00e9tape par \u00e9tape du mod\u00e8le L'analyse des documents fournis Le raisonnement pour arriver \u00e0 la conclusion Les r\u00e9f\u00e9rences explicites aux sources Exemple d'activation: # Mode normal response , search_results = await rag_processor . retrieve_and_generate ( query = \"Comment r\u00e9duire les \u00e9missions de CO2?\" , enable_thinking = False ) # Mode avec raisonnement visible thinking , response , search_results = await rag_processor . retrieve_and_generate ( query = \"Comment r\u00e9duire les \u00e9missions de CO2?\" , enable_thinking = True ) print ( \"Raisonnement du mod\u00e8le:\" ) print ( thinking ) Contexte documentaire Le contexte documentaire permet \u00e0 l'utilisateur de: V\u00e9rifier les sources utilis\u00e9es pour g\u00e9n\u00e9rer la r\u00e9ponse \u00c9valuer la pertinence des documents r\u00e9cup\u00e9r\u00e9s Acc\u00e9der aux m\u00e9tadonn\u00e9es compl\u00e8tes (source, date, score) Explorer le contexte hi\u00e9rarchique (sections parentes) Exploitation du contexte: # R\u00e9cup\u00e9ration de la r\u00e9ponse et du contexte response , search_results = await rag_processor . retrieve_and_generate ( query = \"Quelles sont les r\u00e9glementations sur l'isolation thermique?\" , enable_thinking = False ) # Affichage des sources utilis\u00e9es print ( f \"R\u00e9ponse bas\u00e9e sur { len ( search_results . results ) } source(s):\" ) for i , result in enumerate ( search_results . results ): print ( f \" { i + 1 } . { result . title } (score: { result . score : .2f } )\" ) print ( f \" Th\u00e8me: { result . theme } , Type: { result . document_type } \" ) print ( f \" Date: { result . publish_date } \" ) print ( f \" Extrait: { result . content [: 100 ] } ...\" ) Streaming avec types diff\u00e9renci\u00e9s En mode streaming, chaque fragment \u00e9mis est typ\u00e9 pour permettre: L'affichage diff\u00e9renci\u00e9 du raisonnement et de la r\u00e9ponse L'acc\u00e8s imm\u00e9diat au contexte d\u00e8s sa r\u00e9cup\u00e9ration La mise en forme adapt\u00e9e selon le type dans l'interface async for chunk in rag_processor . retrieve_and_generate_stream ( query = \"Expliquez les normes de s\u00e9curit\u00e9 incendie\" , enable_thinking = True ): if chunk [ \"type\" ] == \"context\" : # Afficher les sources dans l'interface display_sources ( chunk [ \"content\" ][ \"results\" ]) elif chunk [ \"type\" ] == \"thinking\" : # Afficher en italique gris dans une zone d\u00e9di\u00e9e append_to_thinking_area ( chunk [ \"content\" ]) elif chunk [ \"type\" ] == \"response\" : # Afficher en texte normal dans la zone de r\u00e9ponse append_to_response_area ( chunk [ \"content\" ]) Exemple d'utilisation import asyncio from sqlalchemy import create_engine from sqlalchemy.orm import sessionmaker from vectordb.src.search import SearchEngine from askai.src.model_loader import ModelLoader from askai.src.rag import RAGProcessor # Fonction asynchrone principale async def main (): # 1. Pr\u00e9parer la session de base de donn\u00e9es engine_db = create_engine ( \"postgresql://user:password@localhost/clea\" ) SessionLocal = sessionmaker ( bind = engine_db ) db = SessionLocal () # 2. Initialiser le moteur de recherche search_engine = SearchEngine () # 3. Initialiser le chargeur de mod\u00e8le (en mode test pour la d\u00e9mo) model_loader = ModelLoader ( model_name = \"Qwen/Qwen3-0.6B\" , test_mode = True , # Remplacer par False en production thinking_enabled = True ) # 4. Cr\u00e9er le processeur RAG rag_processor = RAGProcessor ( model_loader = model_loader , search_engine = search_engine , db_session = db , max_docs = 5 ) # 5. Exemple de requ\u00eate avec g\u00e9n\u00e9ration compl\u00e8te et acc\u00e8s au raisonnement query = \"Comment r\u00e9duire l'empreinte carbone d'une entreprise industrielle?\" filters = { \"theme\" : \"RSE\" , \"normalize_scores\" : True } thinking , response , search_results = await rag_processor . retrieve_and_generate ( query = query , filters = filters , prompt_type = \"standard\" , enable_thinking = True ) # Afficher la r\u00e9ponse print ( f \"R\u00e9ponse finale: \\n { response } \" ) # Afficher le raisonnement si n\u00e9cessaire print ( \" \\n Raisonnement du mod\u00e8le:\" ) print ( thinking ) # Afficher les sources utilis\u00e9es print ( \" \\n Sources utilis\u00e9es:\" ) for i , result in enumerate ( search_results . results ): print ( f \" { i + 1 } . { result . title } (score: { result . score : .2f } )\" ) # 6. Exemple de requ\u00eate avec g\u00e9n\u00e9ration en streaming typ\u00e9 print ( \" \\n G\u00e9n\u00e9ration en streaming avec types:\" ) # Conteneurs pour collecter diff\u00e9rents types de contenu thinking_content = [] response_content = [] context = None async for chunk in rag_processor . retrieve_and_generate_stream ( query = \"Quelles sont les meilleures pratiques de gestion des d\u00e9chets?\" , filters = { \"theme\" : \"Environnement\" }, enable_thinking = True ): if chunk [ \"type\" ] == \"thinking\" : thinking_content . append ( chunk [ \"content\" ]) print ( \"[Thinking] \" , chunk [ \"content\" ], end = \"\" ) elif chunk [ \"type\" ] == \"response\" : response_content . append ( chunk [ \"content\" ]) print ( \"[R\u00e9ponse] \" , chunk [ \"content\" ], end = \"\" ) elif chunk [ \"type\" ] == \"context\" : context = chunk [ \"content\" ] print ( f \" \\n [Contexte r\u00e9cup\u00e9r\u00e9: { len ( context [ 'results' ]) } documents]\" ) elif chunk [ \"type\" ] == \"done\" : print ( \" \\n [G\u00e9n\u00e9ration termin\u00e9e]\" ) # Ex\u00e9cuter la fonction asynchrone if __name__ == \"__main__\" : asyncio . run ( main ()) Exemple avec interface FastAPI from fastapi import FastAPI , Depends , HTTPException , BackgroundTasks , WebSocket from fastapi.responses import StreamingResponse from sqlalchemy.orm import Session from typing import Dict , Optional , List import json from askai.src.model_loader import ModelLoader from askai.src.rag import RAGProcessor from vectordb.src.search import SearchEngine from vectordb.src.database import get_db app = FastAPI () # Singleton pour le mod\u00e8le (partag\u00e9 entre requ\u00eates) model_loader = ModelLoader ( model_name = \"Qwen/Qwen3-0.6B\" , auto_load = True , # Charge le mod\u00e8le au d\u00e9marrage thinking_enabled = True ) search_engine = SearchEngine () @app . post ( \"/askai/query\" ) async def query ( question : str , theme : Optional [ str ] = None , enable_thinking : bool = False , db : Session = Depends ( get_db ) ): # Cr\u00e9er un processeur RAG pour cette requ\u00eate rag_processor = RAGProcessor ( model_loader = model_loader , search_engine = search_engine , db_session = db ) # G\u00e9n\u00e9rer la r\u00e9ponse avec ou sans raisonnement if enable_thinking : thinking , response , search_results = await rag_processor . retrieve_and_generate ( query = question , filters = { \"theme\" : theme } if theme else {}, enable_thinking = True ) # Retourner \u00e0 la fois la r\u00e9ponse, le raisonnement et les sources return { \"question\" : question , \"answer\" : response , \"thinking\" : thinking , \"sources\" : [ { \"title\" : result . title , \"theme\" : result . theme , \"document_type\" : result . document_type , \"publish_date\" : result . publish_date , \"score\" : result . score } for result in search_results . results ] } else : # Version sans raisonnement response , search_results = await rag_processor . retrieve_and_generate ( query = question , filters = { \"theme\" : theme } if theme else {}, enable_thinking = False ) return { \"question\" : question , \"answer\" : response , \"sources\" : [ { \"title\" : result . title , \"score\" : result . score } for result in search_results . results ] } @app . websocket ( \"/askai/ws\" ) async def websocket_endpoint ( websocket : WebSocket , db : Session = Depends ( get_db )): await websocket . accept () # Recevoir la requ\u00eate initiale data = await websocket . receive_text () request = json . loads ( data ) # Cr\u00e9er un processeur RAG rag_processor = RAGProcessor ( model_loader = model_loader , search_engine = search_engine , db_session = db ) # Transmettre les fragments typ\u00e9s via le WebSocket async for chunk in rag_processor . retrieve_and_generate_stream ( query = request [ \"question\" ], filters = { \"theme\" : request . get ( \"theme\" )} if \"theme\" in request else {}, enable_thinking = request . get ( \"enable_thinking\" , False ) ): await websocket . send_json ( chunk ) Voir aussi : les endpoints FastAPI dans askai_endpoint.py \u2013 POST /askai/query \u2192 g\u00e9n\u00e8re une r\u00e9ponse compl\u00e8te avec raisonnement et contexte optionnels","title":"RAG"},{"location":"lib/askai/rag_lib/#module-rag-retrieval-augmented-generation","text":"Ce module impl\u00e9mente un processeur RAG optimis\u00e9 pour petits LLM qui combine recherche vectorielle et g\u00e9n\u00e9ration de r\u00e9ponses via Qwen3. Il inclut une gestion avanc\u00e9e du raisonnement et du contexte pour une transparence maximale.","title":"Module rag (Retrieval-Augmented Generation)"},{"location":"lib/askai/rag_lib/#table-des-matieres","text":"Installation Mod\u00e8les (schemas) Classe RAGProcessor Constructeur M\u00e9thode format_context M\u00e9thode get_prompt_template M\u00e9thode retrieve_documents M\u00e9thode retrieve_and_generate M\u00e9thode retrieve_and_generate_stream Classe ModelLoader Constructeur M\u00e9thode load M\u00e9thode generate M\u00e9thode generate_with_thinking Classe AsyncStreamedResponse M\u00e9thode generate_stream Mode Thinking & Contexte Exemple d'utilisation","title":"Table des mati\u00e8res"},{"location":"lib/askai/rag_lib/#installation","text":"# Installer les d\u00e9pendances uv pip install -r requirements.txt # Pr\u00e9requis pour les mod\u00e8les Qwen3 uv pip install protobuf tokenizers> = 0 .13.3","title":"Installation"},{"location":"lib/askai/rag_lib/#modeles-schemas","text":"Les templates de prompts utilis\u00e9s par le processeur RAG se trouvent dans prompt_schemas.py : PromptTemplate : mod\u00e8le de base pour les templates de prompts. StandardRAGPrompt : prompt optimis\u00e9 pour les requ\u00eates RAG simples. SummaryRAGPrompt : prompt pour les t\u00e2ches de r\u00e9sum\u00e9 de documents. ComparisonRAGPrompt : prompt pour comparer des \u00e9l\u00e9ments \u00e0 partir des documents. Pour plus de d\u00e9tails sur ces sch\u00e9mas, consultez le fichier source prompt_schemas.py.","title":"Mod\u00e8les (schemas)"},{"location":"lib/askai/rag_lib/#classe-ragprocessor","text":"","title":"Classe RAGProcessor"},{"location":"lib/askai/rag_lib/#constructeur","text":"processor = RAGProcessor ( model_loader : ModelLoader , search_engine : SearchEngine , db_session : Session , max_tokens_per_doc : int = 300 , max_docs : int = 5 , ) Initialise : model_loader : chargeur de mod\u00e8le LLM search_engine : moteur de recherche vectorielle db_session : session de base de donn\u00e9es SQLAlchemy max_tokens_per_doc : nombre maximum de tokens par document max_docs : nombre maximum de documents \u00e0 utiliser","title":"Constructeur"},{"location":"lib/askai/rag_lib/#methode-format_context","text":"def format_context ( self , search_results : SearchResponse ) -> str : \"\"\"Formate les r\u00e9sultats de recherche en contexte structur\u00e9 pour le LLM. Utilise les r\u00e9sultats d'une requ\u00eate pour cr\u00e9er un contexte format\u00e9 qui sera utilis\u00e9 dans le prompt envoy\u00e9 au mod\u00e8le LLM. Args: search_results: R\u00e9ponse de recherche contenant les chunks pertinents. Returns: str: Contexte format\u00e9 pr\u00eat \u00e0 \u00eatre inject\u00e9 dans le prompt. \"\"\" ... But : formater les r\u00e9sultats de recherche en contexte structur\u00e9 pour le LLM Arguments : search_results : r\u00e9ponse de recherche contenant les chunks pertinents Fonctionnement : It\u00e8re sur chaque r\u00e9sultat et cr\u00e9e une repr\u00e9sentation structur\u00e9e Inclut les m\u00e9tadonn\u00e9es: titre, source, th\u00e8me, date Ajoute le contexte hi\u00e9rarchique si disponible (sections parentes) Indique le score de pertinence Retour : cha\u00eene de caract\u00e8res format\u00e9e pour injecter dans le prompt","title":"M\u00e9thode format_context"},{"location":"lib/askai/rag_lib/#methode-get_prompt_template","text":"def get_prompt_template ( self , query : str , context : str , prompt_type : str = \"standard\" , ** kwargs ) -> PromptTemplate : \"\"\"Retourne un template de prompt adapt\u00e9 au type de requ\u00eate. Args: query: Question de l'utilisateur. context: Contexte documentaire format\u00e9. prompt_type: Type de prompt \u00e0 utiliser ('standard', 'summary', 'comparison'). **kwargs: Param\u00e8tres additionnels sp\u00e9cifiques au type de prompt. Returns: PromptTemplate: Template de prompt configur\u00e9 avec les variables appropri\u00e9es. Raises: ValueError: Si le type de prompt sp\u00e9cifi\u00e9 n'est pas reconnu. \"\"\" ... But : cr\u00e9er un template de prompt adapt\u00e9 au type de requ\u00eate Arguments : query : question de l'utilisateur context : contexte documentaire format\u00e9 prompt_type : type de prompt ('standard', 'summary', 'comparison') **kwargs : param\u00e8tres additionnels sp\u00e9cifiques au type de prompt Types disponibles : standard : prompt pour questions/r\u00e9ponses g\u00e9n\u00e9riques summary : prompt pour r\u00e9sum\u00e9 de documents comparison : prompt pour analyse comparative Retour : instance de PromptTemplate configur\u00e9e","title":"M\u00e9thode get_prompt_template"},{"location":"lib/askai/rag_lib/#methode-retrieve_documents","text":"async def retrieve_documents ( self , query : str , filters : Dict [ str , Any ] = None ) -> SearchResponse : \"\"\"R\u00e9cup\u00e8re les documents pertinents pour une requ\u00eate donn\u00e9e. Effectue une recherche dans la base de donn\u00e9es vectorielle et retourne les r\u00e9sultats format\u00e9s selon le sch\u00e9ma standard de l'application. Args: query: Question de l'utilisateur. filters: Filtres \u00e0 appliquer lors de la recherche. Returns: SearchResponse: R\u00e9ponse contenant les r\u00e9sultats de recherche pertinents. \"\"\" ... But : r\u00e9cup\u00e9rer les documents pertinents pour une requ\u00eate donn\u00e9e Arguments : query : question de l'utilisateur filters : filtres \u00e0 appliquer lors de la recherche Fonctionnement : Construit une requ\u00eate SearchRequest avec les param\u00e8tres fournis Ex\u00e9cute la recherche via search_engine.hybrid_search Journalise le nombre de documents r\u00e9cup\u00e9r\u00e9s Retour : r\u00e9ponse de recherche ( SearchResponse ) contenant les documents pertinents","title":"M\u00e9thode retrieve_documents"},{"location":"lib/askai/rag_lib/#methode-retrieve_and_generate","text":"async def retrieve_and_generate ( self , query : str , filters : Dict [ str , Any ] = None , prompt_type : str = \"standard\" , generation_kwargs : Dict [ str , Any ] = None , enable_thinking : Optional [ bool ] = None , ** prompt_kwargs ) -> tuple : \"\"\"R\u00e9cup\u00e8re les documents pertinents et g\u00e9n\u00e8re une r\u00e9ponse. Args: query: Question de l'utilisateur. filters: Filtres \u00e0 appliquer lors de la recherche. prompt_type: Type de prompt \u00e0 utiliser ('standard', 'summary', 'comparison'). generation_kwargs: Param\u00e8tres additionnels pour la g\u00e9n\u00e9ration de texte. enable_thinking: Active ou d\u00e9sactive le mode de r\u00e9flexion. Si None, utilise la configuration du mod\u00e8le. **prompt_kwargs: Param\u00e8tres additionnels pour le template de prompt. Returns: tuple: - Si enable_thinking=True : (thinking, response, search_results) - Si enable_thinking=False : (response, search_results) \"\"\" ... But : r\u00e9cup\u00e9rer les documents pertinents et g\u00e9n\u00e9rer une r\u00e9ponse compl\u00e8te Arguments : query : question de l'utilisateur filters : filtres \u00e0 appliquer lors de la recherche prompt_type : type de prompt \u00e0 utiliser generation_kwargs : param\u00e8tres pour la g\u00e9n\u00e9ration de texte enable_thinking : active le mode de r\u00e9flexion du mod\u00e8le **prompt_kwargs : param\u00e8tres additionnels pour le template de prompt Fonctionnement : R\u00e9cup\u00e8re les documents pertinents via retrieve_documents Formate le contexte documentaire avec format_context Cr\u00e9e le prompt avec le template appropri\u00e9 G\u00e9n\u00e8re la r\u00e9ponse avec le mod\u00e8le LLM Retour : Avec enable_thinking=True : tuple (thinking, response, search_results) Avec enable_thinking=False : tuple (response, search_results)","title":"M\u00e9thode retrieve_and_generate"},{"location":"lib/askai/rag_lib/#methode-retrieve_and_generate_stream","text":"async def retrieve_and_generate_stream ( self , query : str , filters : Dict [ str , Any ] = None , prompt_type : str = \"standard\" , generation_kwargs : Dict [ str , Any ] = None , enable_thinking : Optional [ bool ] = None , ** prompt_kwargs ) -> AsyncGenerator [ Dict [ str , Any ], None ]: \"\"\"R\u00e9cup\u00e8re les documents pertinents et g\u00e9n\u00e8re une r\u00e9ponse en streaming. Cette m\u00e9thode enrichit la r\u00e9ponse avec les documents utilis\u00e9s pour la g\u00e9n\u00e9ration. Chaque fragment retourn\u00e9 est un dictionnaire identifiant son type et contenu. Args: query: Question de l'utilisateur. filters: Filtres \u00e0 appliquer lors de la recherche. prompt_type: Type de prompt \u00e0 utiliser ('standard', 'summary', 'comparison'). generation_kwargs: Param\u00e8tres additionnels pour la g\u00e9n\u00e9ration de texte. enable_thinking: Active ou d\u00e9sactive le mode de r\u00e9flexion. Si None, utilise la configuration du mod\u00e8le. **prompt_kwargs: Param\u00e8tres additionnels pour le template de prompt. Yields: Dict[str, Any]: Fragments de la r\u00e9ponse ou m\u00e9tadonn\u00e9es avec leur type : - {\"type\": \"thinking\", \"content\": str} pour les parties de r\u00e9flexion - {\"type\": \"response\", \"content\": str} pour les parties de r\u00e9ponse - {\"type\": \"context\", \"content\": Dict} pour le contexte utilis\u00e9 - {\"type\": \"error\", \"content\": str} en cas d'erreur - {\"type\": \"done\", \"content\": \"\"} \u00e0 la fin du streaming \"\"\" ... But : r\u00e9cup\u00e9rer les documents et g\u00e9n\u00e9rer une r\u00e9ponse en streaming (progressive) Arguments : identiques \u00e0 retrieve_and_generate Fonctionnement : R\u00e9cup\u00e8re et formate les documents comme retrieve_and_generate Transmet imm\u00e9diatement le contexte au client ( {\"type\": \"context\", \"content\": search_results.dict()} ) Utilise AsyncStreamedResponse pour g\u00e9n\u00e9rer la r\u00e9ponse progressivement \u00c9met chaque fragment selon son type (r\u00e9ponse, r\u00e9flexion, contexte) Signale la fin du streaming avec un \u00e9v\u00e9nement {\"type\": \"done\"} Retour : g\u00e9n\u00e9rateur asynchrone de fragments typ\u00e9s","title":"M\u00e9thode retrieve_and_generate_stream"},{"location":"lib/askai/rag_lib/#classe-modelloader","text":"","title":"Classe ModelLoader"},{"location":"lib/askai/rag_lib/#constructeur_1","text":"loader = ModelLoader ( model_name : str = \"Qwen/Qwen3-0.6B\" , device : Optional [ str ] = None , load_in_8bit : bool = False , base_path : str = \"askai/models\" , thinking_enabled : bool = True , auto_load : bool = False , test_mode : bool = False , auto_fix : bool = True ) But : g\u00e9rer le chargement et la configuration des mod\u00e8les LLM Arguments : model_name : nom du mod\u00e8le \u00e0 charger (Qwen/Qwen3-0.6B, Qwen/Qwen3-1.7B) device : p\u00e9riph\u00e9rique de calcul ('cpu', 'cuda', 'auto') load_in_8bit : active la quantification 8-bit base_path : chemin vers le r\u00e9pertoire des mod\u00e8les thinking_enabled : active le mode de r\u00e9flexion par d\u00e9faut auto_load : charge automatiquement le mod\u00e8le \u00e0 l'initialisation test_mode : active un mode de simulation sans charger de mod\u00e8le","title":"Constructeur"},{"location":"lib/askai/rag_lib/#methode-load","text":"def load ( self ) -> None : \"\"\"Charge le mod\u00e8le et le tokenizer en m\u00e9moire. V\u00e9rifie si le mode test est activ\u00e9, puis charge le tokenizer et le mod\u00e8le depuis HuggingFace ou en local, et configure les options d'optimisation. Raises: RuntimeError: Si le chargement \u00e9choue. \"\"\" ... But : charger le mod\u00e8le et le tokenizer en m\u00e9moire Fonctionnement : V\u00e9rifie si le mode test est activ\u00e9 Charge le tokenizer et le mod\u00e8le depuis HuggingFace ou local Configure les options de quantification et d'optimisation Raises : RuntimeError si le chargement \u00e9choue","title":"M\u00e9thode load"},{"location":"lib/askai/rag_lib/#methode-generate","text":"def generate ( self , prompt : str , enable_thinking : bool = True , max_new_tokens : int = 2048 , do_sample : bool = True , temperature : float = 0.6 , top_p : float = 0.95 , top_k : int = 20 , ** kwargs ) -> str : \"\"\"G\u00e9n\u00e8re du texte \u00e0 partir d'un prompt donn\u00e9. Args: prompt: Texte d'entr\u00e9e pour la g\u00e9n\u00e9ration. enable_thinking: Active le mode de r\u00e9flexion. max_new_tokens: Nombre maximum de tokens \u00e0 g\u00e9n\u00e9rer. do_sample: Utilise l'\u00e9chantillonnage pour la g\u00e9n\u00e9ration. temperature: Contr\u00f4le la cr\u00e9ativit\u00e9 (plus \u00e9lev\u00e9 = plus al\u00e9atoire). top_p: Filtrage nucleus sampling. top_k: Nombre de tokens consid\u00e9r\u00e9s \u00e0 chaque \u00e9tape. **kwargs: Param\u00e8tres additionnels pour la g\u00e9n\u00e9ration. Returns: str: Texte g\u00e9n\u00e9r\u00e9 par le mod\u00e8le. \"\"\" ... But : g\u00e9n\u00e9rer du texte \u00e0 partir d'un prompt donn\u00e9 Arguments : prompt : texte d'entr\u00e9e pour la g\u00e9n\u00e9ration enable_thinking : active le mode de r\u00e9flexion max_new_tokens : nombre maximum de tokens \u00e0 g\u00e9n\u00e9rer do_sample : utilise l'\u00e9chantillonnage pour la g\u00e9n\u00e9ration temperature : contr\u00f4le la cr\u00e9ativit\u00e9 (plus \u00e9lev\u00e9 = plus al\u00e9atoire) top_p : filtrage nucleus sampling top_k : nombre de tokens consid\u00e9r\u00e9s \u00e0 chaque \u00e9tape Retour : texte g\u00e9n\u00e9r\u00e9 par le mod\u00e8le","title":"M\u00e9thode generate"},{"location":"lib/askai/rag_lib/#methode-generate_with_thinking","text":"def generate_with_thinking ( self , prompt : str , max_new_tokens : int = 2048 , do_sample : bool = True , temperature : float = 0.6 , ** kwargs ) -> Tuple [ str , str ]: \"\"\"G\u00e9n\u00e8re du texte avec s\u00e9paration explicite entre r\u00e9flexion et r\u00e9ponse. Cette m\u00e9thode capture le raisonnement \u00e9tape par \u00e9tape du mod\u00e8le et le s\u00e9pare de la r\u00e9ponse finale gr\u00e2ce \u00e0 des balises sp\u00e9cifiques dans le prompt. Args: prompt: Texte d'entr\u00e9e pour la g\u00e9n\u00e9ration. max_new_tokens: Nombre maximum de tokens \u00e0 g\u00e9n\u00e9rer. do_sample: Utilise l'\u00e9chantillonnage pour la g\u00e9n\u00e9ration. temperature: Contr\u00f4le la cr\u00e9ativit\u00e9 (plus \u00e9lev\u00e9 = plus al\u00e9atoire). **kwargs: Param\u00e8tres additionnels pour la g\u00e9n\u00e9ration. Returns: Tuple[str, str]: Tuple contenant (r\u00e9flexion, r\u00e9ponse). \"\"\" ... But : g\u00e9n\u00e9rer du texte avec s\u00e9paration entre r\u00e9flexion et r\u00e9ponse finale Arguments : similaires \u00e0 generate Fonctionnement : Ajoute des instructions au mod\u00e8le pour s\u00e9parer r\u00e9flexion et r\u00e9ponse Utilise des balises sp\u00e9ciales ( <thinking> , </thinking> , <answer> ) dans le prompt Extrait et s\u00e9pare la partie r\u00e9flexion de la partie r\u00e9ponse Retour : tuple ( thinking , answer ) contenant le raisonnement et la r\u00e9ponse","title":"M\u00e9thode generate_with_thinking"},{"location":"lib/askai/rag_lib/#classe-asyncstreamedresponse","text":"","title":"Classe AsyncStreamedResponse"},{"location":"lib/askai/rag_lib/#constructeur_2","text":"streamer = AsyncStreamedResponse ( model_loader : ModelLoader , filter_thinking : bool = False ) But : g\u00e9rer les r\u00e9ponses en streaming pour les interactions avec le LLM Arguments : model_loader : chargeur de mod\u00e8le LLM filter_thinking : si True, filtre le contenu de r\u00e9flexion des r\u00e9ponses","title":"Constructeur"},{"location":"lib/askai/rag_lib/#methode-generate_stream","text":"async def generate_stream ( self , prompt : str , enable_thinking : bool = True , chunk_size : int = 3 , ** kwargs ) -> AsyncGenerator [ Dict [ str , Any ], None ]: \"\"\"G\u00e9n\u00e8re une r\u00e9ponse en streaming progressif avec diff\u00e9renciation des types. Args: prompt: Texte d'entr\u00e9e pour la g\u00e9n\u00e9ration. enable_thinking: Active le mode de r\u00e9flexion. chunk_size: Nombre de mots par fragment en mode simulation. **kwargs: Param\u00e8tres additionnels pour la g\u00e9n\u00e9ration. Yields: Dict[str, Any]: Fragments typ\u00e9s de la g\u00e9n\u00e9ration: - {\"type\": \"thinking\", \"content\": str} pour les parties de r\u00e9flexion - {\"type\": \"response\", \"content\": str} pour les parties de r\u00e9ponse \"\"\" ... But : g\u00e9n\u00e9rer une r\u00e9ponse en streaming progressif avec diff\u00e9renciation des types Arguments : prompt : texte d'entr\u00e9e pour la g\u00e9n\u00e9ration enable_thinking : active le mode de r\u00e9flexion chunk_size : nombre de mots par fragment en mode simulation **kwargs : param\u00e8tres additionnels pour la g\u00e9n\u00e9ration Fonctionnement : D\u00e9tecte si le mod\u00e8le supporte nativement le streaming Si oui, utilise le streaming natif du mod\u00e8le Sinon, simule le streaming par d\u00e9coupage progressif Identifie et type chaque fragment (r\u00e9flexion ou r\u00e9ponse) Retour : g\u00e9n\u00e9rateur asynchrone de fragments typ\u00e9s","title":"M\u00e9thode generate_stream"},{"location":"lib/askai/rag_lib/#mode-thinking-contexte","text":"Le module RAG impl\u00e9mente une approche transparente qui permet aux utilisateurs d'acc\u00e9der: Au raisonnement complet du mod\u00e8le (mode \"thinking\") Au contexte documentaire utilis\u00e9 pour g\u00e9n\u00e9rer la r\u00e9ponse","title":"Mode Thinking &amp; Contexte"},{"location":"lib/askai/rag_lib/#raisonnement-thinking","text":"Le mode thinking permet de visualiser: Le processus de r\u00e9flexion \u00e9tape par \u00e9tape du mod\u00e8le L'analyse des documents fournis Le raisonnement pour arriver \u00e0 la conclusion Les r\u00e9f\u00e9rences explicites aux sources","title":"Raisonnement (Thinking)"},{"location":"lib/askai/rag_lib/#exemple-dactivation","text":"# Mode normal response , search_results = await rag_processor . retrieve_and_generate ( query = \"Comment r\u00e9duire les \u00e9missions de CO2?\" , enable_thinking = False ) # Mode avec raisonnement visible thinking , response , search_results = await rag_processor . retrieve_and_generate ( query = \"Comment r\u00e9duire les \u00e9missions de CO2?\" , enable_thinking = True ) print ( \"Raisonnement du mod\u00e8le:\" ) print ( thinking )","title":"Exemple d'activation:"},{"location":"lib/askai/rag_lib/#contexte-documentaire","text":"Le contexte documentaire permet \u00e0 l'utilisateur de: V\u00e9rifier les sources utilis\u00e9es pour g\u00e9n\u00e9rer la r\u00e9ponse \u00c9valuer la pertinence des documents r\u00e9cup\u00e9r\u00e9s Acc\u00e9der aux m\u00e9tadonn\u00e9es compl\u00e8tes (source, date, score) Explorer le contexte hi\u00e9rarchique (sections parentes)","title":"Contexte documentaire"},{"location":"lib/askai/rag_lib/#exploitation-du-contexte","text":"# R\u00e9cup\u00e9ration de la r\u00e9ponse et du contexte response , search_results = await rag_processor . retrieve_and_generate ( query = \"Quelles sont les r\u00e9glementations sur l'isolation thermique?\" , enable_thinking = False ) # Affichage des sources utilis\u00e9es print ( f \"R\u00e9ponse bas\u00e9e sur { len ( search_results . results ) } source(s):\" ) for i , result in enumerate ( search_results . results ): print ( f \" { i + 1 } . { result . title } (score: { result . score : .2f } )\" ) print ( f \" Th\u00e8me: { result . theme } , Type: { result . document_type } \" ) print ( f \" Date: { result . publish_date } \" ) print ( f \" Extrait: { result . content [: 100 ] } ...\" )","title":"Exploitation du contexte:"},{"location":"lib/askai/rag_lib/#streaming-avec-types-differencies","text":"En mode streaming, chaque fragment \u00e9mis est typ\u00e9 pour permettre: L'affichage diff\u00e9renci\u00e9 du raisonnement et de la r\u00e9ponse L'acc\u00e8s imm\u00e9diat au contexte d\u00e8s sa r\u00e9cup\u00e9ration La mise en forme adapt\u00e9e selon le type dans l'interface async for chunk in rag_processor . retrieve_and_generate_stream ( query = \"Expliquez les normes de s\u00e9curit\u00e9 incendie\" , enable_thinking = True ): if chunk [ \"type\" ] == \"context\" : # Afficher les sources dans l'interface display_sources ( chunk [ \"content\" ][ \"results\" ]) elif chunk [ \"type\" ] == \"thinking\" : # Afficher en italique gris dans une zone d\u00e9di\u00e9e append_to_thinking_area ( chunk [ \"content\" ]) elif chunk [ \"type\" ] == \"response\" : # Afficher en texte normal dans la zone de r\u00e9ponse append_to_response_area ( chunk [ \"content\" ])","title":"Streaming avec types diff\u00e9renci\u00e9s"},{"location":"lib/askai/rag_lib/#exemple-dutilisation","text":"import asyncio from sqlalchemy import create_engine from sqlalchemy.orm import sessionmaker from vectordb.src.search import SearchEngine from askai.src.model_loader import ModelLoader from askai.src.rag import RAGProcessor # Fonction asynchrone principale async def main (): # 1. Pr\u00e9parer la session de base de donn\u00e9es engine_db = create_engine ( \"postgresql://user:password@localhost/clea\" ) SessionLocal = sessionmaker ( bind = engine_db ) db = SessionLocal () # 2. Initialiser le moteur de recherche search_engine = SearchEngine () # 3. Initialiser le chargeur de mod\u00e8le (en mode test pour la d\u00e9mo) model_loader = ModelLoader ( model_name = \"Qwen/Qwen3-0.6B\" , test_mode = True , # Remplacer par False en production thinking_enabled = True ) # 4. Cr\u00e9er le processeur RAG rag_processor = RAGProcessor ( model_loader = model_loader , search_engine = search_engine , db_session = db , max_docs = 5 ) # 5. Exemple de requ\u00eate avec g\u00e9n\u00e9ration compl\u00e8te et acc\u00e8s au raisonnement query = \"Comment r\u00e9duire l'empreinte carbone d'une entreprise industrielle?\" filters = { \"theme\" : \"RSE\" , \"normalize_scores\" : True } thinking , response , search_results = await rag_processor . retrieve_and_generate ( query = query , filters = filters , prompt_type = \"standard\" , enable_thinking = True ) # Afficher la r\u00e9ponse print ( f \"R\u00e9ponse finale: \\n { response } \" ) # Afficher le raisonnement si n\u00e9cessaire print ( \" \\n Raisonnement du mod\u00e8le:\" ) print ( thinking ) # Afficher les sources utilis\u00e9es print ( \" \\n Sources utilis\u00e9es:\" ) for i , result in enumerate ( search_results . results ): print ( f \" { i + 1 } . { result . title } (score: { result . score : .2f } )\" ) # 6. Exemple de requ\u00eate avec g\u00e9n\u00e9ration en streaming typ\u00e9 print ( \" \\n G\u00e9n\u00e9ration en streaming avec types:\" ) # Conteneurs pour collecter diff\u00e9rents types de contenu thinking_content = [] response_content = [] context = None async for chunk in rag_processor . retrieve_and_generate_stream ( query = \"Quelles sont les meilleures pratiques de gestion des d\u00e9chets?\" , filters = { \"theme\" : \"Environnement\" }, enable_thinking = True ): if chunk [ \"type\" ] == \"thinking\" : thinking_content . append ( chunk [ \"content\" ]) print ( \"[Thinking] \" , chunk [ \"content\" ], end = \"\" ) elif chunk [ \"type\" ] == \"response\" : response_content . append ( chunk [ \"content\" ]) print ( \"[R\u00e9ponse] \" , chunk [ \"content\" ], end = \"\" ) elif chunk [ \"type\" ] == \"context\" : context = chunk [ \"content\" ] print ( f \" \\n [Contexte r\u00e9cup\u00e9r\u00e9: { len ( context [ 'results' ]) } documents]\" ) elif chunk [ \"type\" ] == \"done\" : print ( \" \\n [G\u00e9n\u00e9ration termin\u00e9e]\" ) # Ex\u00e9cuter la fonction asynchrone if __name__ == \"__main__\" : asyncio . run ( main ())","title":"Exemple d'utilisation"},{"location":"lib/askai/rag_lib/#exemple-avec-interface-fastapi","text":"from fastapi import FastAPI , Depends , HTTPException , BackgroundTasks , WebSocket from fastapi.responses import StreamingResponse from sqlalchemy.orm import Session from typing import Dict , Optional , List import json from askai.src.model_loader import ModelLoader from askai.src.rag import RAGProcessor from vectordb.src.search import SearchEngine from vectordb.src.database import get_db app = FastAPI () # Singleton pour le mod\u00e8le (partag\u00e9 entre requ\u00eates) model_loader = ModelLoader ( model_name = \"Qwen/Qwen3-0.6B\" , auto_load = True , # Charge le mod\u00e8le au d\u00e9marrage thinking_enabled = True ) search_engine = SearchEngine () @app . post ( \"/askai/query\" ) async def query ( question : str , theme : Optional [ str ] = None , enable_thinking : bool = False , db : Session = Depends ( get_db ) ): # Cr\u00e9er un processeur RAG pour cette requ\u00eate rag_processor = RAGProcessor ( model_loader = model_loader , search_engine = search_engine , db_session = db ) # G\u00e9n\u00e9rer la r\u00e9ponse avec ou sans raisonnement if enable_thinking : thinking , response , search_results = await rag_processor . retrieve_and_generate ( query = question , filters = { \"theme\" : theme } if theme else {}, enable_thinking = True ) # Retourner \u00e0 la fois la r\u00e9ponse, le raisonnement et les sources return { \"question\" : question , \"answer\" : response , \"thinking\" : thinking , \"sources\" : [ { \"title\" : result . title , \"theme\" : result . theme , \"document_type\" : result . document_type , \"publish_date\" : result . publish_date , \"score\" : result . score } for result in search_results . results ] } else : # Version sans raisonnement response , search_results = await rag_processor . retrieve_and_generate ( query = question , filters = { \"theme\" : theme } if theme else {}, enable_thinking = False ) return { \"question\" : question , \"answer\" : response , \"sources\" : [ { \"title\" : result . title , \"score\" : result . score } for result in search_results . results ] } @app . websocket ( \"/askai/ws\" ) async def websocket_endpoint ( websocket : WebSocket , db : Session = Depends ( get_db )): await websocket . accept () # Recevoir la requ\u00eate initiale data = await websocket . receive_text () request = json . loads ( data ) # Cr\u00e9er un processeur RAG rag_processor = RAGProcessor ( model_loader = model_loader , search_engine = search_engine , db_session = db ) # Transmettre les fragments typ\u00e9s via le WebSocket async for chunk in rag_processor . retrieve_and_generate_stream ( query = request [ \"question\" ], filters = { \"theme\" : request . get ( \"theme\" )} if \"theme\" in request else {}, enable_thinking = request . get ( \"enable_thinking\" , False ) ): await websocket . send_json ( chunk ) Voir aussi : les endpoints FastAPI dans askai_endpoint.py \u2013 POST /askai/query \u2192 g\u00e9n\u00e8re une r\u00e9ponse compl\u00e8te avec raisonnement et contexte optionnels","title":"Exemple avec interface FastAPI"},{"location":"lib/doc_loader/extractor_lib/","text":"Librairie doc_loader (Extraction de documents) Ce module fournit une abstraction et des impl\u00e9mentations concr\u00e8tes pour charger et d\u00e9couper des documents de diff\u00e9rents formats en DocumentWithChunks , pr\u00eat \u00e0 \u00eatre inject\u00e9 dans la base de donn\u00e9es via l\u2019API vectordb . Installation La librairie doc_loader fait partie du package clea-doc-loader . Elle se charge automatiquement des d\u00e9pendances via votre pyproject.toml / requirements.txt . # depuis la racine du projet uv install . ```` --- ## Structure du package doc_loader/ \u251c\u2500\u2500 extractor_factory.py # S\u00e9lection de l\u2019extracteur selon l\u2019extension \u251c\u2500\u2500 base.py # Interface et helpers communs \u251c\u2500\u2500 docs_loader.py # Point d\u2019entr\u00e9e : DocsLoader \u2514\u2500\u2500 data_extractor/ # 5 extracteurs concrets \u251c\u2500\u2500 txt_extractor.py # .txt \u251c\u2500\u2500 json_extractor.py # .json \u251c\u2500\u2500 docx_extractor.py # .docx \u251c\u2500\u2500 html_extractor.py # .html \u2514\u2500\u2500 pdf_extractor.py # .pdf --- ## 1. Interface commune ### `BaseExtractor` (`base.py`)&#x20; ```python class BaseExtractor(ABC): def __init__(self, file_path: str) -> None: \"\"\"Chemin vers le fichier \u00e0 traiter.\"\"\" self.file_path = Path(file_path) @abstractmethod def extract_one(self, *, max_length: int = 1000) -> DocumentWithChunks: \"\"\" Extrait l\u2019ensemble du document en un seul objet `DocumentWithChunks`. Args: max_length: taille cible des chunks finaux. Returns: DocumentWithChunks(document: DocumentCreate, chunks: List[ChunkCreate]) \"\"\" 2. Constructeur de payloads build_document_with_chunks(...) ( base.py ) Cette fonction choisit automatiquement entre : Mini-document (un seul chunk si len(full_text) \u2264 max_length ), Segmentation s\u00e9mantique (via NLP, _semantic_segmentation ), Fallback (d\u00e9coupage fixe + overlap). doc_with_chunks = build_document_with_chunks ( title = \"Rapport 2024\" , theme = \"RSE\" , document_type = \"PDF\" , publish_date = date . today (), max_length = 1000 , full_text = \"... texte complet ...\" ) # \u2192 DocumentWithChunks(document=DocumentCreate(...), # chunks=[ChunkCreate(...), ...]) 3. S\u00e9lection de l\u2019extracteur get_extractor(file_path: str) \u2192 BaseExtractor ( extractor_factory.py ) ext = get_extractor ( \"/chemin/vers/fichier.docx\" ) # ext est une instance de DocxExtractor, PdfExtractor, JsonExtractor, HtmlExtractor ou TxtExtractor. L\u00e8ve UnsupportedFileTypeError pour les extensions non list\u00e9es. 4. Point d\u2019entr\u00e9e : DocsLoader DocsLoader ( docs_loader.py ) loader = DocsLoader ( \"/chemin/fichier.txt\" ) doc_with_chunks = loader . extract_documents ( max_length = 1200 ) # renvoie un DocumentWithChunks unique extract_documents(...) d\u00e9l\u00e8gue \u00e0 extract_one() de l\u2019extracteur choisi. 5. Exemple d\u00e9taill\u00e9 : TxtExtractor TxtExtractor ( txt_extractor.py ) class TxtExtractor ( BaseExtractor ): def extract_one ( self , max_length : int = 1000 ) -> DocumentWithChunks : # 1) lit tout le fichier # 2) d\u00e9tecte si c\u2019est un JSON list\u00e9 \u2192 extrait m\u00e9tadonn\u00e9es + contenu # 3) sinon, m\u00e9tadonn\u00e9es par d\u00e9faut (stem, \"G\u00e9n\u00e9rique\", date.today()) # 4) appelle build_document_with_chunks(...) G\u00e8re automatiquement : Fichiers TXT \u00ab bruts \u00bb Fichiers TXT au format JSON [{\"title\":\u2026, \"theme\":\u2026, \"publish_date\":\u2026, \"content\":\u2026}, \u2026] 6. Autres extracteurs Chaque extracteur h\u00e9rite de BaseExtractor et impl\u00e9mente extract_one(...) de mani\u00e8re similaire, en utilisant : DocxExtractor ( .docx ) \u2192 segmentation par paragraphes et m\u00e9tadonn\u00e9es Office \ufe52 PdfExtractor ( .pdf ) \u2192 lecture pypdf , segmentation stream ou adaptive \ufe52 HtmlExtractor ( .html ) \u2192 BeautifulSoup , get_text() , segmentation \ufe52 JsonExtractor ( .json ) \u2192 parse JSON, extrait entries et segmente le contenu. Vous retrouverez la logique sp\u00e9cifique dans data_extractor/{docx,json,html,pdf}_extractor.py . 7. Usage typique from doc_loader.docs_loader import DocsLoader # 1. Choix de l\u2019extracteur et extraction loader = DocsLoader ( \"mon_fichier.pdf\" ) doc_payload = loader . extract_documents ( max_length = 800 ) # 2. Insertion en base (via vectordb) from vectordb.src.database import get_db , add_document_with_chunks db = next ( get_db ()) result = add_document_with_chunks ( db , doc_payload . document , doc_payload . chunks ) Module doc_loader stable \u2013 derni\u00e8re mise \u00e0 jour : 02 mai 2025 .","title":"Extracteurs"},{"location":"lib/doc_loader/extractor_lib/#librairie-doc_loader-extraction-de-documents","text":"Ce module fournit une abstraction et des impl\u00e9mentations concr\u00e8tes pour charger et d\u00e9couper des documents de diff\u00e9rents formats en DocumentWithChunks , pr\u00eat \u00e0 \u00eatre inject\u00e9 dans la base de donn\u00e9es via l\u2019API vectordb .","title":"Librairie doc_loader (Extraction de documents)"},{"location":"lib/doc_loader/extractor_lib/#installation","text":"La librairie doc_loader fait partie du package clea-doc-loader . Elle se charge automatiquement des d\u00e9pendances via votre pyproject.toml / requirements.txt . # depuis la racine du projet uv install . ```` --- ## Structure du package doc_loader/ \u251c\u2500\u2500 extractor_factory.py # S\u00e9lection de l\u2019extracteur selon l\u2019extension \u251c\u2500\u2500 base.py # Interface et helpers communs \u251c\u2500\u2500 docs_loader.py # Point d\u2019entr\u00e9e : DocsLoader \u2514\u2500\u2500 data_extractor/ # 5 extracteurs concrets \u251c\u2500\u2500 txt_extractor.py # .txt \u251c\u2500\u2500 json_extractor.py # .json \u251c\u2500\u2500 docx_extractor.py # .docx \u251c\u2500\u2500 html_extractor.py # .html \u2514\u2500\u2500 pdf_extractor.py # .pdf --- ## 1. Interface commune ### `BaseExtractor` (`base.py`)&#x20; ```python class BaseExtractor(ABC): def __init__(self, file_path: str) -> None: \"\"\"Chemin vers le fichier \u00e0 traiter.\"\"\" self.file_path = Path(file_path) @abstractmethod def extract_one(self, *, max_length: int = 1000) -> DocumentWithChunks: \"\"\" Extrait l\u2019ensemble du document en un seul objet `DocumentWithChunks`. Args: max_length: taille cible des chunks finaux. Returns: DocumentWithChunks(document: DocumentCreate, chunks: List[ChunkCreate]) \"\"\"","title":"Installation"},{"location":"lib/doc_loader/extractor_lib/#2-constructeur-de-payloads","text":"","title":"2. Constructeur de payloads"},{"location":"lib/doc_loader/extractor_lib/#build_document_with_chunks-basepy","text":"Cette fonction choisit automatiquement entre : Mini-document (un seul chunk si len(full_text) \u2264 max_length ), Segmentation s\u00e9mantique (via NLP, _semantic_segmentation ), Fallback (d\u00e9coupage fixe + overlap). doc_with_chunks = build_document_with_chunks ( title = \"Rapport 2024\" , theme = \"RSE\" , document_type = \"PDF\" , publish_date = date . today (), max_length = 1000 , full_text = \"... texte complet ...\" ) # \u2192 DocumentWithChunks(document=DocumentCreate(...), # chunks=[ChunkCreate(...), ...])","title":"build_document_with_chunks(...) (base.py)&#x20;"},{"location":"lib/doc_loader/extractor_lib/#3-selection-de-lextracteur","text":"","title":"3. S\u00e9lection de l\u2019extracteur"},{"location":"lib/doc_loader/extractor_lib/#get_extractorfile_path-str-baseextractor-extractor_factorypy","text":"ext = get_extractor ( \"/chemin/vers/fichier.docx\" ) # ext est une instance de DocxExtractor, PdfExtractor, JsonExtractor, HtmlExtractor ou TxtExtractor. L\u00e8ve UnsupportedFileTypeError pour les extensions non list\u00e9es.","title":"get_extractor(file_path: str) \u2192 BaseExtractor (extractor_factory.py)&#x20;"},{"location":"lib/doc_loader/extractor_lib/#4-point-dentree-docsloader","text":"","title":"4. Point d\u2019entr\u00e9e : DocsLoader"},{"location":"lib/doc_loader/extractor_lib/#docsloader-docs_loaderpy","text":"loader = DocsLoader ( \"/chemin/fichier.txt\" ) doc_with_chunks = loader . extract_documents ( max_length = 1200 ) # renvoie un DocumentWithChunks unique extract_documents(...) d\u00e9l\u00e8gue \u00e0 extract_one() de l\u2019extracteur choisi.","title":"DocsLoader (docs_loader.py)&#x20;"},{"location":"lib/doc_loader/extractor_lib/#5-exemple-detaille-txtextractor","text":"","title":"5. Exemple d\u00e9taill\u00e9 : TxtExtractor"},{"location":"lib/doc_loader/extractor_lib/#txtextractor-txt_extractorpy","text":"class TxtExtractor ( BaseExtractor ): def extract_one ( self , max_length : int = 1000 ) -> DocumentWithChunks : # 1) lit tout le fichier # 2) d\u00e9tecte si c\u2019est un JSON list\u00e9 \u2192 extrait m\u00e9tadonn\u00e9es + contenu # 3) sinon, m\u00e9tadonn\u00e9es par d\u00e9faut (stem, \"G\u00e9n\u00e9rique\", date.today()) # 4) appelle build_document_with_chunks(...) G\u00e8re automatiquement : Fichiers TXT \u00ab bruts \u00bb Fichiers TXT au format JSON [{\"title\":\u2026, \"theme\":\u2026, \"publish_date\":\u2026, \"content\":\u2026}, \u2026]","title":"TxtExtractor (txt_extractor.py)&#x20;"},{"location":"lib/doc_loader/extractor_lib/#6-autres-extracteurs","text":"Chaque extracteur h\u00e9rite de BaseExtractor et impl\u00e9mente extract_one(...) de mani\u00e8re similaire, en utilisant : DocxExtractor ( .docx ) \u2192 segmentation par paragraphes et m\u00e9tadonn\u00e9es Office \ufe52 PdfExtractor ( .pdf ) \u2192 lecture pypdf , segmentation stream ou adaptive \ufe52 HtmlExtractor ( .html ) \u2192 BeautifulSoup , get_text() , segmentation \ufe52 JsonExtractor ( .json ) \u2192 parse JSON, extrait entries et segmente le contenu. Vous retrouverez la logique sp\u00e9cifique dans data_extractor/{docx,json,html,pdf}_extractor.py .","title":"6. Autres extracteurs"},{"location":"lib/doc_loader/extractor_lib/#7-usage-typique","text":"from doc_loader.docs_loader import DocsLoader # 1. Choix de l\u2019extracteur et extraction loader = DocsLoader ( \"mon_fichier.pdf\" ) doc_payload = loader . extract_documents ( max_length = 800 ) # 2. Insertion en base (via vectordb) from vectordb.src.database import get_db , add_document_with_chunks db = next ( get_db ()) result = add_document_with_chunks ( db , doc_payload . document , doc_payload . chunks ) Module doc_loader stable \u2013 derni\u00e8re mise \u00e0 jour : 02 mai 2025 .","title":"7. Usage typique"},{"location":"lib/doc_loader/splitter_lib/","text":"Librairie splitter Ce module fournit des algorithmes de segmentation hi\u00e9rarchique et s\u00e9mantique de textes, avec des strat\u00e9gies de secours pour les corpus non structur\u00e9s. Il est con\u00e7u pour d\u00e9couper efficacement de grands documents en chunks exploitables par le pipeline d'indexation vectorielle. Table des mati\u00e8res Constantes globales Segmentation principale semantic_segmentation_stream _semantic_segmentation fallback_segmentation_stream _fallback_segmentation Extraction s\u00e9mantique _extract_semantic_sections _extract_semantic_paragraphs _create_semantic_chunks Utilitaires texte _get_meaningful_preview is_sentence_boundary find_paragraph_boundaries Exemple d'utilisation 1. Constantes globales D\u00e9finissent les param\u00e8tres limites pour le d\u00e9coupage et le mode de fonctionnement \"stream\" ou \"fallback\". Constante Valeur Description THRESHOLD_LARGE 5 000 000 Seuil (en octets) pour basculer en mode \"stream\" sur gros fichiers MAX_CHUNKS 5 000 Nombre max de chunks g\u00e9n\u00e9r\u00e9s MAX_TEXT_LENGTH 20 000 000 Longueur texte max support\u00e9e MAX_CHUNK_SIZE 8 000 Taille max d'un chunk (chars) MIN_LEVEL3_LENGTH 200 Seuil min pour chunks niveau 3 MAX_LEVEL3_CHUNKS 100 Nombre max de sous-chunks niveau 3 par paragraphe 2. Segmentation principale ( segmentation.py ) semantic_segmentation_stream(text: str, max_length: int) \u2192 Iterator[ChunkCreate] Description: R\u00e9alise un d\u00e9coupage s\u00e9mantique hi\u00e9rarchique en 4 niveaux (0-3) avec gestion optimis\u00e9e de la m\u00e9moire. Args: text: Texte source \u00e0 segmenter max_length: Longueur maximale souhait\u00e9e des chunks Yields: Objets ChunkCreate g\u00e9n\u00e9r\u00e9s s\u00e9quentiellement (mode streaming) \u00c9tapes: Chunk racine (niveau 0) Sections s\u00e9mantiques (niv. 1) via _extract_semantic_sections Paragraphes (niv. 2) via _extract_semantic_paragraphs Sous-chunks (niv. 3) via _create_semantic_chunks S\u00e9curit\u00e9: \u00c9vite les duplications S'arr\u00eate \u00e0 MAX_CHUNKS _semantic_segmentation(text: str, max_length: int) \u2192 List[ChunkCreate] Description: Retourne la liste compl\u00e8te des chunks en utilisant le g\u00e9n\u00e9rateur semantic_segmentation_stream . Args: text: Texte source \u00e0 segmenter max_length: Longueur maximale souhait\u00e9e des chunks Returns: Liste des chunks g\u00e9n\u00e9r\u00e9s, limit\u00e9e \u00e0 MAX_CHUNKS fallback_segmentation_stream(text: str, max_length: int) \u2192 Iterator[ChunkCreate] Description: M\u00e9thode de segmentation de secours robuste pour textes non structur\u00e9s. Args: text: Texte source \u00e0 segmenter max_length: Longueur maximale souhait\u00e9e des chunks Yields: Objets ChunkCreate g\u00e9n\u00e9r\u00e9s s\u00e9quentiellement Strat\u00e9gie: Chunk racine (aper\u00e7u) Segments glissants de taille min(max_length*2, MAX_CHUNK_SIZE) Tentatives de coupure naturelle (phrases, paragraphes) Chevauchement d'environ 10% _fallback_segmentation(text: str, max_length: int) \u2192 List[ChunkCreate] Description: Retourne la segmentation compl\u00e8te des chunks produits par fallback_segmentation_stream . Args: text: Texte source \u00e0 segmenter max_length: Longueur maximale souhait\u00e9e des chunks Returns: Liste des chunks g\u00e9n\u00e9r\u00e9s, limit\u00e9e \u00e0 MAX_CHUNKS 3. Extraction s\u00e9mantique ( text_analysis.py ) _extract_semantic_sections(text: str, max_sections: int = 20) \u2192 List[Dict] Description: Identifie les sections s\u00e9mantiques dans un texte. Args: text: Texte \u00e0 analyser max_sections: Nombre maximal de sections \u00e0 extraire Returns: Liste de dictionnaires [{title, content, start_char, end_char}, ...] M\u00e9thode: D\u00e9tecte titres formels (Markdown, soulign\u00e9s) puis s\u00e9parateurs naturels (sauts de ligne multiples) Si insuffisant, d\u00e9coupe artificiellement en blocs _extract_semantic_paragraphs(text: str, base_offset: int = 0, max_paragraphs: int = 20) \u2192 List[Dict] Description: Divise un texte en paragraphes s\u00e9mantiques. Args: text: Texte \u00e0 analyser base_offset: D\u00e9calage de caract\u00e8res \u00e0 appliquer dans le document original max_paragraphs: Nombre maximal de paragraphes \u00e0 extraire Returns: Liste de dictionnaires [{content, start_char, end_char}, ...] M\u00e9thode: S\u00e9pare sur \\n\\n Si trop peu de blocs, d\u00e9coupes par phrases ou artificiellement Regroupe petits blocs pour coh\u00e9rence _create_semantic_chunks(text: str, max_length: int, min_overlap: int = 50, base_offset: int = 0, max_chunks: int = 20) \u2192 List[Dict] Description: Cr\u00e9e des chunks s\u00e9mantiques \u00e0 partir d'un texte. Args: text: Texte \u00e0 d\u00e9couper max_length: Longueur maximale d'un chunk min_overlap: Chevauchement minimal entre chunks cons\u00e9cutifs base_offset: D\u00e9calage de caract\u00e8res \u00e0 appliquer dans le document original max_chunks: Nombre maximal de chunks \u00e0 g\u00e9n\u00e9rer Returns: Liste de dictionnaires [{content, start_char, end_char}, ...] M\u00e9thode: D\u00e9coupage sur fronti\u00e8res de phrases ou paragraphes Ajustement de effective_max et effective_overlap selon longueur du texte 4. Utilitaires texte ( text_utils.py ) _get_meaningful_preview(text: str, max_length: int) \u2192 str Description: Extrait un aper\u00e7u significatif d'un texte. Args: text: Texte source max_length: Longueur maximale de l'aper\u00e7u Returns: Aper\u00e7u combinant d\u00e9but, phrases cl\u00e9s et fin du texte M\u00e9thode: Extrait du d\u00e9but du texte S\u00e9lectionne phrases cl\u00e9s du milieu (contenant: \"essentiel\", \"cl\u00e9\", etc.) Inclut la fin du texte is_sentence_boundary(text: str, pos: int) \u2192 bool Description: V\u00e9rifie si une position donn\u00e9e correspond \u00e0 une fronti\u00e8re de phrase. Args: text: Texte \u00e0 analyser pos: Position \u00e0 v\u00e9rifier Returns: True si la position marque une fin de phrase, False sinon Crit\u00e8res: Pr\u00e9sence de . ! ? suivi d'un espace ou de la fin de cha\u00eene. find_paragraph_boundaries(text: str) \u2192 List[int] Description: Identifie les positions de d\u00e9but de chaque paragraphe. Args: text: Texte \u00e0 analyser Returns: Liste des indices de d\u00e9but de paragraphe M\u00e9thode: D\u00e9tection des s\u00e9parations de type \\n\\s*\\n . 5. Exemple d'utilisation from splitter.segmentation import semantic_segmentation_stream , fallback_segmentation_stream text = open ( \"mon_document.txt\" , \"r\" , encoding = \"utf-8\" ) . read () # 1. Segmentation s\u00e9mantique (recommand\u00e9e) chunks = list ( semantic_segmentation_stream ( text , max_length = 1000 )) print ( f \" { len ( chunks ) } chunks g\u00e9n\u00e9r\u00e9s (niveaux 0\u20133)\" ) # 2. Fallback si \u00e9chec ou corpus simple if len ( chunks ) == 1 : chunks = list ( fallback_segmentation_stream ( text , max_length = 800 )) print ( f \"Fallback : { len ( chunks ) } chunks g\u00e9n\u00e9r\u00e9s\" )","title":"Segmentation"},{"location":"lib/doc_loader/splitter_lib/#librairie-splitter","text":"Ce module fournit des algorithmes de segmentation hi\u00e9rarchique et s\u00e9mantique de textes, avec des strat\u00e9gies de secours pour les corpus non structur\u00e9s. Il est con\u00e7u pour d\u00e9couper efficacement de grands documents en chunks exploitables par le pipeline d'indexation vectorielle.","title":"Librairie splitter"},{"location":"lib/doc_loader/splitter_lib/#table-des-matieres","text":"Constantes globales Segmentation principale semantic_segmentation_stream _semantic_segmentation fallback_segmentation_stream _fallback_segmentation Extraction s\u00e9mantique _extract_semantic_sections _extract_semantic_paragraphs _create_semantic_chunks Utilitaires texte _get_meaningful_preview is_sentence_boundary find_paragraph_boundaries Exemple d'utilisation","title":"Table des mati\u00e8res"},{"location":"lib/doc_loader/splitter_lib/#1-constantes-globales","text":"D\u00e9finissent les param\u00e8tres limites pour le d\u00e9coupage et le mode de fonctionnement \"stream\" ou \"fallback\". Constante Valeur Description THRESHOLD_LARGE 5 000 000 Seuil (en octets) pour basculer en mode \"stream\" sur gros fichiers MAX_CHUNKS 5 000 Nombre max de chunks g\u00e9n\u00e9r\u00e9s MAX_TEXT_LENGTH 20 000 000 Longueur texte max support\u00e9e MAX_CHUNK_SIZE 8 000 Taille max d'un chunk (chars) MIN_LEVEL3_LENGTH 200 Seuil min pour chunks niveau 3 MAX_LEVEL3_CHUNKS 100 Nombre max de sous-chunks niveau 3 par paragraphe","title":"1. Constantes globales"},{"location":"lib/doc_loader/splitter_lib/#2-segmentation-principale-segmentationpy","text":"","title":"2. Segmentation principale (segmentation.py)"},{"location":"lib/doc_loader/splitter_lib/#semantic_segmentation_streamtext-str-max_length-int-iteratorchunkcreate","text":"Description: R\u00e9alise un d\u00e9coupage s\u00e9mantique hi\u00e9rarchique en 4 niveaux (0-3) avec gestion optimis\u00e9e de la m\u00e9moire. Args: text: Texte source \u00e0 segmenter max_length: Longueur maximale souhait\u00e9e des chunks Yields: Objets ChunkCreate g\u00e9n\u00e9r\u00e9s s\u00e9quentiellement (mode streaming) \u00c9tapes: Chunk racine (niveau 0) Sections s\u00e9mantiques (niv. 1) via _extract_semantic_sections Paragraphes (niv. 2) via _extract_semantic_paragraphs Sous-chunks (niv. 3) via _create_semantic_chunks S\u00e9curit\u00e9: \u00c9vite les duplications S'arr\u00eate \u00e0 MAX_CHUNKS","title":"semantic_segmentation_stream(text: str, max_length: int) \u2192 Iterator[ChunkCreate]"},{"location":"lib/doc_loader/splitter_lib/#_semantic_segmentationtext-str-max_length-int-listchunkcreate","text":"Description: Retourne la liste compl\u00e8te des chunks en utilisant le g\u00e9n\u00e9rateur semantic_segmentation_stream . Args: text: Texte source \u00e0 segmenter max_length: Longueur maximale souhait\u00e9e des chunks Returns: Liste des chunks g\u00e9n\u00e9r\u00e9s, limit\u00e9e \u00e0 MAX_CHUNKS","title":"_semantic_segmentation(text: str, max_length: int) \u2192 List[ChunkCreate]"},{"location":"lib/doc_loader/splitter_lib/#fallback_segmentation_streamtext-str-max_length-int-iteratorchunkcreate","text":"Description: M\u00e9thode de segmentation de secours robuste pour textes non structur\u00e9s. Args: text: Texte source \u00e0 segmenter max_length: Longueur maximale souhait\u00e9e des chunks Yields: Objets ChunkCreate g\u00e9n\u00e9r\u00e9s s\u00e9quentiellement Strat\u00e9gie: Chunk racine (aper\u00e7u) Segments glissants de taille min(max_length*2, MAX_CHUNK_SIZE) Tentatives de coupure naturelle (phrases, paragraphes) Chevauchement d'environ 10%","title":"fallback_segmentation_stream(text: str, max_length: int) \u2192 Iterator[ChunkCreate]"},{"location":"lib/doc_loader/splitter_lib/#_fallback_segmentationtext-str-max_length-int-listchunkcreate","text":"Description: Retourne la segmentation compl\u00e8te des chunks produits par fallback_segmentation_stream . Args: text: Texte source \u00e0 segmenter max_length: Longueur maximale souhait\u00e9e des chunks Returns: Liste des chunks g\u00e9n\u00e9r\u00e9s, limit\u00e9e \u00e0 MAX_CHUNKS","title":"_fallback_segmentation(text: str, max_length: int) \u2192 List[ChunkCreate]"},{"location":"lib/doc_loader/splitter_lib/#3-extraction-semantique-text_analysispy","text":"","title":"3. Extraction s\u00e9mantique (text_analysis.py)"},{"location":"lib/doc_loader/splitter_lib/#_extract_semantic_sectionstext-str-max_sections-int-20-listdict","text":"Description: Identifie les sections s\u00e9mantiques dans un texte. Args: text: Texte \u00e0 analyser max_sections: Nombre maximal de sections \u00e0 extraire Returns: Liste de dictionnaires [{title, content, start_char, end_char}, ...] M\u00e9thode: D\u00e9tecte titres formels (Markdown, soulign\u00e9s) puis s\u00e9parateurs naturels (sauts de ligne multiples) Si insuffisant, d\u00e9coupe artificiellement en blocs","title":"_extract_semantic_sections(text: str, max_sections: int = 20) \u2192 List[Dict]"},{"location":"lib/doc_loader/splitter_lib/#_extract_semantic_paragraphstext-str-base_offset-int-0-max_paragraphs-int-20-listdict","text":"Description: Divise un texte en paragraphes s\u00e9mantiques. Args: text: Texte \u00e0 analyser base_offset: D\u00e9calage de caract\u00e8res \u00e0 appliquer dans le document original max_paragraphs: Nombre maximal de paragraphes \u00e0 extraire Returns: Liste de dictionnaires [{content, start_char, end_char}, ...] M\u00e9thode: S\u00e9pare sur \\n\\n Si trop peu de blocs, d\u00e9coupes par phrases ou artificiellement Regroupe petits blocs pour coh\u00e9rence","title":"_extract_semantic_paragraphs(text: str, base_offset: int = 0, max_paragraphs: int = 20) \u2192 List[Dict]"},{"location":"lib/doc_loader/splitter_lib/#_create_semantic_chunkstext-str-max_length-int-min_overlap-int-50-base_offset-int-0-max_chunks-int-20-listdict","text":"Description: Cr\u00e9e des chunks s\u00e9mantiques \u00e0 partir d'un texte. Args: text: Texte \u00e0 d\u00e9couper max_length: Longueur maximale d'un chunk min_overlap: Chevauchement minimal entre chunks cons\u00e9cutifs base_offset: D\u00e9calage de caract\u00e8res \u00e0 appliquer dans le document original max_chunks: Nombre maximal de chunks \u00e0 g\u00e9n\u00e9rer Returns: Liste de dictionnaires [{content, start_char, end_char}, ...] M\u00e9thode: D\u00e9coupage sur fronti\u00e8res de phrases ou paragraphes Ajustement de effective_max et effective_overlap selon longueur du texte","title":"_create_semantic_chunks(text: str, max_length: int, min_overlap: int = 50, base_offset: int = 0, max_chunks: int = 20) \u2192 List[Dict]"},{"location":"lib/doc_loader/splitter_lib/#4-utilitaires-texte-text_utilspy","text":"","title":"4. Utilitaires texte (text_utils.py)"},{"location":"lib/doc_loader/splitter_lib/#_get_meaningful_previewtext-str-max_length-int-str","text":"Description: Extrait un aper\u00e7u significatif d'un texte. Args: text: Texte source max_length: Longueur maximale de l'aper\u00e7u Returns: Aper\u00e7u combinant d\u00e9but, phrases cl\u00e9s et fin du texte M\u00e9thode: Extrait du d\u00e9but du texte S\u00e9lectionne phrases cl\u00e9s du milieu (contenant: \"essentiel\", \"cl\u00e9\", etc.) Inclut la fin du texte","title":"_get_meaningful_preview(text: str, max_length: int) \u2192 str"},{"location":"lib/doc_loader/splitter_lib/#is_sentence_boundarytext-str-pos-int-bool","text":"Description: V\u00e9rifie si une position donn\u00e9e correspond \u00e0 une fronti\u00e8re de phrase. Args: text: Texte \u00e0 analyser pos: Position \u00e0 v\u00e9rifier Returns: True si la position marque une fin de phrase, False sinon Crit\u00e8res: Pr\u00e9sence de . ! ? suivi d'un espace ou de la fin de cha\u00eene.","title":"is_sentence_boundary(text: str, pos: int) \u2192 bool"},{"location":"lib/doc_loader/splitter_lib/#find_paragraph_boundariestext-str-listint","text":"Description: Identifie les positions de d\u00e9but de chaque paragraphe. Args: text: Texte \u00e0 analyser Returns: Liste des indices de d\u00e9but de paragraphe M\u00e9thode: D\u00e9tection des s\u00e9parations de type \\n\\s*\\n .","title":"find_paragraph_boundaries(text: str) \u2192 List[int]"},{"location":"lib/doc_loader/splitter_lib/#5-exemple-dutilisation","text":"from splitter.segmentation import semantic_segmentation_stream , fallback_segmentation_stream text = open ( \"mon_document.txt\" , \"r\" , encoding = \"utf-8\" ) . read () # 1. Segmentation s\u00e9mantique (recommand\u00e9e) chunks = list ( semantic_segmentation_stream ( text , max_length = 1000 )) print ( f \" { len ( chunks ) } chunks g\u00e9n\u00e9r\u00e9s (niveaux 0\u20133)\" ) # 2. Fallback si \u00e9chec ou corpus simple if len ( chunks ) == 1 : chunks = list ( fallback_segmentation_stream ( text , max_length = 800 )) print ( f \"Fallback : { len ( chunks ) } chunks g\u00e9n\u00e9r\u00e9s\" )","title":"5. Exemple d'utilisation"},{"location":"lib/pipeline/pipeline_lib/","text":"Module pipeline Orchestration du traitement de documents : extraction, segmentation et insertion en base. Installation Le module pipeline.py fait partie du package clea-pipeline . Pour l\u2019installer : pip install clea-pipeline ```` --- ## Fonctions principales ### `process_and_store`&#x20; ``` python def process_and_store ( file_path: str, max_length: int = 500 , overlap: int = 100 , theme: Optional [ str ] = \"Th\u00e8me g\u00e9n\u00e9rique\" , corpus_id: Optional [ str ] = None, ) -> Dict [ str, Any ] : Description V\u00e9rifie que le fichier existe (l\u00e8ve FileNotFoundError sinon). Extrait et segmente le document en chunks hi\u00e9rarchiques via DocsLoader . Applique le th\u00e8me si fourni. Ins\u00e8re le document et ses chunks en base via add_document_with_chunks . Retourne le r\u00e9sultat contenant : document_id : ID du document cr\u00e9\u00e9 chunks : nombre de chunks ins\u00e9r\u00e9s corpus_id : UUID du corpus create_index : bool\u00e9en indiquant si un index doit \u00eatre (re)cr\u00e9\u00e9 index_message : message d\u2019instruction pour la cr\u00e9ation d\u2019index (si applicable) Param\u00e8tres Nom Type Description file_path str Chemin vers le fichier \u00e0 traiter max_length int Taille max d\u2019un chunk final (d\u00e9faut 500) overlap int Chevauchement entre chunks (d\u00e9faut 100) theme Optional[str] Th\u00e8me \u00e0 appliquer (d\u00e9faut \"Th\u00e8me g\u00e9n\u00e9rique\") corpus_id Optional[str] UUID du corpus (g\u00e9n\u00e9r\u00e9 si None) Retour { \"document_id\" : 1 , \"chunks\" : 2 , \"corpus_id\" : \"e0428ce9-4a0a-445d-8f35-f5c9bed89c67\" , \"create_index\" : true , \"index_message\" : \"Un nouvel index pour le corpus \u2026 doit \u00eatre cr\u00e9\u00e9. Utilisez POST /database/indexes/{corpus_id}/create.\" } Exceptions FileNotFoundError : si file_path inexistant ValueError : si aucune extraction ou si l\u2019insertion \u00e9choue Exemple from pipeline import process_and_store result = process_and_store ( \"demo/report.pdf\" , max_length = 800 , overlap = 150 , theme = \"Finance\" , ) print ( result ) # { # \"document_id\": 10, # \"chunks\": 42, # \"corpus_id\": \"abcd-1234-\u2026\", # \"create_index\": true, # \"index_message\": \"\u2026\" # } determine_document_type def determine_document_type ( file_path : str ) -> str : Description D\u00e9duit le type du document ( PDF , TXT , WORD , etc.) \u00e0 partir de l\u2019extension du fichier. Param\u00e8tre Nom Type Description file_path str Chemin complet vers le fichier Retour Une des valeurs suivantes : PDF, TXT, MARKDOWN, WORD, HTML, XML, CSV, JSON, POWERPOINT, EXCEL, UNKNOWN Exemple >>> determine_document_type ( \"guide.docx\" ) \"WORD\" >>> determine_document_type ( \"notes.md\" ) \"MARKDOWN\" Utilisation typique from pipeline import process_and_store , determine_document_type # 1. D\u00e9terminer le type (facultatif) doc_type = determine_document_type ( \"report.pdf\" ) # 2. Traiter et stocker en base res = process_and_store ( \"report.pdf\" , max_length = 1000 , overlap = 200 , theme = \"RSE\" , ) print ( res ) Module : pipeline.py Derni\u00e8re mise \u00e0 jour : 02 mai 2025 Source : pipeline/src/pipeline.py","title":"Pipeline end-to-end"},{"location":"lib/pipeline/pipeline_lib/#module-pipeline","text":"Orchestration du traitement de documents : extraction, segmentation et insertion en base.","title":"Module pipeline"},{"location":"lib/pipeline/pipeline_lib/#installation","text":"Le module pipeline.py fait partie du package clea-pipeline . Pour l\u2019installer : pip install clea-pipeline ```` --- ## Fonctions principales ### `process_and_store`&#x20; ``` python def process_and_store ( file_path: str, max_length: int = 500 , overlap: int = 100 , theme: Optional [ str ] = \"Th\u00e8me g\u00e9n\u00e9rique\" , corpus_id: Optional [ str ] = None, ) -> Dict [ str, Any ] :","title":"Installation"},{"location":"lib/pipeline/pipeline_lib/#description","text":"V\u00e9rifie que le fichier existe (l\u00e8ve FileNotFoundError sinon). Extrait et segmente le document en chunks hi\u00e9rarchiques via DocsLoader . Applique le th\u00e8me si fourni. Ins\u00e8re le document et ses chunks en base via add_document_with_chunks . Retourne le r\u00e9sultat contenant : document_id : ID du document cr\u00e9\u00e9 chunks : nombre de chunks ins\u00e9r\u00e9s corpus_id : UUID du corpus create_index : bool\u00e9en indiquant si un index doit \u00eatre (re)cr\u00e9\u00e9 index_message : message d\u2019instruction pour la cr\u00e9ation d\u2019index (si applicable)","title":"Description"},{"location":"lib/pipeline/pipeline_lib/#parametres","text":"Nom Type Description file_path str Chemin vers le fichier \u00e0 traiter max_length int Taille max d\u2019un chunk final (d\u00e9faut 500) overlap int Chevauchement entre chunks (d\u00e9faut 100) theme Optional[str] Th\u00e8me \u00e0 appliquer (d\u00e9faut \"Th\u00e8me g\u00e9n\u00e9rique\") corpus_id Optional[str] UUID du corpus (g\u00e9n\u00e9r\u00e9 si None)","title":"Param\u00e8tres"},{"location":"lib/pipeline/pipeline_lib/#retour","text":"{ \"document_id\" : 1 , \"chunks\" : 2 , \"corpus_id\" : \"e0428ce9-4a0a-445d-8f35-f5c9bed89c67\" , \"create_index\" : true , \"index_message\" : \"Un nouvel index pour le corpus \u2026 doit \u00eatre cr\u00e9\u00e9. Utilisez POST /database/indexes/{corpus_id}/create.\" }","title":"Retour"},{"location":"lib/pipeline/pipeline_lib/#exceptions","text":"FileNotFoundError : si file_path inexistant ValueError : si aucune extraction ou si l\u2019insertion \u00e9choue","title":"Exceptions"},{"location":"lib/pipeline/pipeline_lib/#exemple","text":"from pipeline import process_and_store result = process_and_store ( \"demo/report.pdf\" , max_length = 800 , overlap = 150 , theme = \"Finance\" , ) print ( result ) # { # \"document_id\": 10, # \"chunks\": 42, # \"corpus_id\": \"abcd-1234-\u2026\", # \"create_index\": true, # \"index_message\": \"\u2026\" # }","title":"Exemple"},{"location":"lib/pipeline/pipeline_lib/#determine_document_type","text":"def determine_document_type ( file_path : str ) -> str :","title":"determine_document_type&#x20;"},{"location":"lib/pipeline/pipeline_lib/#description_1","text":"D\u00e9duit le type du document ( PDF , TXT , WORD , etc.) \u00e0 partir de l\u2019extension du fichier.","title":"Description"},{"location":"lib/pipeline/pipeline_lib/#parametre","text":"Nom Type Description file_path str Chemin complet vers le fichier","title":"Param\u00e8tre"},{"location":"lib/pipeline/pipeline_lib/#retour_1","text":"Une des valeurs suivantes : PDF, TXT, MARKDOWN, WORD, HTML, XML, CSV, JSON, POWERPOINT, EXCEL, UNKNOWN","title":"Retour"},{"location":"lib/pipeline/pipeline_lib/#exemple_1","text":">>> determine_document_type ( \"guide.docx\" ) \"WORD\" >>> determine_document_type ( \"notes.md\" ) \"MARKDOWN\"","title":"Exemple"},{"location":"lib/pipeline/pipeline_lib/#utilisation-typique","text":"from pipeline import process_and_store , determine_document_type # 1. D\u00e9terminer le type (facultatif) doc_type = determine_document_type ( \"report.pdf\" ) # 2. Traiter et stocker en base res = process_and_store ( \"report.pdf\" , max_length = 1000 , overlap = 200 , theme = \"RSE\" , ) print ( res ) Module : pipeline.py Derni\u00e8re mise \u00e0 jour : 02 mai 2025 Source : pipeline/src/pipeline.py","title":"Utilisation typique"},{"location":"lib/stats/stats_computer_lib/","text":"Librairie StatsComputer Cette librairie centralise le calcul de diverses statistiques pour le tableau de bord d\u2019administration de Clea-API, en s\u2019appuyant sur : Les sch\u00e9mas Pydantic d\u00e9finis dans stats_src_schemas.py La classe StatsComputer impl\u00e9ment\u00e9e dans stats_src_compute.py 1. Installation Assurez-vous que le paquet stats est install\u00e9 dans votre environnement Clea-API, et que vectordb est configur\u00e9 (PG + SQLAlchemy). pip install -e stats/ 2. Sch\u00e9mas de sortie Tous les mod\u00e8les Pydantic utilisent la configuration alias_generator pour produire des cl\u00e9s camelCase \u00e0 la sortie JSON. # config globale def _to_camel ( s : str ) -> str : head , * tail = s . split ( \"_\" ) return head + \"\" . join ( word . capitalize () for word in tail ) CamelConfig = { \"alias_generator\" : _to_camel , \"populate_by_name\" : True , } 2.1. DocumentStats Statistiques sur les documents stock\u00e9s. Attribut Type Description totalCount int Nombre total de documents analys\u00e9s byTheme Record<string,int> R\u00e9partition par th\u00e8me byType Record<string,int> R\u00e9partition par type de document recentlyAdded int Documents publi\u00e9s au cours des 30 derniers jours percentChange float % de documents r\u00e9cents par rapport au total 2.2. SearchStats Statistiques sur l\u2019historique des recherches des utilisateurs. Attribut Type Description totalCount int Nombre total de requ\u00eates enregistr\u00e9es lastMonthCount int Requ\u00eates effectu\u00e9es au cours des 30 derniers jours percentChange float % d\u2019\u00e9volution par rapport au mois pr\u00e9c\u00e9dent topQueries Array<{query:string, count:number}> Top 10 des requ\u00eates les plus populaires 2.3. SystemStats Indicateurs globaux de performance et d\u2019\u00e9tat. Attribut Type Description satisfaction float % de recherches jug\u00e9es satisfaisantes (confiance \u2265 0.7) avgConfidence float Confiance moyenne des recherches du dernier mois percentChange float % d\u2019\u00e9volution de la confiance par rapport au mois pr\u00e9c\u00e9dent indexedCorpora int Nombre de corpus/documents d\u00e9j\u00e0 index\u00e9s totalCorpora int Nombre total de corpus/documents dans la base 2.4. DashboardStats Regroupe l\u2019ensemble des stats en un seul objet : interface DashboardStats { documentStats : DocumentStats ; searchStats : SearchStats ; systemStats : SystemStats ; } 3. Classe StatsComputer La classe principale pour g\u00e9n\u00e9rer ces statistiques : from stats.src.stats_src_compute import StatsComputer stats = StatsComputer () 3.1. compute_document_stats(skip=0, limit=100) \u2192 DocumentStats Description : R\u00e9cup\u00e8re jusqu\u2019\u00e0 limit documents (apr\u00e8s avoir ignor\u00e9 skip ), puis calcule : totalCount , byTheme , byType recentlyAdded (\u2190 30 derniers jours) percentChange Usage : doc_stats = stats . compute_document_stats ( skip = 0 , limit = 500 ) print ( doc_stats . json ()) 3.2. compute_search_stats(skip=0, limit=100) \u2192 SearchStats Description : Analyse la table SearchQuery pour : totalCount , lastMonthCount percentChange (mois vs mois) topQueries (top 10 requ\u00eates) Usage : search_stats = stats . compute_search_stats () print ( search_stats . json ()) 3.3. compute_system_stats() \u2192 SystemStats Description : Calcul des indicateurs syst\u00e8me : Confiance moyenne ( avgConfidence ) et \u00e9volution ( percentChange ) Taux de satisfaction ( satisfaction ) indexedCorpora vs totalCorpora Usage : sys_stats = stats . compute_system_stats () print ( sys_stats . json ()) 3.4. compute_all_stats() \u2192 DashboardStats Description : Agr\u00e8ge les trois m\u00e9thodes pr\u00e9c\u00e9dentes et retourne un DashboardStats . Usage : dashboard = stats . compute_all_stats () # affiche un rapport JSON complet print ( dashboard . json ( indent = 2 )) 4. Bonnes pratiques Pagination : ajustez skip / limit pour ne pas surcharger la m\u00e9moire. Logging : la classe utilise le logger \"clea-api.stats\" pour tracer les erreurs. S\u00e9curit\u00e9 : si les calculs sont lourds, envisagez de les d\u00e9porter en t\u00e2che asynchrone. Fichiers source : \u2013 stats/src/stats_src_schemas.py \u2013 stats/src/stats_src_compute.py Derni\u00e8re mise \u00e0 jour : 05 mai 2025","title":"Recherche hybride"},{"location":"lib/stats/stats_computer_lib/#librairie-statscomputer","text":"Cette librairie centralise le calcul de diverses statistiques pour le tableau de bord d\u2019administration de Clea-API, en s\u2019appuyant sur : Les sch\u00e9mas Pydantic d\u00e9finis dans stats_src_schemas.py La classe StatsComputer impl\u00e9ment\u00e9e dans stats_src_compute.py","title":"Librairie StatsComputer"},{"location":"lib/stats/stats_computer_lib/#1-installation","text":"Assurez-vous que le paquet stats est install\u00e9 dans votre environnement Clea-API, et que vectordb est configur\u00e9 (PG + SQLAlchemy). pip install -e stats/","title":"1. Installation"},{"location":"lib/stats/stats_computer_lib/#2-schemas-de-sortie","text":"Tous les mod\u00e8les Pydantic utilisent la configuration alias_generator pour produire des cl\u00e9s camelCase \u00e0 la sortie JSON. # config globale def _to_camel ( s : str ) -> str : head , * tail = s . split ( \"_\" ) return head + \"\" . join ( word . capitalize () for word in tail ) CamelConfig = { \"alias_generator\" : _to_camel , \"populate_by_name\" : True , }","title":"2. Sch\u00e9mas de sortie"},{"location":"lib/stats/stats_computer_lib/#21-documentstats","text":"Statistiques sur les documents stock\u00e9s. Attribut Type Description totalCount int Nombre total de documents analys\u00e9s byTheme Record<string,int> R\u00e9partition par th\u00e8me byType Record<string,int> R\u00e9partition par type de document recentlyAdded int Documents publi\u00e9s au cours des 30 derniers jours percentChange float % de documents r\u00e9cents par rapport au total","title":"2.1. DocumentStats"},{"location":"lib/stats/stats_computer_lib/#22-searchstats","text":"Statistiques sur l\u2019historique des recherches des utilisateurs. Attribut Type Description totalCount int Nombre total de requ\u00eates enregistr\u00e9es lastMonthCount int Requ\u00eates effectu\u00e9es au cours des 30 derniers jours percentChange float % d\u2019\u00e9volution par rapport au mois pr\u00e9c\u00e9dent topQueries Array<{query:string, count:number}> Top 10 des requ\u00eates les plus populaires","title":"2.2. SearchStats"},{"location":"lib/stats/stats_computer_lib/#23-systemstats","text":"Indicateurs globaux de performance et d\u2019\u00e9tat. Attribut Type Description satisfaction float % de recherches jug\u00e9es satisfaisantes (confiance \u2265 0.7) avgConfidence float Confiance moyenne des recherches du dernier mois percentChange float % d\u2019\u00e9volution de la confiance par rapport au mois pr\u00e9c\u00e9dent indexedCorpora int Nombre de corpus/documents d\u00e9j\u00e0 index\u00e9s totalCorpora int Nombre total de corpus/documents dans la base","title":"2.3. SystemStats"},{"location":"lib/stats/stats_computer_lib/#24-dashboardstats","text":"Regroupe l\u2019ensemble des stats en un seul objet : interface DashboardStats { documentStats : DocumentStats ; searchStats : SearchStats ; systemStats : SystemStats ; }","title":"2.4. DashboardStats"},{"location":"lib/stats/stats_computer_lib/#3-classe-statscomputer","text":"La classe principale pour g\u00e9n\u00e9rer ces statistiques : from stats.src.stats_src_compute import StatsComputer stats = StatsComputer ()","title":"3. Classe StatsComputer"},{"location":"lib/stats/stats_computer_lib/#31-compute_document_statsskip0-limit100-documentstats","text":"Description : R\u00e9cup\u00e8re jusqu\u2019\u00e0 limit documents (apr\u00e8s avoir ignor\u00e9 skip ), puis calcule : totalCount , byTheme , byType recentlyAdded (\u2190 30 derniers jours) percentChange Usage : doc_stats = stats . compute_document_stats ( skip = 0 , limit = 500 ) print ( doc_stats . json ())","title":"3.1. compute_document_stats(skip=0, limit=100) \u2192 DocumentStats"},{"location":"lib/stats/stats_computer_lib/#32-compute_search_statsskip0-limit100-searchstats","text":"Description : Analyse la table SearchQuery pour : totalCount , lastMonthCount percentChange (mois vs mois) topQueries (top 10 requ\u00eates) Usage : search_stats = stats . compute_search_stats () print ( search_stats . json ())","title":"3.2. compute_search_stats(skip=0, limit=100) \u2192 SearchStats"},{"location":"lib/stats/stats_computer_lib/#33-compute_system_stats-systemstats","text":"Description : Calcul des indicateurs syst\u00e8me : Confiance moyenne ( avgConfidence ) et \u00e9volution ( percentChange ) Taux de satisfaction ( satisfaction ) indexedCorpora vs totalCorpora Usage : sys_stats = stats . compute_system_stats () print ( sys_stats . json ())","title":"3.3. compute_system_stats() \u2192 SystemStats"},{"location":"lib/stats/stats_computer_lib/#34-compute_all_stats-dashboardstats","text":"Description : Agr\u00e8ge les trois m\u00e9thodes pr\u00e9c\u00e9dentes et retourne un DashboardStats . Usage : dashboard = stats . compute_all_stats () # affiche un rapport JSON complet print ( dashboard . json ( indent = 2 ))","title":"3.4. compute_all_stats() \u2192 DashboardStats"},{"location":"lib/stats/stats_computer_lib/#4-bonnes-pratiques","text":"Pagination : ajustez skip / limit pour ne pas surcharger la m\u00e9moire. Logging : la classe utilise le logger \"clea-api.stats\" pour tracer les erreurs. S\u00e9curit\u00e9 : si les calculs sont lourds, envisagez de les d\u00e9porter en t\u00e2che asynchrone. Fichiers source : \u2013 stats/src/stats_src_schemas.py \u2013 stats/src/stats_src_compute.py Derni\u00e8re mise \u00e0 jour : 05 mai 2025","title":"4. Bonnes pratiques"},{"location":"lib/vectordb/crud_lib/","text":"Module crud Ce module fournit des op\u00e9rations CRUD de haut niveau sur les entit\u00e9s Document , Chunk et IndexConfig , avec gestion des embeddings et des index pgvector. Installation Ce module est inclus dans le package vectordb . Pour l\u2019installer : pip install vectordb ```` --- ## Table des mati\u00e8res 1 . [ add \\_ document \\_ with \\_ chunks ]( #add_document_with_chunks) 2 . [ update \\_ document \\_ with \\_ chunks ]( #update_document_with_chunks) 3 . [ delete \\_ document \\_ chunks ]( #delete_document_chunks) 4 . [ delete \\_ document ]( #delete_document) --- <a name = \"add_document_with_chunks\" ></a> ## 1. `add_document_with_chunks(db, doc, chunks, batch_size=10) \u2192 Dict[str, Any]` Ajoute un **document** et ses **chunks** en base, g\u00e9n\u00e8re les embeddings en lot, et g\u00e8re la configuration de l\u2019index. ``` python from sqlalchemy.orm import Session from vectordb.src.schemas import DocumentCreate from vectordb.src.crud import add_document_with_chunks result = add_document_with_chunks ( db: Session, doc: DocumentCreate, chunks: List [ dict ] , batch_size = 20 ) Description G\u00e9n\u00e8re corpus_id si manquant. Ins\u00e8re le document ( Document ) avec un flag index_needed . Pour chaque lot de batch_size chunks : Calcule les embeddings via EmbeddingGenerator.generate_embeddings_batch . Ins\u00e8re les objets Chunk (avec flush interm\u00e9diaires). Construit les relations parent\u2194enfant. Met \u00e0 jour ou cr\u00e9e la configuration d\u2019index ( IndexConfig ). Commit ou rollback en cas d\u2019erreur. Param\u00e8tres Nom Type Description db Session Session SQLAlchemy active. doc DocumentCreate M\u00e9tadonn\u00e9es du document \u00e0 cr\u00e9er. chunks List[dict] Liste de chunks { content, hierarchy_level, ... } . batch_size int (d\u00e9faut 10) Taille des sous-lots pour la g\u00e9n\u00e9ration d\u2019embeddings. Retour { \"document_id\" : i nt , \"chunks\" : i nt , \"corpus_id\" : s tr , \"index_needed\" : bool } index_needed = True si un nouvel index doit \u00eatre (re)cr\u00e9\u00e9. 2. update_document_with_chunks(document_update, new_chunks=None) \u2192 Dict[str, Any] Met \u00e0 jour un document existant et ajoute \u00e9ventuellement de nouveaux chunks. from vectordb.src.schemas import DocumentUpdate from vectordb.src.crud import update_document_with_chunks result = update_document_with_chunks ( document_update : DocumentUpdate , new_chunks : List [ dict ] # facultatif ) Description Charge le Document par son id . Met \u00e0 jour les champs fournis ( title , theme , etc.). Si new_chunks est fourni : Calcule les embeddings un par un. Ins\u00e8re en bulk via insert(Chunk) . Met \u00e0 jour le compteur chunk_count dans IndexConfig . Si corpus_id change, ajuste les compteurs sur les anciennes/nouvelles configurations d\u2019index. Retourne les m\u00e9tadonn\u00e9es mises \u00e0 jour et si index_needed suite \u00e0 un changement de corpus. Param\u00e8tres Nom Type Description document_update DocumentUpdate DTO avec l\u2019 id du document et champs modifi\u00e9s. new_chunks List[dict] (optionnel) Nouveaux chunks \u00e0 ajouter. Retour { \"id\" : i nt , \"title\" : s tr , \"theme\" : s tr , \"document_type\" : s tr , \"publish_date\" : da te , \"corpus_id\" : s tr , \"chunks\" : { \"total\" : i nt , \"added\" : i nt }, \"index_needed\" : bool } En cas de document introuvable : {\"error\": \"\u2026 introuvable.\"} 3. delete_document_chunks(document_id, chunk_ids=None) \u2192 Dict[str, Any] Supprime un ou plusieurs chunks d\u2019un document, ou tous si chunk_ids non fourni. from vectordb.src.crud import delete_document_chunks result = delete_document_chunks ( document_id : int , chunk_ids : Optional [ List [ int ]]) Description V\u00e9rifie l\u2019existence du Document . Si chunk_ids est une liste : Supprime uniquement ces chunks. Sinon : Supprime tous les chunks associ\u00e9s. Met \u00e0 jour chunk_count dans IndexConfig . Commit ou rollback en cas d\u2019erreur. Param\u00e8tres Nom Type Description document_id int Identifiant du document. chunk_ids List[int] (optionnel) Liste d\u2019IDs de chunks \u00e0 supprimer (None \u2192 tous). Retour { \"document_id\" : i nt , \"chunks_deleted\" : i nt , \"remaining_chunks\" : i nt } Remarque : si document introuvable \u2192 {\"error\": \"\u2026 introuvable.\"} 4. delete_document(document_id) \u2192 Dict[str, Any] Supprime un document et tous ses chunks en cascade. from vectordb.src.crud import delete_document result = delete_document ( document_id : int ) Description Charge le Document par id . Met \u00e0 jour chunk_count dans la configuration d\u2019index (diminue du nombre de chunks supprim\u00e9s). Supprime le document (cascade supprime les chunks). Commit ou rollback en cas d\u2019erreur. Param\u00e8tres Nom Type Description document_id int Identifiant du document \u00e0 supprimer. Retour En cas de succ\u00e8s : { \"success\" : \"Document avec ID X supprim\u00e9 avec succ\u00e8s.\" } * Si introuvable : { \"error\" : \"Document avec ID X introuvable.\" } ``` : co ntent Re feren ce [ oaici te : 8 ]{ i n dex= 8 }: co ntent Re feren ce [ oaici te : 9 ]{ i n dex= 9 } Module : vectordb/src/crud.py Derni\u00e8re mise \u00e0 jour : 02 mai 2025","title":"Op\u00e9rations CRUD"},{"location":"lib/vectordb/crud_lib/#module-crud","text":"Ce module fournit des op\u00e9rations CRUD de haut niveau sur les entit\u00e9s Document , Chunk et IndexConfig , avec gestion des embeddings et des index pgvector.","title":"Module crud"},{"location":"lib/vectordb/crud_lib/#installation","text":"Ce module est inclus dans le package vectordb . Pour l\u2019installer : pip install vectordb ```` --- ## Table des mati\u00e8res 1 . [ add \\_ document \\_ with \\_ chunks ]( #add_document_with_chunks) 2 . [ update \\_ document \\_ with \\_ chunks ]( #update_document_with_chunks) 3 . [ delete \\_ document \\_ chunks ]( #delete_document_chunks) 4 . [ delete \\_ document ]( #delete_document) --- <a name = \"add_document_with_chunks\" ></a> ## 1. `add_document_with_chunks(db, doc, chunks, batch_size=10) \u2192 Dict[str, Any]` Ajoute un **document** et ses **chunks** en base, g\u00e9n\u00e8re les embeddings en lot, et g\u00e8re la configuration de l\u2019index. ``` python from sqlalchemy.orm import Session from vectordb.src.schemas import DocumentCreate from vectordb.src.crud import add_document_with_chunks result = add_document_with_chunks ( db: Session, doc: DocumentCreate, chunks: List [ dict ] , batch_size = 20 )","title":"Installation"},{"location":"lib/vectordb/crud_lib/#description","text":"G\u00e9n\u00e8re corpus_id si manquant. Ins\u00e8re le document ( Document ) avec un flag index_needed . Pour chaque lot de batch_size chunks : Calcule les embeddings via EmbeddingGenerator.generate_embeddings_batch . Ins\u00e8re les objets Chunk (avec flush interm\u00e9diaires). Construit les relations parent\u2194enfant. Met \u00e0 jour ou cr\u00e9e la configuration d\u2019index ( IndexConfig ). Commit ou rollback en cas d\u2019erreur.","title":"Description"},{"location":"lib/vectordb/crud_lib/#parametres","text":"Nom Type Description db Session Session SQLAlchemy active. doc DocumentCreate M\u00e9tadonn\u00e9es du document \u00e0 cr\u00e9er. chunks List[dict] Liste de chunks { content, hierarchy_level, ... } . batch_size int (d\u00e9faut 10) Taille des sous-lots pour la g\u00e9n\u00e9ration d\u2019embeddings.","title":"Param\u00e8tres"},{"location":"lib/vectordb/crud_lib/#retour","text":"{ \"document_id\" : i nt , \"chunks\" : i nt , \"corpus_id\" : s tr , \"index_needed\" : bool } index_needed = True si un nouvel index doit \u00eatre (re)cr\u00e9\u00e9.","title":"Retour"},{"location":"lib/vectordb/crud_lib/#2-update_document_with_chunksdocument_update-new_chunksnone-dictstr-any","text":"Met \u00e0 jour un document existant et ajoute \u00e9ventuellement de nouveaux chunks. from vectordb.src.schemas import DocumentUpdate from vectordb.src.crud import update_document_with_chunks result = update_document_with_chunks ( document_update : DocumentUpdate , new_chunks : List [ dict ] # facultatif )","title":"2. update_document_with_chunks(document_update, new_chunks=None) \u2192 Dict[str, Any]"},{"location":"lib/vectordb/crud_lib/#description_1","text":"Charge le Document par son id . Met \u00e0 jour les champs fournis ( title , theme , etc.). Si new_chunks est fourni : Calcule les embeddings un par un. Ins\u00e8re en bulk via insert(Chunk) . Met \u00e0 jour le compteur chunk_count dans IndexConfig . Si corpus_id change, ajuste les compteurs sur les anciennes/nouvelles configurations d\u2019index. Retourne les m\u00e9tadonn\u00e9es mises \u00e0 jour et si index_needed suite \u00e0 un changement de corpus.","title":"Description"},{"location":"lib/vectordb/crud_lib/#parametres_1","text":"Nom Type Description document_update DocumentUpdate DTO avec l\u2019 id du document et champs modifi\u00e9s. new_chunks List[dict] (optionnel) Nouveaux chunks \u00e0 ajouter.","title":"Param\u00e8tres"},{"location":"lib/vectordb/crud_lib/#retour_1","text":"{ \"id\" : i nt , \"title\" : s tr , \"theme\" : s tr , \"document_type\" : s tr , \"publish_date\" : da te , \"corpus_id\" : s tr , \"chunks\" : { \"total\" : i nt , \"added\" : i nt }, \"index_needed\" : bool } En cas de document introuvable : {\"error\": \"\u2026 introuvable.\"}","title":"Retour"},{"location":"lib/vectordb/crud_lib/#3-delete_document_chunksdocument_id-chunk_idsnone-dictstr-any","text":"Supprime un ou plusieurs chunks d\u2019un document, ou tous si chunk_ids non fourni. from vectordb.src.crud import delete_document_chunks result = delete_document_chunks ( document_id : int , chunk_ids : Optional [ List [ int ]])","title":"3. delete_document_chunks(document_id, chunk_ids=None) \u2192 Dict[str, Any]"},{"location":"lib/vectordb/crud_lib/#description_2","text":"V\u00e9rifie l\u2019existence du Document . Si chunk_ids est une liste : Supprime uniquement ces chunks. Sinon : Supprime tous les chunks associ\u00e9s. Met \u00e0 jour chunk_count dans IndexConfig . Commit ou rollback en cas d\u2019erreur.","title":"Description"},{"location":"lib/vectordb/crud_lib/#parametres_2","text":"Nom Type Description document_id int Identifiant du document. chunk_ids List[int] (optionnel) Liste d\u2019IDs de chunks \u00e0 supprimer (None \u2192 tous).","title":"Param\u00e8tres"},{"location":"lib/vectordb/crud_lib/#retour_2","text":"{ \"document_id\" : i nt , \"chunks_deleted\" : i nt , \"remaining_chunks\" : i nt } Remarque : si document introuvable \u2192 {\"error\": \"\u2026 introuvable.\"}","title":"Retour"},{"location":"lib/vectordb/crud_lib/#4-delete_documentdocument_id-dictstr-any","text":"Supprime un document et tous ses chunks en cascade. from vectordb.src.crud import delete_document result = delete_document ( document_id : int )","title":"4. delete_document(document_id) \u2192 Dict[str, Any]"},{"location":"lib/vectordb/crud_lib/#description_3","text":"Charge le Document par id . Met \u00e0 jour chunk_count dans la configuration d\u2019index (diminue du nombre de chunks supprim\u00e9s). Supprime le document (cascade supprime les chunks). Commit ou rollback en cas d\u2019erreur.","title":"Description"},{"location":"lib/vectordb/crud_lib/#parametres_3","text":"Nom Type Description document_id int Identifiant du document \u00e0 supprimer.","title":"Param\u00e8tres"},{"location":"lib/vectordb/crud_lib/#retour_3","text":"En cas de succ\u00e8s : { \"success\" : \"Document avec ID X supprim\u00e9 avec succ\u00e8s.\" } * Si introuvable : { \"error\" : \"Document avec ID X introuvable.\" } ``` : co ntent Re feren ce [ oaici te : 8 ]{ i n dex= 8 }: co ntent Re feren ce [ oaici te : 9 ]{ i n dex= 9 } Module : vectordb/src/crud.py Derni\u00e8re mise \u00e0 jour : 02 mai 2025","title":"Retour"},{"location":"lib/vectordb/index_lib/","text":"Module index_manager Gestion des index vectoriels pour pgvector au sein de Cl\u00e9a-API . Ce module offre une API simple pour cr\u00e9er, supprimer et contr\u00f4ler l'\u00e9tat des index IVFFLAT associ\u00e9s aux chunks de documents. Installation Le module index_manager fait partie du package vectordb . Aucun paquet externe n'est n\u00e9cessaire, si ce n'est votre installation PostgreSQL/pgvector et SQLAlchemy. uv pip install vectordb Table des mati\u00e8res Cr\u00e9er un index simple Supprimer un index V\u00e9rifier l'\u00e9tat d'un index V\u00e9rifier tous les index Nettoyer les index orphelins Programmation du nettoyage automatique 1. create_simple_index(corpus_id: str) \u2192 dict Cr\u00e9e un index IVFFLAT standard sur une vue mat\u00e9rialis\u00e9e des chunks appartenant au corpus. from vectordb.src.index_manager import create_simple_index result = create_simple_index ( \"3159e84e-9dc6-41a7-a464-bb6c3894a5ad\" ) print ( result ) Param\u00e8tre Type Description corpus_id str UUID du corpus \u00e0 indexer (avec tirets). Retourne un dictionnaire comportant : status : \"success\" , \"exists\" ou \"error\" . message : message human-readable. Sur succ\u00e8s : index_type : toujours \"ivfflat\" . lists : nombre de listes utilis\u00e9es pour IVFFLAT. documents_updated : nombre de documents flagu\u00e9s index_needed=False . view_name : nom de la vue mat\u00e9rialis\u00e9e cr\u00e9\u00e9e. Exemple de sortie { \"status\" : \"success\" , \"message\" : \"Index vectoriel cr\u00e9\u00e9 pour 123 chunks dans le corpus 3159e84e-9dc6-41a7-a464-bb6c3894a5ad\" , \"index_type\" : \"ivfflat\" , \"lists\" : 11 , \"documents_updated\" : 42 , \"view_name\" : \"temp_corpus_chunks_3159e84e_9dc6_41a7_a464_bb6c3894a5ad\" } 2. drop_index(corpus_id: str) \u2192 dict Supprime l'index et la vue mat\u00e9rialis\u00e9e correspondant au corpus. from vectordb.src.index_manager import drop_index result = drop_index ( \"3159e84e-9dc6-41a7-a464-bb6c3894a5ad\" ) print ( result ) Param\u00e8tre Type Description corpus_id str UUID du corpus dont on veut supprimer l'index. Retourne : status : \"success\" , \"warning\" (si l'index n'existait pas) ou \"error\" . message : explication de l'op\u00e9ration. Exemple de sortie { \"status\" : \"success\" , \"message\" : \"Index idx_vector_3159e84e_9dc6_41a7_a464_bb6c3894a5ad et vue temp_corpus_chunks_3159e84e_9dc6_41a7_a464_bb6c3894a5ad supprim\u00e9s avec succ\u00e8s\" } 3. check_index_status(corpus_id: str) \u2192 dict R\u00e9cup\u00e8re l'\u00e9tat courant de l'index vectoriel pour un corpus donn\u00e9. from vectordb.src.index_manager import check_index_status status = check_index_status ( \"3159e84e-9dc6-41a7-a464-bb6c3894a5ad\" ) print ( status ) Param\u00e8tre Type Description corpus_id str UUID du corpus. Retourne un objet contenant : corpus_id : UUID interrog\u00e9. index_exists : bool\u00e9en, l'index existe-t-il en base ? config_exists : bool\u00e9en, la config Pydantic/SQLAlchemy existe-t-elle ? is_indexed : bool\u00e9en, l'index est-il actif selon la config ? index_type : \"ivfflat\" ou \"hnsw\" ou null . chunk_count : nombre total de chunks dans le corpus. indexed_chunks : nombre de chunks r\u00e9ellement index\u00e9s (config). last_indexed : date du dernier indexe ( datetime ) ou null . Exemple de sortie { \"corpus_id\" : \"3159e84e-9dc6-41a7-a464-bb6c3894a5ad\" , \"index_exists\" : true , \"config_exists\" : true , \"is_indexed\" : true , \"index_type\" : \"ivfflat\" , \"chunk_count\" : 123 , \"indexed_chunks\" : 123 , \"last_indexed\" : \"2025-05-02T14:23:10.123456\" } 4. check_all_indexes() \u2192 dict Balaye tous les corpus en base et renvoie l'\u00e9tat de leurs index. from vectordb.src.index_manager import check_all_indexes all_status = check_all_indexes () print ( all_status ) Retourne : status : \"success\" ou \"error\" . corpus_count : nombre de corpus trouv\u00e9s. indexes : tableau d'objets identiques \u00e0 la sortie de check_index_status() . Exemple de sortie { \"status\" : \"success\" , \"corpus_count\" : 3 , \"indexes\" : [ { \"corpus_id\" : \"aaa111...\" , \"index_exists\" : true , \"config_exists\" : true , \"is_indexed\" : true , \"index_type\" : \"ivfflat\" , \"chunk_count\" : 200 , \"indexed_chunks\" : 200 , \"last_indexed\" : \"2025-05-01T09:12:34\" }, { \"corpus_id\" : \"bbb222...\" , \"index_exists\" : false , \"config_exists\" : false , \"is_indexed\" : false , \"index_type\" : null , \"chunk_count\" : 0 , \"indexed_chunks\" : 0 , \"last_indexed\" : null } ] } 5. clean_orphaned_indexes() \u2192 dict Nettoie les index et configurations orphelins qui ne sont plus associ\u00e9s \u00e0 des corpus existants. Cette fonction est utile pour maintenir la propret\u00e9 de la base de donn\u00e9es apr\u00e8s la suppression de documents : - Elle identifie les configurations d'index dont le corpus n'existe plus - Elle supprime les vues mat\u00e9rialis\u00e9es et index PostgreSQL associ\u00e9s - Elle nettoie les entr\u00e9es dans la table index_configs from vectordb.src.index_cleaner import clean_orphaned_indexes result = clean_orphaned_indexes () print ( result ) Retourne un dictionnaire comportant : status : \"success\" , \"partial_success\" (certaines suppressions ont \u00e9chou\u00e9) ou \"error\" . deleted_count : nombre de configurations d'index supprim\u00e9es. cleaned_corpus_ids : liste des identifiants de corpus nettoy\u00e9s. errors : liste d'erreurs \u00e9ventuelles lors du nettoyage (si partial_success ). timestamp : horodatage de l'op\u00e9ration. Exemple de sortie { \"status\" : \"success\" , \"deleted_count\" : 2 , \"cleaned_corpus_ids\" : [ \"cccc111-dddd-eeee-ffff-gggghhhhiiii\" , \"jjjj222-kkkk-llll-mmmm-nnnnoooooppp\" ], \"errors\" : [], \"timestamp\" : \"2025-05-05T10:30:45.123456\" } 6. schedule_cleanup_job(interval_hours: int = 24) \u2192 None Configure un travail p\u00e9riodique qui nettoie automatiquement les index orphelins. Cette fonction utilise APScheduler pour ex\u00e9cuter clean_orphaned_indexes \u00e0 intervalles r\u00e9guliers. Elle est g\u00e9n\u00e9ralement appel\u00e9e au d\u00e9marrage de l'application via la lifespan FastAPI. from vectordb.src.index_cleaner import schedule_cleanup_job # Configurer un nettoyage toutes les 12 heures schedule_cleanup_job ( interval_hours = 12 ) Param\u00e8tre Type Description D\u00e9faut interval_hours int Intervalle en heures entre nettoyages 24 Exemple d'int\u00e9gration dans la lifespan FastAPI : from contextlib import asynccontextmanager from fastapi import FastAPI from vectordb.src.index_cleaner import schedule_cleanup_job @asynccontextmanager async def lifespan ( app : FastAPI ): # D\u00e9marrage de l'application schedule_cleanup_job ( interval_hours = 24 ) yield # Arr\u00eat de l'application - rien \u00e0 faire car APScheduler s'arr\u00eate automatiquement app = FastAPI ( lifespan = lifespan ) Logging & erreurs Toutes les op\u00e9rations journalisent en niveau INFO et WARNING via le logger standard. En cas d'erreur, la transaction est roll-back\u00e9e et {\"status\":\"error\",\"message\":...} est retourn\u00e9. Le nettoyage des index orphelins g\u00e9n\u00e8re des logs d\u00e9taill\u00e9s pour faciliter le suivi des op\u00e9rations. Modules : vectordb/src/index_manager.py , vectordb/src/index_cleaner.py Derni\u00e8re mise \u00e0 jour : 05 mai 2025","title":"Indexation vectorielle"},{"location":"lib/vectordb/index_lib/#module-index_manager","text":"Gestion des index vectoriels pour pgvector au sein de Cl\u00e9a-API . Ce module offre une API simple pour cr\u00e9er, supprimer et contr\u00f4ler l'\u00e9tat des index IVFFLAT associ\u00e9s aux chunks de documents.","title":"Module index_manager"},{"location":"lib/vectordb/index_lib/#installation","text":"Le module index_manager fait partie du package vectordb . Aucun paquet externe n'est n\u00e9cessaire, si ce n'est votre installation PostgreSQL/pgvector et SQLAlchemy. uv pip install vectordb","title":"Installation"},{"location":"lib/vectordb/index_lib/#table-des-matieres","text":"Cr\u00e9er un index simple Supprimer un index V\u00e9rifier l'\u00e9tat d'un index V\u00e9rifier tous les index Nettoyer les index orphelins Programmation du nettoyage automatique","title":"Table des mati\u00e8res"},{"location":"lib/vectordb/index_lib/#1-create_simple_indexcorpus_id-str-dict","text":"Cr\u00e9e un index IVFFLAT standard sur une vue mat\u00e9rialis\u00e9e des chunks appartenant au corpus. from vectordb.src.index_manager import create_simple_index result = create_simple_index ( \"3159e84e-9dc6-41a7-a464-bb6c3894a5ad\" ) print ( result ) Param\u00e8tre Type Description corpus_id str UUID du corpus \u00e0 indexer (avec tirets). Retourne un dictionnaire comportant : status : \"success\" , \"exists\" ou \"error\" . message : message human-readable. Sur succ\u00e8s : index_type : toujours \"ivfflat\" . lists : nombre de listes utilis\u00e9es pour IVFFLAT. documents_updated : nombre de documents flagu\u00e9s index_needed=False . view_name : nom de la vue mat\u00e9rialis\u00e9e cr\u00e9\u00e9e. Exemple de sortie { \"status\" : \"success\" , \"message\" : \"Index vectoriel cr\u00e9\u00e9 pour 123 chunks dans le corpus 3159e84e-9dc6-41a7-a464-bb6c3894a5ad\" , \"index_type\" : \"ivfflat\" , \"lists\" : 11 , \"documents_updated\" : 42 , \"view_name\" : \"temp_corpus_chunks_3159e84e_9dc6_41a7_a464_bb6c3894a5ad\" }","title":"1. create_simple_index(corpus_id: str) \u2192 dict"},{"location":"lib/vectordb/index_lib/#2-drop_indexcorpus_id-str-dict","text":"Supprime l'index et la vue mat\u00e9rialis\u00e9e correspondant au corpus. from vectordb.src.index_manager import drop_index result = drop_index ( \"3159e84e-9dc6-41a7-a464-bb6c3894a5ad\" ) print ( result ) Param\u00e8tre Type Description corpus_id str UUID du corpus dont on veut supprimer l'index. Retourne : status : \"success\" , \"warning\" (si l'index n'existait pas) ou \"error\" . message : explication de l'op\u00e9ration. Exemple de sortie { \"status\" : \"success\" , \"message\" : \"Index idx_vector_3159e84e_9dc6_41a7_a464_bb6c3894a5ad et vue temp_corpus_chunks_3159e84e_9dc6_41a7_a464_bb6c3894a5ad supprim\u00e9s avec succ\u00e8s\" }","title":"2. drop_index(corpus_id: str) \u2192 dict"},{"location":"lib/vectordb/index_lib/#3-check_index_statuscorpus_id-str-dict","text":"R\u00e9cup\u00e8re l'\u00e9tat courant de l'index vectoriel pour un corpus donn\u00e9. from vectordb.src.index_manager import check_index_status status = check_index_status ( \"3159e84e-9dc6-41a7-a464-bb6c3894a5ad\" ) print ( status ) Param\u00e8tre Type Description corpus_id str UUID du corpus. Retourne un objet contenant : corpus_id : UUID interrog\u00e9. index_exists : bool\u00e9en, l'index existe-t-il en base ? config_exists : bool\u00e9en, la config Pydantic/SQLAlchemy existe-t-elle ? is_indexed : bool\u00e9en, l'index est-il actif selon la config ? index_type : \"ivfflat\" ou \"hnsw\" ou null . chunk_count : nombre total de chunks dans le corpus. indexed_chunks : nombre de chunks r\u00e9ellement index\u00e9s (config). last_indexed : date du dernier indexe ( datetime ) ou null . Exemple de sortie { \"corpus_id\" : \"3159e84e-9dc6-41a7-a464-bb6c3894a5ad\" , \"index_exists\" : true , \"config_exists\" : true , \"is_indexed\" : true , \"index_type\" : \"ivfflat\" , \"chunk_count\" : 123 , \"indexed_chunks\" : 123 , \"last_indexed\" : \"2025-05-02T14:23:10.123456\" }","title":"3. check_index_status(corpus_id: str) \u2192 dict"},{"location":"lib/vectordb/index_lib/#4-check_all_indexes-dict","text":"Balaye tous les corpus en base et renvoie l'\u00e9tat de leurs index. from vectordb.src.index_manager import check_all_indexes all_status = check_all_indexes () print ( all_status ) Retourne : status : \"success\" ou \"error\" . corpus_count : nombre de corpus trouv\u00e9s. indexes : tableau d'objets identiques \u00e0 la sortie de check_index_status() . Exemple de sortie { \"status\" : \"success\" , \"corpus_count\" : 3 , \"indexes\" : [ { \"corpus_id\" : \"aaa111...\" , \"index_exists\" : true , \"config_exists\" : true , \"is_indexed\" : true , \"index_type\" : \"ivfflat\" , \"chunk_count\" : 200 , \"indexed_chunks\" : 200 , \"last_indexed\" : \"2025-05-01T09:12:34\" }, { \"corpus_id\" : \"bbb222...\" , \"index_exists\" : false , \"config_exists\" : false , \"is_indexed\" : false , \"index_type\" : null , \"chunk_count\" : 0 , \"indexed_chunks\" : 0 , \"last_indexed\" : null } ] }","title":"4. check_all_indexes() \u2192 dict"},{"location":"lib/vectordb/index_lib/#5-clean_orphaned_indexes-dict","text":"Nettoie les index et configurations orphelins qui ne sont plus associ\u00e9s \u00e0 des corpus existants. Cette fonction est utile pour maintenir la propret\u00e9 de la base de donn\u00e9es apr\u00e8s la suppression de documents : - Elle identifie les configurations d'index dont le corpus n'existe plus - Elle supprime les vues mat\u00e9rialis\u00e9es et index PostgreSQL associ\u00e9s - Elle nettoie les entr\u00e9es dans la table index_configs from vectordb.src.index_cleaner import clean_orphaned_indexes result = clean_orphaned_indexes () print ( result ) Retourne un dictionnaire comportant : status : \"success\" , \"partial_success\" (certaines suppressions ont \u00e9chou\u00e9) ou \"error\" . deleted_count : nombre de configurations d'index supprim\u00e9es. cleaned_corpus_ids : liste des identifiants de corpus nettoy\u00e9s. errors : liste d'erreurs \u00e9ventuelles lors du nettoyage (si partial_success ). timestamp : horodatage de l'op\u00e9ration. Exemple de sortie { \"status\" : \"success\" , \"deleted_count\" : 2 , \"cleaned_corpus_ids\" : [ \"cccc111-dddd-eeee-ffff-gggghhhhiiii\" , \"jjjj222-kkkk-llll-mmmm-nnnnoooooppp\" ], \"errors\" : [], \"timestamp\" : \"2025-05-05T10:30:45.123456\" }","title":"5. clean_orphaned_indexes() \u2192 dict"},{"location":"lib/vectordb/index_lib/#6-schedule_cleanup_jobinterval_hours-int-24-none","text":"Configure un travail p\u00e9riodique qui nettoie automatiquement les index orphelins. Cette fonction utilise APScheduler pour ex\u00e9cuter clean_orphaned_indexes \u00e0 intervalles r\u00e9guliers. Elle est g\u00e9n\u00e9ralement appel\u00e9e au d\u00e9marrage de l'application via la lifespan FastAPI. from vectordb.src.index_cleaner import schedule_cleanup_job # Configurer un nettoyage toutes les 12 heures schedule_cleanup_job ( interval_hours = 12 ) Param\u00e8tre Type Description D\u00e9faut interval_hours int Intervalle en heures entre nettoyages 24 Exemple d'int\u00e9gration dans la lifespan FastAPI : from contextlib import asynccontextmanager from fastapi import FastAPI from vectordb.src.index_cleaner import schedule_cleanup_job @asynccontextmanager async def lifespan ( app : FastAPI ): # D\u00e9marrage de l'application schedule_cleanup_job ( interval_hours = 24 ) yield # Arr\u00eat de l'application - rien \u00e0 faire car APScheduler s'arr\u00eate automatiquement app = FastAPI ( lifespan = lifespan )","title":"6. schedule_cleanup_job(interval_hours: int = 24) \u2192 None"},{"location":"lib/vectordb/index_lib/#logging-erreurs","text":"Toutes les op\u00e9rations journalisent en niveau INFO et WARNING via le logger standard. En cas d'erreur, la transaction est roll-back\u00e9e et {\"status\":\"error\",\"message\":...} est retourn\u00e9. Le nettoyage des index orphelins g\u00e9n\u00e8re des logs d\u00e9taill\u00e9s pour faciliter le suivi des op\u00e9rations. Modules : vectordb/src/index_manager.py , vectordb/src/index_cleaner.py Derni\u00e8re mise \u00e0 jour : 05 mai 2025","title":"Logging &amp; erreurs"},{"location":"lib/vectordb/search_lib/","text":"Module search (Hybrid Semantic / Metadata Search) Ce module impl\u00e9mente un moteur de recherche hybride combinant filtres SQL, similarit\u00e9 vectorielle (pgvector) et rerank via Cross-Encoder. Table des mati\u00e8res Installation Mod\u00e8les (schemas) Classe SearchEngine Constructeur M\u00e9thode hybrid_search M\u00e9thode log_search_query M\u00e9thode evaluate_confidence M\u00e9thode normalize_scores M\u00e9thode priv\u00e9e _build_sql M\u00e9thode priv\u00e9e _get_context Historisation des recherches Exemple d'utilisation Installation uv pip install clea_vectordb # ou via votre setup.py/pyproject.toml Mod\u00e8les (schemas) Les Pydantic schemas utilis\u00e9s par le moteur se trouvent dans vectordb/src/schemas.py : SearchRequest : param\u00e8tres de la recherche (requ\u00eate, filtres, pagination, etc.). ChunkResult : un chunk renvoy\u00e9 (camelCase). HierarchicalContext : contexte parent (niveaux 0\u20132). SearchResponse : enveloppe de r\u00e9ponse (requ\u00eate, total, liste des ChunkResult ). Pour la d\u00e9finition d\u00e9taill\u00e9e de ces sch\u00e9mas, r\u00e9f\u00e9rez-vous \u00e0 la section Components \u2192 Schemas dans votre OpenAPI/Swagger. Classe SearchEngine Constructeur engine = SearchEngine () Initialise : un g\u00e9n\u00e9rateur d'embeddings ( EmbeddingGenerator ) un ranker Cross-Encoder ( ResultRanker ) les seuils de pertinence configurables ( min_relevance_threshold , high_confidence_threshold ) M\u00e9thode hybrid_search def hybrid_search ( self , db : Session , req : SearchRequest ) -> SearchResponse : ... Arguments db: Session \u2013 session SQLAlchemy req: SearchRequest \u2013 param\u00e8tres de la recherche Fonctionnement G\u00e9n\u00e8re l'embedding de la requ\u00eate. Monte la requ\u00eate SQL pour ANN + m\u00e9tadonn\u00e9es (via _build_sql ). Ex\u00e9cute db.execute(text(sql), params) . Si pas de r\u00e9sultat, renvoie un SearchResponse vide. Rerank les top k \u00d7 3 r\u00e9sultats avec un Cross-Encoder. Construit la liste finale de ChunkResult , en r\u00e9cup\u00e9rant le contexte hi\u00e9rarchique si req.hierarchical=True . Historise la recherche dans la base de donn\u00e9es via log_search_query . Renvoie un SearchResponse(query, topK, totalResults, results) . Retour SearchResponse contenant : query (str) topK (int) totalResults (int) results ( List[ChunkResult] ) confidence ( ConfidenceMetrics ) normalized (bool) message (str) M\u00e9thode log_search_query def log_search_query ( self , db : Session , req : SearchRequest , response : SearchResponse ) -> None : ... Arguments db: Session \u2013 session SQLAlchemy active req: SearchRequest \u2013 requ\u00eate de recherche soumise response: SearchResponse \u2013 r\u00e9ponse g\u00e9n\u00e9r\u00e9e avec les r\u00e9sultats Fonctionnement Cr\u00e9e une entr\u00e9e dans la table SearchQuery avec les informations sur la recherche Persiste les m\u00e9tadonn\u00e9es de la requ\u00eate (texte, th\u00e8me, type de document, corpus) Enregistre les m\u00e9triques de r\u00e9sultats (nombre, niveau de confiance) G\u00e8re silencieusement les erreurs pour ne pas perturber le flux principal M\u00e9thode evaluate_confidence def evaluate_confidence ( self , scores : List [ float ]) -> ConfidenceMetrics : ... But : analyser les scores des r\u00e9sultats pour d\u00e9terminer la pertinence globale et d\u00e9tecter les requ\u00eates hors domaine. Arguments : scores: List[float] \u2013 liste des scores de pertinence issus du ranker Fonctionnement : Calcule les statistiques de base (min, max, moyenne, m\u00e9diane) D\u00e9termine le niveau de confiance (0.1 \u00e0 0.9) et le message associ\u00e9 Les seuils utilis\u00e9s sont min_relevance_threshold et high_confidence_threshold Niveaux de confiance : 0.1 \u2013 \"Requ\u00eate probablement hors du domaine de connaissances\" 0.4 \u2013 \"Pertinence moyenne: r\u00e9sultats disponibles mais peu sp\u00e9cifiques\" 0.7 \u2013 \"Bonne pertinence: r\u00e9sultats g\u00e9n\u00e9ralement pertinents\" 0.9 \u2013 \"Haute pertinence: r\u00e9sultats fiables trouv\u00e9s\" Retour : ConfidenceMetrics contenant le niveau, le message et les statistiques M\u00e9thode normalize_scores def normalize_scores ( self , scores : List [ float ]) -> List [ float ]: ... But : transformer les scores bruts (potentiellement n\u00e9gatifs) en valeurs entre 0 et 1 Arguments : scores: List[float] \u2013 liste des scores bruts du mod\u00e8le Retour : liste de scores normalis\u00e9s entre 0 et 1, facilitant l'interpr\u00e9tation M\u00e9thode priv\u00e9e _build_sql @staticmethod def _build_sql ( req : SearchRequest ) -> Tuple [ str , dict [ str , Any ]]: ... But : assembler dynamiquement la clause WHERE SQL selon les filtres de req Filtres g\u00e9r\u00e9s : theme , document_type plage start_date \u2013 end_date corpus_id hierarchy_level Structure : WITH ranked AS ( SELECT \u2026 , c . embedding <=> (: query_embedding ):: vector AS distance FROM chunks c JOIN documents d ON \u2026 WHERE 1 = 1 [ AND d . theme = : theme ] [ AND \u2026 ] ORDER BY distance LIMIT : expanded_limit ) SELECT * FROM ranked ORDER BY distance LIMIT : top_k ; * Retour : tuple (sql: str, params: dict) . M\u00e9thode priv\u00e9e _get_context @staticmethod def _get_context ( db : Session , chunk_id : int ) -> Optional [ HierarchicalContext ]: ... But : pour un chunk donn\u00e9, remonter r\u00e9cursivement ses parents (niveaux 0\u20132) Retour : instanciation de HierarchicalContext ou None si pas de parent . Historisation des recherches Le module int\u00e8gre un syst\u00e8me d'historisation qui enregistre automatiquement toutes les recherches effectu\u00e9es dans la base de donn\u00e9es. Cette fonctionnalit\u00e9 permet: M\u00e9morisation des requ\u00eates pour analyse Calcul de statistiques sur les comportements de recherche G\u00e9n\u00e9ration de rapports sur les th\u00e8mes et termes recherch\u00e9s Sch\u00e9ma de la table SearchQuery class SearchQuery ( Base ): \"\"\"Historique des recherches utilisateurs.\"\"\" __tablename__ = \"search_queries\" id = Column ( Integer , primary_key = True ) query_text = Column ( String , nullable = False ) theme = Column ( String , nullable = True ) document_type = Column ( String , nullable = True ) corpus_id = Column ( String , nullable = True ) results_count = Column ( Integer , default = 0 ) confidence_level = Column ( Float , default = 0.0 ) created_at = Column ( DateTime , default = datetime . now ) user_id = Column ( String , nullable = True ) # Pour int\u00e9gration future d'authentification Activation de l'historisation L'historisation est activ\u00e9e automatiquement pour toute requ\u00eate effectu\u00e9e via la m\u00e9thode hybrid_search . Pour les applications intensives o\u00f9 la performance est critique, elle peut \u00eatre d\u00e9sactiv\u00e9e via un param\u00e8tre de configuration dans le fichier d'environnement: LOG_SEARCH_QUERIES = false Exploitation des donn\u00e9es d'historique Les donn\u00e9es historis\u00e9es peuvent \u00eatre consult\u00e9es: Via SQL standard sur la table search_queries Via le module de statistiques de Cl\u00e9a-API ( stats/src/stats_src_compute.py ) Via les endpoints API du tableau de bord admin ( /stats/searches ) Exemple d'utilisation avanc\u00e9e # Requ\u00eate avec options avanc\u00e9es req = SearchRequest ( query = \"analyse risques climatiques\" , top_k = 5 , theme = \"RSE\" , filter_by_relevance = True , # Filtrer les r\u00e9sultats peu pertinents normalize_scores = True , # Normaliser les scores entre 0 et 1 hierarchical = True , ) # Ex\u00e9cuter la recherche response = searcher . hybrid_search ( db , req ) # Analyser la confiance dans les r\u00e9sultats print ( f \"Confiance: { response . confidence . level : .2f } - { response . confidence . message } \" ) print ( f \"Statistiques: min= { response . confidence . stats [ 'min' ] : .2f } , \" f \"max= { response . confidence . stats [ 'max' ] : .2f } , \" f \"avg= { response . confidence . stats [ 'avg' ] : .2f } \" ) # V\u00e9rifier si des r\u00e9sultats ont \u00e9t\u00e9 trouv\u00e9s if not response . results : print ( \"Aucun r\u00e9sultat pertinent trouv\u00e9\" ) if response . confidence . level < 0.3 : print ( \"La requ\u00eate semble hors du domaine de connaissances\" ) else : for chunk in response . results : print ( f \"[ { chunk . score : .2f } ] { chunk . title } \u2192 { chunk . content [: 60 ] } \u2026\" ) # Les donn\u00e9es de cette recherche sont automatiquement historis\u00e9es en base Exemple d'utilisation simple from sqlalchemy import create_engine from sqlalchemy.orm import sessionmaker from vectordb.src.search import SearchEngine , SearchRequest # 1. Pr\u00e9parer la session engine_db = create_engine ( \"postgresql://\u2026\" ) SessionLocal = sessionmaker ( bind = engine_db ) db = SessionLocal () # 2. Instancier le SearchEngine searcher = SearchEngine () # 3. Construire la requ\u00eate req = SearchRequest ( query = \"analyse risques climatiques\" , top_k = 5 , theme = \"RSE\" , corpus_id = \"0207a0ec-394b-475f-912e-edf0315f6fa3\" , hierarchical = True , ) # 4. Ex\u00e9cuter la recherche response = searcher . hybrid_search ( db , req ) # 5. Parcourir les r\u00e9sultats for chunk in response . results : print ( f \"[ { chunk . score : .2f } ] { chunk . title } \u2192 { chunk . content [: 60 ] } \u2026\" ) Voir aussi : les endpoints FastAPI dans search_endpoint.py \u2013 POST /search/hybrid_search \u2192 renvoie List[ChunkResult] .","title":"Module search (Hybrid Semantic / Metadata Search)"},{"location":"lib/vectordb/search_lib/#module-search-hybrid-semantic-metadata-search","text":"Ce module impl\u00e9mente un moteur de recherche hybride combinant filtres SQL, similarit\u00e9 vectorielle (pgvector) et rerank via Cross-Encoder.","title":"Module search (Hybrid Semantic / Metadata Search)"},{"location":"lib/vectordb/search_lib/#table-des-matieres","text":"Installation Mod\u00e8les (schemas) Classe SearchEngine Constructeur M\u00e9thode hybrid_search M\u00e9thode log_search_query M\u00e9thode evaluate_confidence M\u00e9thode normalize_scores M\u00e9thode priv\u00e9e _build_sql M\u00e9thode priv\u00e9e _get_context Historisation des recherches Exemple d'utilisation","title":"Table des mati\u00e8res"},{"location":"lib/vectordb/search_lib/#installation","text":"uv pip install clea_vectordb # ou via votre setup.py/pyproject.toml","title":"Installation"},{"location":"lib/vectordb/search_lib/#modeles-schemas","text":"Les Pydantic schemas utilis\u00e9s par le moteur se trouvent dans vectordb/src/schemas.py : SearchRequest : param\u00e8tres de la recherche (requ\u00eate, filtres, pagination, etc.). ChunkResult : un chunk renvoy\u00e9 (camelCase). HierarchicalContext : contexte parent (niveaux 0\u20132). SearchResponse : enveloppe de r\u00e9ponse (requ\u00eate, total, liste des ChunkResult ). Pour la d\u00e9finition d\u00e9taill\u00e9e de ces sch\u00e9mas, r\u00e9f\u00e9rez-vous \u00e0 la section Components \u2192 Schemas dans votre OpenAPI/Swagger.","title":"Mod\u00e8les (schemas)"},{"location":"lib/vectordb/search_lib/#classe-searchengine","text":"","title":"Classe SearchEngine&#x20;"},{"location":"lib/vectordb/search_lib/#constructeur","text":"engine = SearchEngine () Initialise : un g\u00e9n\u00e9rateur d'embeddings ( EmbeddingGenerator ) un ranker Cross-Encoder ( ResultRanker ) les seuils de pertinence configurables ( min_relevance_threshold , high_confidence_threshold )","title":"Constructeur"},{"location":"lib/vectordb/search_lib/#methode-hybrid_search","text":"def hybrid_search ( self , db : Session , req : SearchRequest ) -> SearchResponse : ... Arguments db: Session \u2013 session SQLAlchemy req: SearchRequest \u2013 param\u00e8tres de la recherche Fonctionnement G\u00e9n\u00e8re l'embedding de la requ\u00eate. Monte la requ\u00eate SQL pour ANN + m\u00e9tadonn\u00e9es (via _build_sql ). Ex\u00e9cute db.execute(text(sql), params) . Si pas de r\u00e9sultat, renvoie un SearchResponse vide. Rerank les top k \u00d7 3 r\u00e9sultats avec un Cross-Encoder. Construit la liste finale de ChunkResult , en r\u00e9cup\u00e9rant le contexte hi\u00e9rarchique si req.hierarchical=True . Historise la recherche dans la base de donn\u00e9es via log_search_query . Renvoie un SearchResponse(query, topK, totalResults, results) . Retour SearchResponse contenant : query (str) topK (int) totalResults (int) results ( List[ChunkResult] ) confidence ( ConfidenceMetrics ) normalized (bool) message (str)","title":"M\u00e9thode hybrid_search"},{"location":"lib/vectordb/search_lib/#methode-log_search_query","text":"def log_search_query ( self , db : Session , req : SearchRequest , response : SearchResponse ) -> None : ... Arguments db: Session \u2013 session SQLAlchemy active req: SearchRequest \u2013 requ\u00eate de recherche soumise response: SearchResponse \u2013 r\u00e9ponse g\u00e9n\u00e9r\u00e9e avec les r\u00e9sultats Fonctionnement Cr\u00e9e une entr\u00e9e dans la table SearchQuery avec les informations sur la recherche Persiste les m\u00e9tadonn\u00e9es de la requ\u00eate (texte, th\u00e8me, type de document, corpus) Enregistre les m\u00e9triques de r\u00e9sultats (nombre, niveau de confiance) G\u00e8re silencieusement les erreurs pour ne pas perturber le flux principal","title":"M\u00e9thode log_search_query"},{"location":"lib/vectordb/search_lib/#methode-evaluate_confidence","text":"def evaluate_confidence ( self , scores : List [ float ]) -> ConfidenceMetrics : ... But : analyser les scores des r\u00e9sultats pour d\u00e9terminer la pertinence globale et d\u00e9tecter les requ\u00eates hors domaine. Arguments : scores: List[float] \u2013 liste des scores de pertinence issus du ranker Fonctionnement : Calcule les statistiques de base (min, max, moyenne, m\u00e9diane) D\u00e9termine le niveau de confiance (0.1 \u00e0 0.9) et le message associ\u00e9 Les seuils utilis\u00e9s sont min_relevance_threshold et high_confidence_threshold Niveaux de confiance : 0.1 \u2013 \"Requ\u00eate probablement hors du domaine de connaissances\" 0.4 \u2013 \"Pertinence moyenne: r\u00e9sultats disponibles mais peu sp\u00e9cifiques\" 0.7 \u2013 \"Bonne pertinence: r\u00e9sultats g\u00e9n\u00e9ralement pertinents\" 0.9 \u2013 \"Haute pertinence: r\u00e9sultats fiables trouv\u00e9s\" Retour : ConfidenceMetrics contenant le niveau, le message et les statistiques","title":"M\u00e9thode evaluate_confidence"},{"location":"lib/vectordb/search_lib/#methode-normalize_scores","text":"def normalize_scores ( self , scores : List [ float ]) -> List [ float ]: ... But : transformer les scores bruts (potentiellement n\u00e9gatifs) en valeurs entre 0 et 1 Arguments : scores: List[float] \u2013 liste des scores bruts du mod\u00e8le Retour : liste de scores normalis\u00e9s entre 0 et 1, facilitant l'interpr\u00e9tation","title":"M\u00e9thode normalize_scores"},{"location":"lib/vectordb/search_lib/#methode-privee-_build_sql","text":"@staticmethod def _build_sql ( req : SearchRequest ) -> Tuple [ str , dict [ str , Any ]]: ... But : assembler dynamiquement la clause WHERE SQL selon les filtres de req Filtres g\u00e9r\u00e9s : theme , document_type plage start_date \u2013 end_date corpus_id hierarchy_level Structure : WITH ranked AS ( SELECT \u2026 , c . embedding <=> (: query_embedding ):: vector AS distance FROM chunks c JOIN documents d ON \u2026 WHERE 1 = 1 [ AND d . theme = : theme ] [ AND \u2026 ] ORDER BY distance LIMIT : expanded_limit ) SELECT * FROM ranked ORDER BY distance LIMIT : top_k ; * Retour : tuple (sql: str, params: dict) .","title":"M\u00e9thode priv\u00e9e _build_sql"},{"location":"lib/vectordb/search_lib/#methode-privee-_get_context","text":"@staticmethod def _get_context ( db : Session , chunk_id : int ) -> Optional [ HierarchicalContext ]: ... But : pour un chunk donn\u00e9, remonter r\u00e9cursivement ses parents (niveaux 0\u20132) Retour : instanciation de HierarchicalContext ou None si pas de parent .","title":"M\u00e9thode priv\u00e9e _get_context"},{"location":"lib/vectordb/search_lib/#historisation-des-recherches","text":"Le module int\u00e8gre un syst\u00e8me d'historisation qui enregistre automatiquement toutes les recherches effectu\u00e9es dans la base de donn\u00e9es. Cette fonctionnalit\u00e9 permet: M\u00e9morisation des requ\u00eates pour analyse Calcul de statistiques sur les comportements de recherche G\u00e9n\u00e9ration de rapports sur les th\u00e8mes et termes recherch\u00e9s","title":"Historisation des recherches"},{"location":"lib/vectordb/search_lib/#schema-de-la-table-searchquery","text":"class SearchQuery ( Base ): \"\"\"Historique des recherches utilisateurs.\"\"\" __tablename__ = \"search_queries\" id = Column ( Integer , primary_key = True ) query_text = Column ( String , nullable = False ) theme = Column ( String , nullable = True ) document_type = Column ( String , nullable = True ) corpus_id = Column ( String , nullable = True ) results_count = Column ( Integer , default = 0 ) confidence_level = Column ( Float , default = 0.0 ) created_at = Column ( DateTime , default = datetime . now ) user_id = Column ( String , nullable = True ) # Pour int\u00e9gration future d'authentification","title":"Sch\u00e9ma de la table SearchQuery"},{"location":"lib/vectordb/search_lib/#activation-de-lhistorisation","text":"L'historisation est activ\u00e9e automatiquement pour toute requ\u00eate effectu\u00e9e via la m\u00e9thode hybrid_search . Pour les applications intensives o\u00f9 la performance est critique, elle peut \u00eatre d\u00e9sactiv\u00e9e via un param\u00e8tre de configuration dans le fichier d'environnement: LOG_SEARCH_QUERIES = false","title":"Activation de l'historisation"},{"location":"lib/vectordb/search_lib/#exploitation-des-donnees-dhistorique","text":"Les donn\u00e9es historis\u00e9es peuvent \u00eatre consult\u00e9es: Via SQL standard sur la table search_queries Via le module de statistiques de Cl\u00e9a-API ( stats/src/stats_src_compute.py ) Via les endpoints API du tableau de bord admin ( /stats/searches )","title":"Exploitation des donn\u00e9es d'historique"},{"location":"lib/vectordb/search_lib/#exemple-dutilisation-avancee","text":"# Requ\u00eate avec options avanc\u00e9es req = SearchRequest ( query = \"analyse risques climatiques\" , top_k = 5 , theme = \"RSE\" , filter_by_relevance = True , # Filtrer les r\u00e9sultats peu pertinents normalize_scores = True , # Normaliser les scores entre 0 et 1 hierarchical = True , ) # Ex\u00e9cuter la recherche response = searcher . hybrid_search ( db , req ) # Analyser la confiance dans les r\u00e9sultats print ( f \"Confiance: { response . confidence . level : .2f } - { response . confidence . message } \" ) print ( f \"Statistiques: min= { response . confidence . stats [ 'min' ] : .2f } , \" f \"max= { response . confidence . stats [ 'max' ] : .2f } , \" f \"avg= { response . confidence . stats [ 'avg' ] : .2f } \" ) # V\u00e9rifier si des r\u00e9sultats ont \u00e9t\u00e9 trouv\u00e9s if not response . results : print ( \"Aucun r\u00e9sultat pertinent trouv\u00e9\" ) if response . confidence . level < 0.3 : print ( \"La requ\u00eate semble hors du domaine de connaissances\" ) else : for chunk in response . results : print ( f \"[ { chunk . score : .2f } ] { chunk . title } \u2192 { chunk . content [: 60 ] } \u2026\" ) # Les donn\u00e9es de cette recherche sont automatiquement historis\u00e9es en base","title":"Exemple d'utilisation avanc\u00e9e"},{"location":"lib/vectordb/search_lib/#exemple-dutilisation-simple","text":"from sqlalchemy import create_engine from sqlalchemy.orm import sessionmaker from vectordb.src.search import SearchEngine , SearchRequest # 1. Pr\u00e9parer la session engine_db = create_engine ( \"postgresql://\u2026\" ) SessionLocal = sessionmaker ( bind = engine_db ) db = SessionLocal () # 2. Instancier le SearchEngine searcher = SearchEngine () # 3. Construire la requ\u00eate req = SearchRequest ( query = \"analyse risques climatiques\" , top_k = 5 , theme = \"RSE\" , corpus_id = \"0207a0ec-394b-475f-912e-edf0315f6fa3\" , hierarchical = True , ) # 4. Ex\u00e9cuter la recherche response = searcher . hybrid_search ( db , req ) # 5. Parcourir les r\u00e9sultats for chunk in response . results : print ( f \"[ { chunk . score : .2f } ] { chunk . title } \u2192 { chunk . content [: 60 ] } \u2026\" ) Voir aussi : les endpoints FastAPI dans search_endpoint.py \u2013 POST /search/hybrid_search \u2192 renvoie List[ChunkResult] .","title":"Exemple d'utilisation simple"}]}