{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"database/","text":"Technologie de stockage Clea-API Ce document pr\u00e9sente l'architecture et les choix technologiques de la base de donn\u00e9es utilis\u00e9e par Clea-API . La solution repose sur PostgreSQL enrichi de l'extension pgvector , pilot\u00e9 via SQLAlchemy . Table des mati\u00e8res Configuration et connexion Mod\u00e8les de donn\u00e9es Document Chunk IndexConfig Indexation vectorielle Sch\u00e9ma global Bonnes pratiques Installation et configuration 1. Configuration et connexion Les param\u00e8tres de connexion sont lus depuis le fichier .env : DB_USER = os . getenv ( \"DB_USER\" , \"postgres\" ) DB_PASSWORD = os . getenv ( \"DB_PASSWORD\" , \"password\" ) DB_HOST = os . getenv ( \"DB_HOST\" , \"localhost\" ) DB_PORT = os . getenv ( \"DB_PORT\" , \"5432\" ) DB_NAME = os . getenv ( \"DB_NAME\" , \"vectordb\" ) DATABASE_URL = f \"postgresql:// { DB_USER } : { DB_PASSWORD } @ { DB_HOST } : { DB_PORT } / { DB_NAME } \" Composants principaux Engine SQLAlchemy : create_engine(DATABASE_URL) Session factory : SessionLocal = sessionmaker(bind=engine, autocommit=False, autoflush=False) Base d\u00e9clarative : Base = declarative_base() Utilitaire de session def get_db (): \"\"\"Cr\u00e9e et retourne une session de base de donn\u00e9es. G\u00e9n\u00e8re une session SQLAlchemy compatible avec FastAPI \u00e0 utiliser comme d\u00e9pendance dans les endpoints. Yields: Session: Session SQLAlchemy pour les op\u00e9rations de base de donn\u00e9es \"\"\" db = SessionLocal () try : yield db finally : db . close () Initialisation def init_db (): \"\"\"Initialise la base de donn\u00e9es avec les tables et extensions n\u00e9cessaires. Cr\u00e9e l'extension pgvector si elle n'existe pas d\u00e9j\u00e0 et g\u00e9n\u00e8re toutes les tables d\u00e9finies dans les mod\u00e8les SQLAlchemy. \"\"\" with engine . connect () as conn : conn . execute ( text ( \"CREATE EXTENSION IF NOT EXISTS vector\" )) conn . commit () Base . metadata . create_all ( bind = engine ) 2. Mod\u00e8les de donn\u00e9es 2.1. Document class Document ( Base ): \"\"\"Stocke les m\u00e9tadonn\u00e9es globales des documents. Attrs: id (int): Identifiant unique du document title (str): Titre du document theme (str): Th\u00e8me ou cat\u00e9gorie du document document_type (str): Type de document (PDF, DOCX, etc.) publish_date (Date): Date de publication du document corpus_id (str): UUID du corpus auquel appartient ce document created_at (Date): Date d'ajout \u00e0 la base de donn\u00e9es index_needed (bool): Indique si une mise \u00e0 jour d'index est n\u00e9cessaire chunks (relationship): Relation avec les fragments textuels \"\"\" __tablename__ = \"documents\" id = mapped_column ( Integer , primary_key = True ) title = mapped_column ( String ( 255 ), nullable = False ) theme = mapped_column ( String ( 100 )) document_type = mapped_column ( String ( 100 )) publish_date = mapped_column ( Date ) corpus_id = mapped_column ( String ( 36 ), index = True ) created_at = mapped_column ( Date , default = datetime . now ) index_needed = mapped_column ( Boolean , default = False ) chunks = relationship ( \"Chunk\" , back_populates = \"document\" , cascade = \"all, delete-orphan\" ) __table_args__ = ( Index ( \"idx_document_theme\" , \"theme\" ), Index ( \"idx_document_type\" , \"document_type\" ), Index ( \"idx_document_date\" , \"publish_date\" ), Index ( \"idx_document_corpus\" , \"corpus_id\" ), ) M\u00e9tadonn\u00e9es : titre, th\u00e8me, type, date de publication, identifiant de corpus (UUID) Relation 1\u2013N vers les Chunk (cascade delete) Index SQL sur les colonnes fr\u00e9quemment utilis\u00e9es pour le filtrage 2.2. Chunk class Chunk ( Base ): \"\"\"Stocke les fragments de texte avec leurs embeddings vectoriels. Attrs: id (int): Identifiant unique du fragment document_id (int): R\u00e9f\u00e9rence au document parent content (str): Texte du fragment embedding (Vector): Repr\u00e9sentation vectorielle (768 dim) start_char (int): Position de d\u00e9but dans le document source end_char (int): Position de fin dans le document source hierarchy_level (int): Niveau hi\u00e9rarchique (0-3) parent_chunk_id (int): R\u00e9f\u00e9rence au fragment parent document (relationship): Relation avec le document parent parent (relationship): Relation avec le fragment parent children (relationship): Relation avec les fragments enfants \"\"\" __tablename__ = \"chunks\" id = mapped_column ( Integer , primary_key = True ) document_id = mapped_column ( Integer , ForeignKey ( \"documents.id\" , ondelete = \"CASCADE\" ), nullable = False ) content = mapped_column ( Text , nullable = False ) embedding = mapped_column ( Vector ( 768 )) start_char = mapped_column ( Integer ) end_char = mapped_column ( Integer ) hierarchy_level = mapped_column ( Integer , default = 3 ) parent_chunk_id = mapped_column ( Integer , ForeignKey ( \"chunks.id\" , ondelete = \"CASCADE\" )) document = relationship ( \"Document\" , back_populates = \"chunks\" ) parent = relationship ( \"Chunk\" , remote_side = [ id ], back_populates = \"children\" ) children = relationship ( \"Chunk\" , back_populates = \"parent\" , cascade = \"all, delete-orphan\" , single_parent = True ) __table_args__ = ( Index ( \"idx_chunk_document_level\" , \"document_id\" , \"hierarchy_level\" ), Index ( \"idx_chunk_parent\" , \"parent_chunk_id\" ), ) Contenu texte segment\u00e9 en chunks hi\u00e9rarchiques (niveau 0 \u00e0 3) Embedding : vecteur de dimension 768 stock\u00e9 via pgvector Auto-relation parent\u2013enfant pour reconstruire l'arborescence textuelle Index pour acc\u00e9l\u00e9rer les recherches par document et niveau hi\u00e9rarchique Hi\u00e9rarchie des chunks Niveau Description Usage 0 Document entier Aper\u00e7u global 1 Sections Structure principale 2 Paragraphes Contexte interm\u00e9diaire 3 Fragments Recherche pr\u00e9cise 2.3. IndexConfig class IndexConfig ( Base ): \"\"\"Configure les index vectoriels par corpus. Attrs: id (int): Identifiant unique de la configuration corpus_id (str): Identifiant du corpus concern\u00e9 index_type (str): Type d'index vectoriel ('ivfflat' ou 'hnsw') is_indexed (bool): Indique si l'index a \u00e9t\u00e9 cr\u00e9\u00e9 chunk_count (int): Nombre de fragments dans le corpus last_indexed (Date): Derni\u00e8re date d'indexation ivf_lists (int): Nombre de listes pour index IVFFLAT hnsw_m (int): Nombre de connexions par n\u0153ud pour HNSW hnsw_ef_construction (int): Facteur d'exploration pour HNSW \"\"\" __tablename__ = \"index_configs\" id = mapped_column ( Integer , primary_key = True ) corpus_id = mapped_column ( String ( 36 ), unique = True , nullable = False ) index_type = mapped_column ( String ( 20 ), default = \"ivfflat\" ) is_indexed = mapped_column ( Boolean , default = False ) chunk_count = mapped_column ( Integer , default = 0 ) last_indexed = mapped_column ( Date , nullable = True ) ivf_lists = mapped_column ( Integer , default = 100 ) hnsw_m = mapped_column ( Integer , default = 16 ) hnsw_ef_construction = mapped_column ( Integer , default = 200 ) Configuration vectorielle par corpus_id Types d'index disponibles : ivfflat (inverted file) - rapide pour corpus moyens, moins pr\u00e9cis hnsw (graph-based) - plus pr\u00e9cis, meilleur pour grands corpus Param\u00e8tres ajustables pour optimiser vitesse et pr\u00e9cision de recherche 3. Indexation vectorielle Principes de base Extension pgvector : int\u00e9gr\u00e9e \u00e0 PostgreSQL via CREATE EXTENSION vector Stockage : embeddings conserv\u00e9s dans des colonnes de type Vector(dimension) Index dynamiques : cr\u00e9ation SQL \u00e0 la demande selon la typologie du corpus Types d'index disponibles IVFFLAT (Inverted File with Flat Compression) CREATE INDEX idx_ivfflat ON chunks USING ivfflat ( embedding vector_cosine_ops ) WITH ( lists = 100 ) WHERE document_id IN ( SELECT id FROM documents WHERE corpus_id = 'corpus_xyz' ) Avantages : Rapide \u00e0 construire, efficace pour des volumes moyens (< 300K chunks) Param\u00e9trage : lists (nombre de clusters) - plus \u00e9lev\u00e9 = plus pr\u00e9cis mais plus lent HNSW (Hierarchical Navigable Small World) CREATE INDEX idx_hnsw ON chunks USING hnsw ( embedding vector_cosine_ops ) WITH ( m = 16 , ef_construction = 200 ) WHERE document_id IN ( SELECT id FROM documents WHERE corpus_id = 'corpus_xyz' ) Avantages : Plus pr\u00e9cis, excellentes performances m\u00eame avec des millions de vecteurs Param\u00e9trage : m (connexions par n\u0153ud) - \u00e9quilibre entre vitesse et pr\u00e9cision ef_construction (facteur d'exploration) - plus \u00e9lev\u00e9 = plus pr\u00e9cis mais plus lent \u00e0 construire 4. Sch\u00e9ma global \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u2551 Document \u2551 \u2551 (m\u00e9tadonn\u00e9es)\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u2551 \u2502 \u2551 1:N \u2502 1:N \u2551 \u25bc \u25bc \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u2551 Chunk \u2551 \u2551 Chunk \u2551 \u2551 Niveau 0-2 \u2551\u25c4\u2500\u2500\u2500\u2551 Niveau 3 \u2551 \u2551 (sections) \u2551 \u2551 (d\u00e9tails) \u2551 \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u2502 \u2502 \u25bc \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u2551 IndexConfig \u2551 \u2551 (par corpus)\u2551 \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d Document : contient uniquement les m\u00e9tadonn\u00e9es Chunks : stockent le contenu textuel hi\u00e9rarchique et les embeddings IndexConfig : param\u00e9trage des index vectoriels par corpus 5. Bonnes pratiques Performances \u00c9vitez les transactions longues avec des embeddings : consomment beaucoup de m\u00e9moire Cr\u00e9ez des index par corpus plut\u00f4t qu'un seul global Ajustez les param\u00e8tres selon votre volume : Petits corpus (< 50K chunks) : IVFFLAT avec 50-100 listes Corpus moyens (50K-300K) : IVFFLAT avec 100-300 listes Grands corpus (> 300K) : HNSW avec m=16, ef_construction=200 Monitoring Surveillez last_indexed et chunk_count pour d\u00e9tecter les d\u00e9rives de performance Reconstruisez les index si la recherche se d\u00e9grade ( VACUUM ANALYZE chunks ) S\u00e9curit\u00e9 Limitez les dimensions des vecteurs (768 ici) pour \u00e9viter la surcharge m\u00e9moire Utilisez des corpus_id en UUID pour l'isolation et la s\u00e9curit\u00e9 6. Installation et configuration Pr\u00e9requis PostgreSQL \u2265 14 Extension pgvector install\u00e9e Python \u2265 3.11 Installation sur openSUSE Tumbleweed (WSL) # Installer PostgreSQL et les d\u00e9pendances sudo zypper install postgresql14 postgresql14-server postgresql14-devel git gcc # Installer pgvector depuis les sources git clone https://github.com/pgvector/pgvector.git cd pgvector make sudo make install # Initialiser la base de donn\u00e9es PostgreSQL sudo systemctl enable postgresql sudo systemctl start postgresql sudo -u postgres createuser -s $USER createdb vectordb # Configurer le projet Python cd /chemin/vers/clea-api uv pip install -r requirements.txt # Initialiser la base de donn\u00e9es uv python -m vectordb.src.database Configuration minimale du fichier .env DB_USER=votre_utilisateur DB_PASSWORD=votre_mot_de_passe DB_HOST=localhost DB_PORT=5432 DB_NAME=vectordb Source : database.py Derni\u00e8re mise \u00e0 jour : 03 mai 2025","title":"Base de donn\u00e9es"},{"location":"database/#technologie-de-stockage-clea-api","text":"Ce document pr\u00e9sente l'architecture et les choix technologiques de la base de donn\u00e9es utilis\u00e9e par Clea-API . La solution repose sur PostgreSQL enrichi de l'extension pgvector , pilot\u00e9 via SQLAlchemy .","title":"Technologie de stockage Clea-API"},{"location":"database/#table-des-matieres","text":"Configuration et connexion Mod\u00e8les de donn\u00e9es Document Chunk IndexConfig Indexation vectorielle Sch\u00e9ma global Bonnes pratiques Installation et configuration","title":"Table des mati\u00e8res"},{"location":"database/#1-configuration-et-connexion","text":"Les param\u00e8tres de connexion sont lus depuis le fichier .env : DB_USER = os . getenv ( \"DB_USER\" , \"postgres\" ) DB_PASSWORD = os . getenv ( \"DB_PASSWORD\" , \"password\" ) DB_HOST = os . getenv ( \"DB_HOST\" , \"localhost\" ) DB_PORT = os . getenv ( \"DB_PORT\" , \"5432\" ) DB_NAME = os . getenv ( \"DB_NAME\" , \"vectordb\" ) DATABASE_URL = f \"postgresql:// { DB_USER } : { DB_PASSWORD } @ { DB_HOST } : { DB_PORT } / { DB_NAME } \"","title":"1. Configuration et connexion"},{"location":"database/#composants-principaux","text":"Engine SQLAlchemy : create_engine(DATABASE_URL) Session factory : SessionLocal = sessionmaker(bind=engine, autocommit=False, autoflush=False) Base d\u00e9clarative : Base = declarative_base()","title":"Composants principaux"},{"location":"database/#utilitaire-de-session","text":"def get_db (): \"\"\"Cr\u00e9e et retourne une session de base de donn\u00e9es. G\u00e9n\u00e8re une session SQLAlchemy compatible avec FastAPI \u00e0 utiliser comme d\u00e9pendance dans les endpoints. Yields: Session: Session SQLAlchemy pour les op\u00e9rations de base de donn\u00e9es \"\"\" db = SessionLocal () try : yield db finally : db . close ()","title":"Utilitaire de session"},{"location":"database/#initialisation","text":"def init_db (): \"\"\"Initialise la base de donn\u00e9es avec les tables et extensions n\u00e9cessaires. Cr\u00e9e l'extension pgvector si elle n'existe pas d\u00e9j\u00e0 et g\u00e9n\u00e8re toutes les tables d\u00e9finies dans les mod\u00e8les SQLAlchemy. \"\"\" with engine . connect () as conn : conn . execute ( text ( \"CREATE EXTENSION IF NOT EXISTS vector\" )) conn . commit () Base . metadata . create_all ( bind = engine )","title":"Initialisation"},{"location":"database/#2-modeles-de-donnees","text":"","title":"2. Mod\u00e8les de donn\u00e9es"},{"location":"database/#21-document","text":"class Document ( Base ): \"\"\"Stocke les m\u00e9tadonn\u00e9es globales des documents. Attrs: id (int): Identifiant unique du document title (str): Titre du document theme (str): Th\u00e8me ou cat\u00e9gorie du document document_type (str): Type de document (PDF, DOCX, etc.) publish_date (Date): Date de publication du document corpus_id (str): UUID du corpus auquel appartient ce document created_at (Date): Date d'ajout \u00e0 la base de donn\u00e9es index_needed (bool): Indique si une mise \u00e0 jour d'index est n\u00e9cessaire chunks (relationship): Relation avec les fragments textuels \"\"\" __tablename__ = \"documents\" id = mapped_column ( Integer , primary_key = True ) title = mapped_column ( String ( 255 ), nullable = False ) theme = mapped_column ( String ( 100 )) document_type = mapped_column ( String ( 100 )) publish_date = mapped_column ( Date ) corpus_id = mapped_column ( String ( 36 ), index = True ) created_at = mapped_column ( Date , default = datetime . now ) index_needed = mapped_column ( Boolean , default = False ) chunks = relationship ( \"Chunk\" , back_populates = \"document\" , cascade = \"all, delete-orphan\" ) __table_args__ = ( Index ( \"idx_document_theme\" , \"theme\" ), Index ( \"idx_document_type\" , \"document_type\" ), Index ( \"idx_document_date\" , \"publish_date\" ), Index ( \"idx_document_corpus\" , \"corpus_id\" ), ) M\u00e9tadonn\u00e9es : titre, th\u00e8me, type, date de publication, identifiant de corpus (UUID) Relation 1\u2013N vers les Chunk (cascade delete) Index SQL sur les colonnes fr\u00e9quemment utilis\u00e9es pour le filtrage","title":"2.1. Document"},{"location":"database/#22-chunk","text":"class Chunk ( Base ): \"\"\"Stocke les fragments de texte avec leurs embeddings vectoriels. Attrs: id (int): Identifiant unique du fragment document_id (int): R\u00e9f\u00e9rence au document parent content (str): Texte du fragment embedding (Vector): Repr\u00e9sentation vectorielle (768 dim) start_char (int): Position de d\u00e9but dans le document source end_char (int): Position de fin dans le document source hierarchy_level (int): Niveau hi\u00e9rarchique (0-3) parent_chunk_id (int): R\u00e9f\u00e9rence au fragment parent document (relationship): Relation avec le document parent parent (relationship): Relation avec le fragment parent children (relationship): Relation avec les fragments enfants \"\"\" __tablename__ = \"chunks\" id = mapped_column ( Integer , primary_key = True ) document_id = mapped_column ( Integer , ForeignKey ( \"documents.id\" , ondelete = \"CASCADE\" ), nullable = False ) content = mapped_column ( Text , nullable = False ) embedding = mapped_column ( Vector ( 768 )) start_char = mapped_column ( Integer ) end_char = mapped_column ( Integer ) hierarchy_level = mapped_column ( Integer , default = 3 ) parent_chunk_id = mapped_column ( Integer , ForeignKey ( \"chunks.id\" , ondelete = \"CASCADE\" )) document = relationship ( \"Document\" , back_populates = \"chunks\" ) parent = relationship ( \"Chunk\" , remote_side = [ id ], back_populates = \"children\" ) children = relationship ( \"Chunk\" , back_populates = \"parent\" , cascade = \"all, delete-orphan\" , single_parent = True ) __table_args__ = ( Index ( \"idx_chunk_document_level\" , \"document_id\" , \"hierarchy_level\" ), Index ( \"idx_chunk_parent\" , \"parent_chunk_id\" ), ) Contenu texte segment\u00e9 en chunks hi\u00e9rarchiques (niveau 0 \u00e0 3) Embedding : vecteur de dimension 768 stock\u00e9 via pgvector Auto-relation parent\u2013enfant pour reconstruire l'arborescence textuelle Index pour acc\u00e9l\u00e9rer les recherches par document et niveau hi\u00e9rarchique","title":"2.2. Chunk"},{"location":"database/#hierarchie-des-chunks","text":"Niveau Description Usage 0 Document entier Aper\u00e7u global 1 Sections Structure principale 2 Paragraphes Contexte interm\u00e9diaire 3 Fragments Recherche pr\u00e9cise","title":"Hi\u00e9rarchie des chunks"},{"location":"database/#23-indexconfig","text":"class IndexConfig ( Base ): \"\"\"Configure les index vectoriels par corpus. Attrs: id (int): Identifiant unique de la configuration corpus_id (str): Identifiant du corpus concern\u00e9 index_type (str): Type d'index vectoriel ('ivfflat' ou 'hnsw') is_indexed (bool): Indique si l'index a \u00e9t\u00e9 cr\u00e9\u00e9 chunk_count (int): Nombre de fragments dans le corpus last_indexed (Date): Derni\u00e8re date d'indexation ivf_lists (int): Nombre de listes pour index IVFFLAT hnsw_m (int): Nombre de connexions par n\u0153ud pour HNSW hnsw_ef_construction (int): Facteur d'exploration pour HNSW \"\"\" __tablename__ = \"index_configs\" id = mapped_column ( Integer , primary_key = True ) corpus_id = mapped_column ( String ( 36 ), unique = True , nullable = False ) index_type = mapped_column ( String ( 20 ), default = \"ivfflat\" ) is_indexed = mapped_column ( Boolean , default = False ) chunk_count = mapped_column ( Integer , default = 0 ) last_indexed = mapped_column ( Date , nullable = True ) ivf_lists = mapped_column ( Integer , default = 100 ) hnsw_m = mapped_column ( Integer , default = 16 ) hnsw_ef_construction = mapped_column ( Integer , default = 200 ) Configuration vectorielle par corpus_id Types d'index disponibles : ivfflat (inverted file) - rapide pour corpus moyens, moins pr\u00e9cis hnsw (graph-based) - plus pr\u00e9cis, meilleur pour grands corpus Param\u00e8tres ajustables pour optimiser vitesse et pr\u00e9cision de recherche","title":"2.3. IndexConfig"},{"location":"database/#3-indexation-vectorielle","text":"","title":"3. Indexation vectorielle"},{"location":"database/#principes-de-base","text":"Extension pgvector : int\u00e9gr\u00e9e \u00e0 PostgreSQL via CREATE EXTENSION vector Stockage : embeddings conserv\u00e9s dans des colonnes de type Vector(dimension) Index dynamiques : cr\u00e9ation SQL \u00e0 la demande selon la typologie du corpus","title":"Principes de base"},{"location":"database/#types-dindex-disponibles","text":"","title":"Types d'index disponibles"},{"location":"database/#ivfflat-inverted-file-with-flat-compression","text":"CREATE INDEX idx_ivfflat ON chunks USING ivfflat ( embedding vector_cosine_ops ) WITH ( lists = 100 ) WHERE document_id IN ( SELECT id FROM documents WHERE corpus_id = 'corpus_xyz' ) Avantages : Rapide \u00e0 construire, efficace pour des volumes moyens (< 300K chunks) Param\u00e9trage : lists (nombre de clusters) - plus \u00e9lev\u00e9 = plus pr\u00e9cis mais plus lent","title":"IVFFLAT (Inverted File with Flat Compression)"},{"location":"database/#hnsw-hierarchical-navigable-small-world","text":"CREATE INDEX idx_hnsw ON chunks USING hnsw ( embedding vector_cosine_ops ) WITH ( m = 16 , ef_construction = 200 ) WHERE document_id IN ( SELECT id FROM documents WHERE corpus_id = 'corpus_xyz' ) Avantages : Plus pr\u00e9cis, excellentes performances m\u00eame avec des millions de vecteurs Param\u00e9trage : m (connexions par n\u0153ud) - \u00e9quilibre entre vitesse et pr\u00e9cision ef_construction (facteur d'exploration) - plus \u00e9lev\u00e9 = plus pr\u00e9cis mais plus lent \u00e0 construire","title":"HNSW (Hierarchical Navigable Small World)"},{"location":"database/#4-schema-global","text":"\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u2551 Document \u2551 \u2551 (m\u00e9tadonn\u00e9es)\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u2551 \u2502 \u2551 1:N \u2502 1:N \u2551 \u25bc \u25bc \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u2551 Chunk \u2551 \u2551 Chunk \u2551 \u2551 Niveau 0-2 \u2551\u25c4\u2500\u2500\u2500\u2551 Niveau 3 \u2551 \u2551 (sections) \u2551 \u2551 (d\u00e9tails) \u2551 \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u2502 \u2502 \u25bc \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u2551 IndexConfig \u2551 \u2551 (par corpus)\u2551 \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d Document : contient uniquement les m\u00e9tadonn\u00e9es Chunks : stockent le contenu textuel hi\u00e9rarchique et les embeddings IndexConfig : param\u00e9trage des index vectoriels par corpus","title":"4. Sch\u00e9ma global"},{"location":"database/#5-bonnes-pratiques","text":"","title":"5. Bonnes pratiques"},{"location":"database/#performances","text":"\u00c9vitez les transactions longues avec des embeddings : consomment beaucoup de m\u00e9moire Cr\u00e9ez des index par corpus plut\u00f4t qu'un seul global Ajustez les param\u00e8tres selon votre volume : Petits corpus (< 50K chunks) : IVFFLAT avec 50-100 listes Corpus moyens (50K-300K) : IVFFLAT avec 100-300 listes Grands corpus (> 300K) : HNSW avec m=16, ef_construction=200","title":"Performances"},{"location":"database/#monitoring","text":"Surveillez last_indexed et chunk_count pour d\u00e9tecter les d\u00e9rives de performance Reconstruisez les index si la recherche se d\u00e9grade ( VACUUM ANALYZE chunks )","title":"Monitoring"},{"location":"database/#securite","text":"Limitez les dimensions des vecteurs (768 ici) pour \u00e9viter la surcharge m\u00e9moire Utilisez des corpus_id en UUID pour l'isolation et la s\u00e9curit\u00e9","title":"S\u00e9curit\u00e9"},{"location":"database/#6-installation-et-configuration","text":"","title":"6. Installation et configuration"},{"location":"database/#prerequis","text":"PostgreSQL \u2265 14 Extension pgvector install\u00e9e Python \u2265 3.11","title":"Pr\u00e9requis"},{"location":"database/#installation-sur-opensuse-tumbleweed-wsl","text":"# Installer PostgreSQL et les d\u00e9pendances sudo zypper install postgresql14 postgresql14-server postgresql14-devel git gcc # Installer pgvector depuis les sources git clone https://github.com/pgvector/pgvector.git cd pgvector make sudo make install # Initialiser la base de donn\u00e9es PostgreSQL sudo systemctl enable postgresql sudo systemctl start postgresql sudo -u postgres createuser -s $USER createdb vectordb # Configurer le projet Python cd /chemin/vers/clea-api uv pip install -r requirements.txt # Initialiser la base de donn\u00e9es uv python -m vectordb.src.database","title":"Installation sur openSUSE Tumbleweed (WSL)"},{"location":"database/#configuration-minimale-du-fichier-env","text":"DB_USER=votre_utilisateur DB_PASSWORD=votre_mot_de_passe DB_HOST=localhost DB_PORT=5432 DB_NAME=vectordb Source : database.py Derni\u00e8re mise \u00e0 jour : 03 mai 2025","title":"Configuration minimale du fichier .env"},{"location":"home/","text":"Cl\u00e9a-API Framework de chargement de documents et de recherche hybride (vectorielle + m\u00e9tadonn\u00e9es) pour PostgreSQL + pgvector. Il offre : endpoints REST pr\u00eats \u00e0 l'emploi ; classes Python modulaires ; pipeline d'ingestion complet ; documentation g\u00e9n\u00e9r\u00e9e avec MkDocs / mkdocstrings . Acc\u00e8s rapides \ud83d\udcda Sujet Documentation Chargement & extraction Extracteurs \u00b7 Segmentation Base de donn\u00e9es & index vectoriels Database Moteur de recherche hybride Search Pipeline end-to-end Pipeline R\u00e9f\u00e9rence API Python (autogen) Doc Loader \u00b7 Vectordb \u00b7 Pipeline OpenAPI / Endpoints REST REST API Caract\u00e9ristiques principales Chargement de documents : extraction multi-formats (PDF, DOCX, HTML, JSON, TXT\u2026). Voir Extracteurs . Recherche hybride : ANN pgvector + filtres SQL + re-ranking Cross-Encoder. Voir Pipeline de recherche . Segmentation hi\u00e9rarchique : Section \u2192 Paragraphe \u2192 Chunk ( adaptive segmentation ). Voir Splitter . Pipeline pr\u00eat \u00e0 l'emploi : une ligne pour ing\u00e9rer et indexer : from pipeline import process_and_store process_and_store ( \"report.pdf\" , theme = \"R&D\" ) D\u00e9tails : Pipeline . - Extensibilit\u00e9 : ajoutez un extracteur en deux \u00e9tapes ( BaseExtractor + mapping dans extractor_factory ). - Support PostgreSQL / pgvector : index ivfflat ou hnsw , cr\u00e9ation automatique par corpus. Structure du d\u00e9p\u00f4t . \u251c\u2500\u2500 doc_loader/ # Extraction & chargement \u251c\u2500\u2500 vectordb/ # Mod\u00e8les & recherche \u251c\u2500\u2500 pipeline/ # Orchestration end-to-end \u251c\u2500\u2500 demo/ # Exemples de fichiers \u251c\u2500\u2500 docs/ # Documentation MkDocs \u2190 vous \u00eates ici \u2514\u2500\u2500 ... Arborescence d\u00e9taill\u00e9e : voir Structure des extracteurs. Installation Cloner git clone https://github.com/votre-repo/clea-api.git cd clea-api D\u00e9pendances uv pip install -r requirements.txt Variables d'environnement DB_USER=postgres DB_PASSWORD=your_password DB_NAME=clea_db DB_HOST=localhost DB_PORT=5432 Initialiser la DB uv python -m clea_vectordb.init_db Lancer ./start.sh \u2192 API : http://localhost:8080 \u2013 Swagger dispo \u00e0 docs. Endpoints essentiels Op\u00e9ration M\u00e9thode/Path Doc Upload & extraction POST /doc_loader/upload-file Extracteurs Pipeline complet POST /pipeline/process-and-store Process and store CRUD Document POST /database/add_document CRUD Recherche hybride POST /search/hybrid_search Search Tests uv run pytest Tous les tests unitaires r\u00e9sident dans */test/ . D\u00e9ploiement Docker docker build -t clea-api . docker run -p 8080 :8080 clea-api Contribuer Fork \u2192 branche \u2192 PR. V\u00e9rifiez que pytest + mkdocs build passent. Merci ! \ud83d\udc9c Licence MIT \u2013 voir LICENSE.","title":"Accueil"},{"location":"home/#clea-api","text":"Framework de chargement de documents et de recherche hybride (vectorielle + m\u00e9tadonn\u00e9es) pour PostgreSQL + pgvector. Il offre : endpoints REST pr\u00eats \u00e0 l'emploi ; classes Python modulaires ; pipeline d'ingestion complet ; documentation g\u00e9n\u00e9r\u00e9e avec MkDocs / mkdocstrings .","title":"Cl\u00e9a-API"},{"location":"home/#acces-rapides","text":"Sujet Documentation Chargement & extraction Extracteurs \u00b7 Segmentation Base de donn\u00e9es & index vectoriels Database Moteur de recherche hybride Search Pipeline end-to-end Pipeline R\u00e9f\u00e9rence API Python (autogen) Doc Loader \u00b7 Vectordb \u00b7 Pipeline OpenAPI / Endpoints REST REST API","title":"Acc\u00e8s rapides \ud83d\udcda"},{"location":"home/#caracteristiques-principales","text":"Chargement de documents : extraction multi-formats (PDF, DOCX, HTML, JSON, TXT\u2026). Voir Extracteurs . Recherche hybride : ANN pgvector + filtres SQL + re-ranking Cross-Encoder. Voir Pipeline de recherche . Segmentation hi\u00e9rarchique : Section \u2192 Paragraphe \u2192 Chunk ( adaptive segmentation ). Voir Splitter . Pipeline pr\u00eat \u00e0 l'emploi : une ligne pour ing\u00e9rer et indexer : from pipeline import process_and_store process_and_store ( \"report.pdf\" , theme = \"R&D\" ) D\u00e9tails : Pipeline . - Extensibilit\u00e9 : ajoutez un extracteur en deux \u00e9tapes ( BaseExtractor + mapping dans extractor_factory ). - Support PostgreSQL / pgvector : index ivfflat ou hnsw , cr\u00e9ation automatique par corpus.","title":"Caract\u00e9ristiques principales"},{"location":"home/#structure-du-depot","text":". \u251c\u2500\u2500 doc_loader/ # Extraction & chargement \u251c\u2500\u2500 vectordb/ # Mod\u00e8les & recherche \u251c\u2500\u2500 pipeline/ # Orchestration end-to-end \u251c\u2500\u2500 demo/ # Exemples de fichiers \u251c\u2500\u2500 docs/ # Documentation MkDocs \u2190 vous \u00eates ici \u2514\u2500\u2500 ... Arborescence d\u00e9taill\u00e9e : voir Structure des extracteurs.","title":"Structure du d\u00e9p\u00f4t"},{"location":"home/#installation","text":"Cloner git clone https://github.com/votre-repo/clea-api.git cd clea-api D\u00e9pendances uv pip install -r requirements.txt Variables d'environnement DB_USER=postgres DB_PASSWORD=your_password DB_NAME=clea_db DB_HOST=localhost DB_PORT=5432 Initialiser la DB uv python -m clea_vectordb.init_db Lancer ./start.sh \u2192 API : http://localhost:8080 \u2013 Swagger dispo \u00e0 docs.","title":"Installation"},{"location":"home/#endpoints-essentiels","text":"Op\u00e9ration M\u00e9thode/Path Doc Upload & extraction POST /doc_loader/upload-file Extracteurs Pipeline complet POST /pipeline/process-and-store Process and store CRUD Document POST /database/add_document CRUD Recherche hybride POST /search/hybrid_search Search","title":"Endpoints essentiels"},{"location":"home/#tests","text":"uv run pytest Tous les tests unitaires r\u00e9sident dans */test/ .","title":"Tests"},{"location":"home/#deploiement-docker","text":"docker build -t clea-api . docker run -p 8080 :8080 clea-api","title":"D\u00e9ploiement Docker"},{"location":"home/#contribuer","text":"Fork \u2192 branche \u2192 PR. V\u00e9rifiez que pytest + mkdocs build passent. Merci ! \ud83d\udc9c","title":"Contribuer"},{"location":"home/#licence","text":"MIT \u2013 voir LICENSE.","title":"Licence"},{"location":"main/","text":"Point d\u2019entr\u00e9e principal : main.py Le fichier main.py constitue le d\u00e9marreur de tout le framework Cl\u00e9a-API , initialisant l\u2019environnement, la base de donn\u00e9es, les extensions, puis lan\u00e7ant l\u2019application FastAPI via Uvicorn. 1. Chargement de la configuration Charge les variables d\u2019environnement depuis .env (via python-dotenv ) Configure le logger global Lit les param\u00e8tres PostgreSQL et API (h\u00f4te, port, workers, niveau de log) :contentReference[oaicite:0]{index=0}:contentReference[oaicite:1]{index=1} load_dotenv () POSTGRES_USER = os . getenv ( \"DB_USER\" ) POSTGRES_PASSWORD = os . getenv ( \"DB_PASSWORD\" ) \u2026 API_HOST = os . getenv ( \"API_HOST\" , \"localhost\" ) API_PORT = int ( os . getenv ( \"API_PORT\" , 8080 )) API_WORKERS = int ( os . getenv ( \"API_WORKERS\" , 1 )) API_LOG_LEVEL = os . getenv ( \"API_LOG_LEVEL\" , \"info\" ) ```` --- ## 2. V\u00e9rification et d\u00e9marrage de PostgreSQL ### `start_postgres() \u2192 bool` 1. R\u00e9cup\u00e8re l \u2019 utilisateur courant ( ` get_current_user ` ) . 2. V\u00e9rifie la disponibilit\u00e9 de PostgreSQL ( ` check_postgres_status ` ) . 3. Si indisponible , affiche des ** suggestions d \u2019 installation ** pour Linux , notamment OpenSUSE Tumbleweed . 4. Retourne ` True ` si PostgreSQL est accessible , ` False ` sinon .& #x20; --- ## 3. Initialisation de la base de donn\u00e9es ### `setup_database() \u2192 bool` 1. Cr\u00e9e les tables SQLAlchemy ( ` Base . metadata . create_all ` ) et l \u2019 extension ** vector **. 2. Appelle ` init_db () ` pour installer pgvector et valider la cr\u00e9ation des tables . 3. Renvoie ` True ` si l \u2019 extension et les tables sont correctement cr\u00e9\u00e9es .& #x20; --- ## 4. Cycle de vie de l\u2019application ### `lifespan(app: FastAPI)` Gestionnaire ** asynchrone ** ex\u00e9cut\u00e9 au d\u00e9marrage et \u00e0 l \u2019 arr\u00eat : * ** Au d\u00e9marrage ** : 1. D\u00e9marre / v\u00e9rifie PostgreSQL via ` start_postgres ` . 2. V\u00e9rifie l \u2019 existence des tables ( ` verify_database_tables ` ), et lance ` setup_database ` si n\u00e9cessaire . * ** \u00c0 l \u2019 arr\u00eat ** : * Vide le cache de ressources globales .& #x20; --- ## 5. Cr\u00e9ation de l\u2019application FastAPI ``` python app = FastAPI ( title = \"Cl\u00e9a API\" , description = \"API pour g\u00e9rer les documents et effectuer des recherches s\u00e9mantiques.\" , version = \"0.1.1\" , docs_url = \"/docs\" , redoc_url = \"/redoc\" , lifespan = lifespan , ) Active Swagger UI ( /docs ) et ReDoc ( /redoc ). Associe le lifespan d\u00e9fini ci-dessus. 6. Middleware et routeurs CORS : autorise toutes les origines, m\u00e9thodes et headers ( CORSMiddleware ). Inclusion des routers : /database \u2192 crud documents & chunks /search \u2192 recherche hybride /index \u2192 gestion des index vectoriels /doc_loader \u2192 upload et extraction de documents /pipeline \u2192 traitement complet (extract \u2192 segment \u2192 insert) 7. Gestion des erreurs globales StarletteHTTPException \u2192 renvoie { \"error\": detail } RequestValidationError \u2192 renvoie { \"error\": \"Invalid request\", \"details\": [...] } Exception \u2192 capture toute erreur non g\u00e9r\u00e9e et renvoie 500 Internal server error . 8. Endpoints de sant\u00e9 @app . get ( \"/\" ) async def root (): return { \"message\" : \"Cl\u00e9a API is running\" } \u2013 V\u00e9rifie simplement que l\u2019API est en ligne. 9. Lancement du serveur Lorsque main.py est ex\u00e9cut\u00e9 directement ( if __name__ == \"__main__\" ), il : Cr\u00e9e une configuration Uvicorn optimis\u00e9e (reload, workers, proxy headers, choix auto de loop/http). D\u00e9marre le serveur avec uvicorn.Server(config).run() . python main.py # ou uvicorn main:app --host $API_HOST --port $API_PORT --workers $API_WORKERS Fichier source : main.py Derni\u00e8re mise \u00e0 jour : 03 mai 2025","title":"Introduction"},{"location":"main/#point-dentree-principal-mainpy","text":"Le fichier main.py constitue le d\u00e9marreur de tout le framework Cl\u00e9a-API , initialisant l\u2019environnement, la base de donn\u00e9es, les extensions, puis lan\u00e7ant l\u2019application FastAPI via Uvicorn.","title":"Point d\u2019entr\u00e9e principal : main.py"},{"location":"main/#1-chargement-de-la-configuration","text":"Charge les variables d\u2019environnement depuis .env (via python-dotenv ) Configure le logger global Lit les param\u00e8tres PostgreSQL et API (h\u00f4te, port, workers, niveau de log) :contentReference[oaicite:0]{index=0}:contentReference[oaicite:1]{index=1} load_dotenv () POSTGRES_USER = os . getenv ( \"DB_USER\" ) POSTGRES_PASSWORD = os . getenv ( \"DB_PASSWORD\" ) \u2026 API_HOST = os . getenv ( \"API_HOST\" , \"localhost\" ) API_PORT = int ( os . getenv ( \"API_PORT\" , 8080 )) API_WORKERS = int ( os . getenv ( \"API_WORKERS\" , 1 )) API_LOG_LEVEL = os . getenv ( \"API_LOG_LEVEL\" , \"info\" ) ```` --- ## 2. V\u00e9rification et d\u00e9marrage de PostgreSQL ### `start_postgres() \u2192 bool` 1. R\u00e9cup\u00e8re l \u2019 utilisateur courant ( ` get_current_user ` ) . 2. V\u00e9rifie la disponibilit\u00e9 de PostgreSQL ( ` check_postgres_status ` ) . 3. Si indisponible , affiche des ** suggestions d \u2019 installation ** pour Linux , notamment OpenSUSE Tumbleweed . 4. Retourne ` True ` si PostgreSQL est accessible , ` False ` sinon .& #x20; --- ## 3. Initialisation de la base de donn\u00e9es ### `setup_database() \u2192 bool` 1. Cr\u00e9e les tables SQLAlchemy ( ` Base . metadata . create_all ` ) et l \u2019 extension ** vector **. 2. Appelle ` init_db () ` pour installer pgvector et valider la cr\u00e9ation des tables . 3. Renvoie ` True ` si l \u2019 extension et les tables sont correctement cr\u00e9\u00e9es .& #x20; --- ## 4. Cycle de vie de l\u2019application ### `lifespan(app: FastAPI)` Gestionnaire ** asynchrone ** ex\u00e9cut\u00e9 au d\u00e9marrage et \u00e0 l \u2019 arr\u00eat : * ** Au d\u00e9marrage ** : 1. D\u00e9marre / v\u00e9rifie PostgreSQL via ` start_postgres ` . 2. V\u00e9rifie l \u2019 existence des tables ( ` verify_database_tables ` ), et lance ` setup_database ` si n\u00e9cessaire . * ** \u00c0 l \u2019 arr\u00eat ** : * Vide le cache de ressources globales .& #x20; --- ## 5. Cr\u00e9ation de l\u2019application FastAPI ``` python app = FastAPI ( title = \"Cl\u00e9a API\" , description = \"API pour g\u00e9rer les documents et effectuer des recherches s\u00e9mantiques.\" , version = \"0.1.1\" , docs_url = \"/docs\" , redoc_url = \"/redoc\" , lifespan = lifespan , ) Active Swagger UI ( /docs ) et ReDoc ( /redoc ). Associe le lifespan d\u00e9fini ci-dessus.","title":"1. Chargement de la configuration"},{"location":"main/#6-middleware-et-routeurs","text":"CORS : autorise toutes les origines, m\u00e9thodes et headers ( CORSMiddleware ). Inclusion des routers : /database \u2192 crud documents & chunks /search \u2192 recherche hybride /index \u2192 gestion des index vectoriels /doc_loader \u2192 upload et extraction de documents /pipeline \u2192 traitement complet (extract \u2192 segment \u2192 insert)","title":"6. Middleware et routeurs"},{"location":"main/#7-gestion-des-erreurs-globales","text":"StarletteHTTPException \u2192 renvoie { \"error\": detail } RequestValidationError \u2192 renvoie { \"error\": \"Invalid request\", \"details\": [...] } Exception \u2192 capture toute erreur non g\u00e9r\u00e9e et renvoie 500 Internal server error .","title":"7. Gestion des erreurs globales"},{"location":"main/#8-endpoints-de-sante","text":"@app . get ( \"/\" ) async def root (): return { \"message\" : \"Cl\u00e9a API is running\" } \u2013 V\u00e9rifie simplement que l\u2019API est en ligne.","title":"8. Endpoints de sant\u00e9"},{"location":"main/#9-lancement-du-serveur","text":"Lorsque main.py est ex\u00e9cut\u00e9 directement ( if __name__ == \"__main__\" ), il : Cr\u00e9e une configuration Uvicorn optimis\u00e9e (reload, workers, proxy headers, choix auto de loop/http). D\u00e9marre le serveur avec uvicorn.Server(config).run() . python main.py # ou uvicorn main:app --host $API_HOST --port $API_PORT --workers $API_WORKERS Fichier source : main.py Derni\u00e8re mise \u00e0 jour : 03 mai 2025","title":"9. Lancement du serveur"},{"location":"schemas/","text":"Schemas Pydantic de Clea-API Les sch\u00e9mas Pydantic d\u00e9finissent la forme des donn\u00e9es \u00e9chang\u00e9es avec l\u2019API (requ\u00eates et r\u00e9ponses). Tous les mod\u00e8les utilisent la configuration CamelConfig pour accepter et produire du camelCase . DocumentCreate Payload minimal pour cr\u00e9er un document (sans contenu). Champ Type Requis Alias Description title string Oui title Titre du document. theme string Oui theme Th\u00e8me du document. document_type string Oui documentType Type du document (PDF, TXT, etc.). publish_date date Oui publishDate Date de publication (YYYY-MM-DD). corpus_id string Non corpusId UUID du corpus (optionnel). type DocumentCreate = { title : string ; theme : string ; documentType : string ; publishDate : string ; // \"2025-05-01\" corpusId? : string ; } ```` --- ## ChunkCreate ** Payload ** pour cr\u00e9er un chunk ( fragment de texte et m\u00e9tadonn\u00e9es hi\u00e9rarchiques ). | Champ | Type | Requis | Alias | Description | | ----------------- | --------- | ------ | ---------------- | ------------------------------------------------------------ | | `id` | `number` | Non | `id` | Identifiant temporaire ( uniquement pour hi\u00e9rarchie interne ). | | `content` | `string` | Oui | `content` | Contenu textuel du chunk . | | `start_char` | `integer` | Oui | `startChar` | Position de d\u00e9but dans le texte source ( >= 0 ). | | `end_char` | `integer` | Oui | `endChar` | Position de fin dans le texte source ( > `startChar` ). | | `hierarchy_level` | `integer` | Oui | `hierarchyLevel` | Niveau hi\u00e9rarchique ( 0 \u2013 3 ). | | `parent_chunk_id` | `integer` | Non | `parentChunkId` | ID du chunk parent ( ou `null` ). | ```ts type ChunkCreate = { id?: number; content: string; startChar: number; endChar: number; hierarchyLevel: 0 | 1 | 2 | 3; parentChunkId?: number | null; } DocumentWithChunks Payload complet pour POST /database/documents . Champ Type Requis Alias Description document DocumentCreate Oui document M\u00e9tadonn\u00e9es du document. chunks ChunkCreate[] Oui chunks Liste des chunks \u00e0 ins\u00e9rer. type DocumentWithChunks = { document : DocumentCreate ; chunks : ChunkCreate []; } DocumentResponse R\u00e9ponse standard pour toutes les op\u00e9rations CRUD sur les documents. Champ Type Requis Alias Description id integer Oui id Identifiant du document. title string Oui title Titre du document. theme string Oui theme Th\u00e8me du document. document_type string Oui documentType Type du document. publish_date date Oui publishDate Date de publication. corpus_id string Non corpusId UUID du corpus. chunk_count integer Oui chunkCount Nombre de chunks associ\u00e9s (\u2265 0). index_needed boolean Oui indexNeeded true si un nouvel index est requis. type DocumentResponse = { id : number ; title : string ; theme : string ; documentType : string ; publishDate : string ; corpusId? : string ; chunkCount : number ; indexNeeded : boolean ; } DocumentUpdate Payload pour PUT /database/documents/{id} (mise \u00e0 jour). Champ Type Requis Alias Description id integer Oui id Identifiant du document \u00e0 mettre \u00e0 jour. title string Non title Nouveau titre. theme string Non theme Nouveau th\u00e8me. document_type string Non documentType Nouveau type de document. publish_date date Non publishDate Nouvelle date de publication. corpus_id string Non corpusId Nouvel UUID de corpus. type DocumentUpdate = { id : number ; title? : string ; theme? : string ; documentType? : string ; publishDate? : string ; corpusId? : string ; } UpdateWithChunks Payload pour PUT /database/documents/{id} avec ajout de chunks. Champ Type Requis Alias Description document DocumentUpdate Oui document M\u00e9tadonn\u00e9es \u00e0 mettre \u00e0 jour. new_chunks ChunkCreate[] Non newChunks Liste de nouveaux chunks \u00e0 ajouter. type UpdateWithChunks = { document : DocumentUpdate ; newChunks? : ChunkCreate []; } HierarchicalContext Parents (niveaux 0\u20132) renvoy\u00e9s quand hierarchical=true en recherche. Champ Type Requis Alias Description level_0 object or null Non level0 Chunk de section (lvl 0). level_1 object or null Non level1 Chunk de sous-section (lvl 1). level_2 object or null Non level2 Chunk de paragraphe (lvl 2). ChunkResult Un chunk renvoy\u00e9 par POST /search/hybrid_search . Champ Type Requis Alias Description chunk_id integer Oui chunkId Identifiant du chunk. document_id integer Oui documentId ID du document parent. title string Oui title Titre du document parent. content string Oui content Contenu du chunk. theme string Oui theme Th\u00e8me du document. document_type string Oui documentType Type du document. publish_date date Oui publishDate Date de publication. score number Oui score Score de similarit\u00e9 (distance ou score). hierarchy_level integer Oui hierarchyLevel Niveau hi\u00e9rarchique (0\u20133). context HierarchicalContext or null Non context Contexte parent (facultatif). SearchRequest Param\u00e8tres de la recherche hybride. Champ Type Requis Alias Description query string Oui query Texte de la requ\u00eate. top_k integer Non topK Nombre max de r\u00e9sultats (d\u00e9faut 10). theme string Non theme Filtrer sur le th\u00e8me. document_type string Non documentType Filtrer sur le type de document. start_date date Non startDate Filtrer date \u2265. end_date date Non endDate Filtrer date \u2264. corpus_id string Non corpusId Filtrer sur l\u2019UUID de corpus. hierarchical boolean Non hierarchical true pour renvoyer le contexte hi\u00e9rarchique. hierarchy_level integer Non hierarchyLevel Filtrer un niveau pr\u00e9cis (0\u20133). SearchResponse R\u00e9ponse compl\u00e8te du moteur de recherche. Champ Type Requis Alias Description query string Oui query Texte de la requ\u00eate. top_k integer Oui topK Nombre de r\u00e9sultats retourn\u00e9s. total_results integer Oui totalResults Nombre total de r\u00e9sultats trouv\u00e9s. results ChunkResult[] Oui results Liste des chunks class\u00e9s. IndexStatus Statut d\u2019indexation d\u2019un corpus. Champ Type Requis Alias Description corpus_id string Oui corpusId UUID du corpus interrog\u00e9. index_exists boolean Oui indexExists true si l\u2019index physique existe en base. config_exists boolean Oui configExists true si la config SQLAlchemy ( IndexConfig ) existe. is_indexed boolean Oui isIndexed true si la config est marqu\u00e9e index\u00e9e. index_type string Non indexType Type d\u2019index ( ivfflat , hnsw ). chunk_count integer Oui chunkCount Nombre total de chunks dans le corpus. indexed_chunks integer Oui indexedChunks Nombre de chunks effectivement index\u00e9s. last_indexed date Non lastIndexed Date du dernier index (ou null ). Tous les mod\u00e8les se trouvent dans vectordb/src/schemas.py Derni\u00e8re mise \u00e0 jour : 03 mai 2025 .","title":"Sch\u00e9mas"},{"location":"schemas/#schemas-pydantic-de-clea-api","text":"Les sch\u00e9mas Pydantic d\u00e9finissent la forme des donn\u00e9es \u00e9chang\u00e9es avec l\u2019API (requ\u00eates et r\u00e9ponses). Tous les mod\u00e8les utilisent la configuration CamelConfig pour accepter et produire du camelCase .","title":"Schemas Pydantic de Clea-API"},{"location":"schemas/#documentcreate","text":"Payload minimal pour cr\u00e9er un document (sans contenu). Champ Type Requis Alias Description title string Oui title Titre du document. theme string Oui theme Th\u00e8me du document. document_type string Oui documentType Type du document (PDF, TXT, etc.). publish_date date Oui publishDate Date de publication (YYYY-MM-DD). corpus_id string Non corpusId UUID du corpus (optionnel). type DocumentCreate = { title : string ; theme : string ; documentType : string ; publishDate : string ; // \"2025-05-01\" corpusId? : string ; } ```` --- ## ChunkCreate ** Payload ** pour cr\u00e9er un chunk ( fragment de texte et m\u00e9tadonn\u00e9es hi\u00e9rarchiques ). | Champ | Type | Requis | Alias | Description | | ----------------- | --------- | ------ | ---------------- | ------------------------------------------------------------ | | `id` | `number` | Non | `id` | Identifiant temporaire ( uniquement pour hi\u00e9rarchie interne ). | | `content` | `string` | Oui | `content` | Contenu textuel du chunk . | | `start_char` | `integer` | Oui | `startChar` | Position de d\u00e9but dans le texte source ( >= 0 ). | | `end_char` | `integer` | Oui | `endChar` | Position de fin dans le texte source ( > `startChar` ). | | `hierarchy_level` | `integer` | Oui | `hierarchyLevel` | Niveau hi\u00e9rarchique ( 0 \u2013 3 ). | | `parent_chunk_id` | `integer` | Non | `parentChunkId` | ID du chunk parent ( ou `null` ). | ```ts type ChunkCreate = { id?: number; content: string; startChar: number; endChar: number; hierarchyLevel: 0 | 1 | 2 | 3; parentChunkId?: number | null; }","title":"DocumentCreate"},{"location":"schemas/#documentwithchunks","text":"Payload complet pour POST /database/documents . Champ Type Requis Alias Description document DocumentCreate Oui document M\u00e9tadonn\u00e9es du document. chunks ChunkCreate[] Oui chunks Liste des chunks \u00e0 ins\u00e9rer. type DocumentWithChunks = { document : DocumentCreate ; chunks : ChunkCreate []; }","title":"DocumentWithChunks"},{"location":"schemas/#documentresponse","text":"R\u00e9ponse standard pour toutes les op\u00e9rations CRUD sur les documents. Champ Type Requis Alias Description id integer Oui id Identifiant du document. title string Oui title Titre du document. theme string Oui theme Th\u00e8me du document. document_type string Oui documentType Type du document. publish_date date Oui publishDate Date de publication. corpus_id string Non corpusId UUID du corpus. chunk_count integer Oui chunkCount Nombre de chunks associ\u00e9s (\u2265 0). index_needed boolean Oui indexNeeded true si un nouvel index est requis. type DocumentResponse = { id : number ; title : string ; theme : string ; documentType : string ; publishDate : string ; corpusId? : string ; chunkCount : number ; indexNeeded : boolean ; }","title":"DocumentResponse"},{"location":"schemas/#documentupdate","text":"Payload pour PUT /database/documents/{id} (mise \u00e0 jour). Champ Type Requis Alias Description id integer Oui id Identifiant du document \u00e0 mettre \u00e0 jour. title string Non title Nouveau titre. theme string Non theme Nouveau th\u00e8me. document_type string Non documentType Nouveau type de document. publish_date date Non publishDate Nouvelle date de publication. corpus_id string Non corpusId Nouvel UUID de corpus. type DocumentUpdate = { id : number ; title? : string ; theme? : string ; documentType? : string ; publishDate? : string ; corpusId? : string ; }","title":"DocumentUpdate"},{"location":"schemas/#updatewithchunks","text":"Payload pour PUT /database/documents/{id} avec ajout de chunks. Champ Type Requis Alias Description document DocumentUpdate Oui document M\u00e9tadonn\u00e9es \u00e0 mettre \u00e0 jour. new_chunks ChunkCreate[] Non newChunks Liste de nouveaux chunks \u00e0 ajouter. type UpdateWithChunks = { document : DocumentUpdate ; newChunks? : ChunkCreate []; }","title":"UpdateWithChunks"},{"location":"schemas/#hierarchicalcontext","text":"Parents (niveaux 0\u20132) renvoy\u00e9s quand hierarchical=true en recherche. Champ Type Requis Alias Description level_0 object or null Non level0 Chunk de section (lvl 0). level_1 object or null Non level1 Chunk de sous-section (lvl 1). level_2 object or null Non level2 Chunk de paragraphe (lvl 2).","title":"HierarchicalContext"},{"location":"schemas/#chunkresult","text":"Un chunk renvoy\u00e9 par POST /search/hybrid_search . Champ Type Requis Alias Description chunk_id integer Oui chunkId Identifiant du chunk. document_id integer Oui documentId ID du document parent. title string Oui title Titre du document parent. content string Oui content Contenu du chunk. theme string Oui theme Th\u00e8me du document. document_type string Oui documentType Type du document. publish_date date Oui publishDate Date de publication. score number Oui score Score de similarit\u00e9 (distance ou score). hierarchy_level integer Oui hierarchyLevel Niveau hi\u00e9rarchique (0\u20133). context HierarchicalContext or null Non context Contexte parent (facultatif).","title":"ChunkResult"},{"location":"schemas/#searchrequest","text":"Param\u00e8tres de la recherche hybride. Champ Type Requis Alias Description query string Oui query Texte de la requ\u00eate. top_k integer Non topK Nombre max de r\u00e9sultats (d\u00e9faut 10). theme string Non theme Filtrer sur le th\u00e8me. document_type string Non documentType Filtrer sur le type de document. start_date date Non startDate Filtrer date \u2265. end_date date Non endDate Filtrer date \u2264. corpus_id string Non corpusId Filtrer sur l\u2019UUID de corpus. hierarchical boolean Non hierarchical true pour renvoyer le contexte hi\u00e9rarchique. hierarchy_level integer Non hierarchyLevel Filtrer un niveau pr\u00e9cis (0\u20133).","title":"SearchRequest"},{"location":"schemas/#searchresponse","text":"R\u00e9ponse compl\u00e8te du moteur de recherche. Champ Type Requis Alias Description query string Oui query Texte de la requ\u00eate. top_k integer Oui topK Nombre de r\u00e9sultats retourn\u00e9s. total_results integer Oui totalResults Nombre total de r\u00e9sultats trouv\u00e9s. results ChunkResult[] Oui results Liste des chunks class\u00e9s.","title":"SearchResponse"},{"location":"schemas/#indexstatus","text":"Statut d\u2019indexation d\u2019un corpus. Champ Type Requis Alias Description corpus_id string Oui corpusId UUID du corpus interrog\u00e9. index_exists boolean Oui indexExists true si l\u2019index physique existe en base. config_exists boolean Oui configExists true si la config SQLAlchemy ( IndexConfig ) existe. is_indexed boolean Oui isIndexed true si la config est marqu\u00e9e index\u00e9e. index_type string Non indexType Type d\u2019index ( ivfflat , hnsw ). chunk_count integer Oui chunkCount Nombre total de chunks dans le corpus. indexed_chunks integer Oui indexedChunks Nombre de chunks effectivement index\u00e9s. last_indexed date Non lastIndexed Date du dernier index (ou null ). Tous les mod\u00e8les se trouvent dans vectordb/src/schemas.py Derni\u00e8re mise \u00e0 jour : 03 mai 2025 .","title":"IndexStatus"},{"location":"Integration/doc_loader/doc_loader_js/","text":"","title":"Extraction de documents"},{"location":"Integration/pipeline/pipeline_js/","text":"","title":"Pipeline end-to-end"},{"location":"Integration/vectordb/crud_js/","text":"","title":"Op\u00e9rations CRUD"},{"location":"Integration/vectordb/index_js/","text":"","title":"Indexation"},{"location":"Integration/vectordb/search_js/","text":"","title":"Recherche"},{"location":"api/lib/doc_loader/extractor_references/","text":"References for EXTRACTOR BaseExtractor Bases: ABC Interface abstraite pour tous les extracteurs de documents. Attributes: file_path ( Path ) \u2013 Chemin vers le fichier \u00e0 extraire. __init__ ( file_path ) Initialise un extracteur avec le chemin du fichier. Parameters: file_path ( str ) \u2013 Chemin vers le fichier \u00e0 extraire. extract_one ( * , max_length = 1000 ) abstractmethod Retourne un seul DocumentWithChunks pr\u00eat pour la DB. Parameters: max_length ( int , default: 1000 ) \u2013 Longueur maximale d'un chunk. D\u00e9faut \u00e0 1000. Returns: DocumentWithChunks ( DocumentWithChunks ) \u2013 Document avec ses chunks hi\u00e9rarchiques. Raises: NotImplementedError \u2013 Si la m\u00e9thode n'est pas impl\u00e9ment\u00e9e dans la classe d\u00e9riv\u00e9e. DocxExtractor Bases: BaseExtractor Convertit un fichier .docx en un unique payload DocumentWithChunks pr\u00eat \u00e0 \u00eatre POST\u00e9 sur /database/documents . __init__ ( file_path ) Initialise l'extracteur DOCX avec les m\u00e9tadonn\u00e9es par d\u00e9faut. Parameters: file_path ( str ) \u2013 Chemin vers le fichier DOCX. Raises: ValueError \u2013 Si le fichier DOCX ne peut pas \u00eatre ouvert. extract_one ( max_length = 1000 ) Extrait un seul objet DocumentWithChunks \u00e0 partir du fichier DOCX. Parameters: max_length ( int , default: 1000 ) \u2013 Longueur cible pour les segments finaux. Returns: DocumentWithChunks ( DocumentWithChunks ) \u2013 Objet contenant les chunks et m\u00e9tadonn\u00e9es. HtmlExtractor Bases: BaseExtractor Convertit n\u2019importe quel document HTML en un ou plusieurs payloads DocumentWithChunks pr\u00eats pour l\u2019endpoint POST /database/documents . Le titre du <title> ou du nom de fichier est utilis\u00e9 comme document.title . Tout le texte visible (sans balises) est extrait. __init__ ( file_path ) Initialise l'extracteur HTML avec les m\u00e9tadonn\u00e9es par d\u00e9faut. Parameters: file_path ( str ) \u2013 Chemin vers le fichier HTML. Raises: ValueError \u2013 Si le fichier HTML ne peut pas \u00eatre lu. extract_one ( max_length = 1000 ) Extrait un seul objet DocumentWithChunks \u00e0 partir du fichier HTML. Parameters: max_length ( int , default: 1000 ) \u2013 Longueur cible pour les segments finaux. Returns: DocumentWithChunks ( DocumentWithChunks ) \u2013 Objet contenant les chunks et m\u00e9tadonn\u00e9es. Raises: ValueError \u2013 Si aucun contenu n'est extrait du document HTML. iter_text () Renvoie le texte brut, ligne par ligne (utile pour le stream ). Returns: Iterator [ str ] \u2013 Iterator[str]: Texte brut extrait du document HTML. JsonExtractor Bases: BaseExtractor Extrait des contenus stock\u00e9s dans un fichier JSON . Le fichier doit \u00eatre une liste de dictionnaires : [ {\"title\": \"...\", \"content\": \"...\", \"theme\": \"...\", ...}, ... ] Chaque entr\u00e9e g\u00e9n\u00e8re un DocumentWithChunks pr\u00eat \u00e0 \u00eatre envoy\u00e9 \u00e0 l\u2019endpoint POST /database/documents . __init__ ( file_path ) Initialise l'extracteur JSON avec les m\u00e9tadonn\u00e9es par d\u00e9faut. Parameters: file_path ( str ) \u2013 Chemin vers le fichier JSON. Raises: ValueError \u2013 Si le fichier JSON ne peut pas \u00eatre ouvert ou est vide. PdfExtractor Bases: BaseExtractor Convertit un fichier .pdf en un unique payload DocumentWithChunks , pr\u00eat \u00e0 \u00eatre ins\u00e9r\u00e9 via /database/documents . extract_one ( max_length = 1000 ) Extrait un seul objet DocumentWithChunks \u00e0 partir du fichier PDF. Parameters: max_length ( int , default: 1000 ) \u2013 Longueur cible pour les segments finaux. Returns: DocumentWithChunks ( DocumentWithChunks ) \u2013 Objet contenant les chunks et m\u00e9tadonn\u00e9es. TxtExtractor Bases: BaseExtractor Convertit un fichier .txt en un unique payload DocumentWithChunks , pr\u00eat \u00e0 \u00eatre envoy\u00e9 vers /database/documents . extract_one ( max_length = 1000 ) Extrait un seul objet DocumentWithChunks \u00e0 partir du fichier TXT. Parameters: max_length ( int , default: 1000 ) \u2013 Longueur cible pour les segments finaux. Returns: DocumentWithChunks ( DocumentWithChunks ) \u2013 Objet contenant les chunks et m\u00e9tadonn\u00e9es. Raises: ValueError \u2013 Si le fichier est vide ou si aucune m\u00e9tadonn\u00e9e valide n'est trouv\u00e9e.","title":"Extracteurs"},{"location":"api/lib/doc_loader/extractor_references/#references-for-extractor","text":"","title":"References for EXTRACTOR"},{"location":"api/lib/doc_loader/extractor_references/#doc_loader.src.data_extractor.BaseExtractor","text":"Bases: ABC Interface abstraite pour tous les extracteurs de documents. Attributes: file_path ( Path ) \u2013 Chemin vers le fichier \u00e0 extraire.","title":"BaseExtractor"},{"location":"api/lib/doc_loader/extractor_references/#doc_loader.src.data_extractor.BaseExtractor.__init__","text":"Initialise un extracteur avec le chemin du fichier. Parameters: file_path ( str ) \u2013 Chemin vers le fichier \u00e0 extraire.","title":"__init__"},{"location":"api/lib/doc_loader/extractor_references/#doc_loader.src.data_extractor.BaseExtractor.extract_one","text":"Retourne un seul DocumentWithChunks pr\u00eat pour la DB. Parameters: max_length ( int , default: 1000 ) \u2013 Longueur maximale d'un chunk. D\u00e9faut \u00e0 1000. Returns: DocumentWithChunks ( DocumentWithChunks ) \u2013 Document avec ses chunks hi\u00e9rarchiques. Raises: NotImplementedError \u2013 Si la m\u00e9thode n'est pas impl\u00e9ment\u00e9e dans la classe d\u00e9riv\u00e9e.","title":"extract_one"},{"location":"api/lib/doc_loader/extractor_references/#doc_loader.src.data_extractor.DocxExtractor","text":"Bases: BaseExtractor Convertit un fichier .docx en un unique payload DocumentWithChunks pr\u00eat \u00e0 \u00eatre POST\u00e9 sur /database/documents .","title":"DocxExtractor"},{"location":"api/lib/doc_loader/extractor_references/#doc_loader.src.data_extractor.DocxExtractor.__init__","text":"Initialise l'extracteur DOCX avec les m\u00e9tadonn\u00e9es par d\u00e9faut. Parameters: file_path ( str ) \u2013 Chemin vers le fichier DOCX. Raises: ValueError \u2013 Si le fichier DOCX ne peut pas \u00eatre ouvert.","title":"__init__"},{"location":"api/lib/doc_loader/extractor_references/#doc_loader.src.data_extractor.DocxExtractor.extract_one","text":"Extrait un seul objet DocumentWithChunks \u00e0 partir du fichier DOCX. Parameters: max_length ( int , default: 1000 ) \u2013 Longueur cible pour les segments finaux. Returns: DocumentWithChunks ( DocumentWithChunks ) \u2013 Objet contenant les chunks et m\u00e9tadonn\u00e9es.","title":"extract_one"},{"location":"api/lib/doc_loader/extractor_references/#doc_loader.src.data_extractor.HtmlExtractor","text":"Bases: BaseExtractor Convertit n\u2019importe quel document HTML en un ou plusieurs payloads DocumentWithChunks pr\u00eats pour l\u2019endpoint POST /database/documents . Le titre du <title> ou du nom de fichier est utilis\u00e9 comme document.title . Tout le texte visible (sans balises) est extrait.","title":"HtmlExtractor"},{"location":"api/lib/doc_loader/extractor_references/#doc_loader.src.data_extractor.HtmlExtractor.__init__","text":"Initialise l'extracteur HTML avec les m\u00e9tadonn\u00e9es par d\u00e9faut. Parameters: file_path ( str ) \u2013 Chemin vers le fichier HTML. Raises: ValueError \u2013 Si le fichier HTML ne peut pas \u00eatre lu.","title":"__init__"},{"location":"api/lib/doc_loader/extractor_references/#doc_loader.src.data_extractor.HtmlExtractor.extract_one","text":"Extrait un seul objet DocumentWithChunks \u00e0 partir du fichier HTML. Parameters: max_length ( int , default: 1000 ) \u2013 Longueur cible pour les segments finaux. Returns: DocumentWithChunks ( DocumentWithChunks ) \u2013 Objet contenant les chunks et m\u00e9tadonn\u00e9es. Raises: ValueError \u2013 Si aucun contenu n'est extrait du document HTML.","title":"extract_one"},{"location":"api/lib/doc_loader/extractor_references/#doc_loader.src.data_extractor.HtmlExtractor.iter_text","text":"Renvoie le texte brut, ligne par ligne (utile pour le stream ). Returns: Iterator [ str ] \u2013 Iterator[str]: Texte brut extrait du document HTML.","title":"iter_text"},{"location":"api/lib/doc_loader/extractor_references/#doc_loader.src.data_extractor.JsonExtractor","text":"Bases: BaseExtractor Extrait des contenus stock\u00e9s dans un fichier JSON . Le fichier doit \u00eatre une liste de dictionnaires : [ {\"title\": \"...\", \"content\": \"...\", \"theme\": \"...\", ...}, ... ] Chaque entr\u00e9e g\u00e9n\u00e8re un DocumentWithChunks pr\u00eat \u00e0 \u00eatre envoy\u00e9 \u00e0 l\u2019endpoint POST /database/documents .","title":"JsonExtractor"},{"location":"api/lib/doc_loader/extractor_references/#doc_loader.src.data_extractor.JsonExtractor.__init__","text":"Initialise l'extracteur JSON avec les m\u00e9tadonn\u00e9es par d\u00e9faut. Parameters: file_path ( str ) \u2013 Chemin vers le fichier JSON. Raises: ValueError \u2013 Si le fichier JSON ne peut pas \u00eatre ouvert ou est vide.","title":"__init__"},{"location":"api/lib/doc_loader/extractor_references/#doc_loader.src.data_extractor.PdfExtractor","text":"Bases: BaseExtractor Convertit un fichier .pdf en un unique payload DocumentWithChunks , pr\u00eat \u00e0 \u00eatre ins\u00e9r\u00e9 via /database/documents .","title":"PdfExtractor"},{"location":"api/lib/doc_loader/extractor_references/#doc_loader.src.data_extractor.PdfExtractor.extract_one","text":"Extrait un seul objet DocumentWithChunks \u00e0 partir du fichier PDF. Parameters: max_length ( int , default: 1000 ) \u2013 Longueur cible pour les segments finaux. Returns: DocumentWithChunks ( DocumentWithChunks ) \u2013 Objet contenant les chunks et m\u00e9tadonn\u00e9es.","title":"extract_one"},{"location":"api/lib/doc_loader/extractor_references/#doc_loader.src.data_extractor.TxtExtractor","text":"Bases: BaseExtractor Convertit un fichier .txt en un unique payload DocumentWithChunks , pr\u00eat \u00e0 \u00eatre envoy\u00e9 vers /database/documents .","title":"TxtExtractor"},{"location":"api/lib/doc_loader/extractor_references/#doc_loader.src.data_extractor.TxtExtractor.extract_one","text":"Extrait un seul objet DocumentWithChunks \u00e0 partir du fichier TXT. Parameters: max_length ( int , default: 1000 ) \u2013 Longueur cible pour les segments finaux. Returns: DocumentWithChunks ( DocumentWithChunks ) \u2013 Objet contenant les chunks et m\u00e9tadonn\u00e9es. Raises: ValueError \u2013 Si le fichier est vide ou si aucune m\u00e9tadonn\u00e9e valide n'est trouv\u00e9e.","title":"extract_one"},{"location":"api/lib/doc_loader/splitter_references/","text":"References for SPLITTER Package splitter pour la segmentation s\u00e9mantique des documents. Fournit les outils pour segmenter des documents textuels en chunks hi\u00e9rarchiques optimis\u00e9s pour la recherche vectorielle. fallback_segmentation_stream ( text , max_length ) Version streaming de la segmentation de secours pour \u00e9conomiser de la m\u00e9moire. Parameters: text ( str ) \u2013 Texte \u00e0 segmenter. max_length ( int ) \u2013 Longueur maximale d'un chunk. Yields: ChunkCreate ( ChunkCreate ) \u2013 Les chunks g\u00e9n\u00e9r\u00e9s un par un. semantic_segmentation_stream ( text , max_length ) G\u00e9n\u00e8re les chunks s\u00e9mantiques d'un document au fil de l'eau. Version optimis\u00e9e pour les grands corpus non structur\u00e9s avec une meilleure extraction des fronti\u00e8res naturelles de texte et davantage de chunks. Parameters: text ( str ) \u2013 Texte \u00e0 segmenter. max_length ( int ) \u2013 Longueur maximale d'un chunk. Yields: ChunkCreate ( ChunkCreate ) \u2013 Un chunk \u00e0 la fois, g\u00e9n\u00e9r\u00e9 dans l'ordre hi\u00e9rarchique.","title":"Segmentation"},{"location":"api/lib/doc_loader/splitter_references/#references-for-splitter","text":"Package splitter pour la segmentation s\u00e9mantique des documents. Fournit les outils pour segmenter des documents textuels en chunks hi\u00e9rarchiques optimis\u00e9s pour la recherche vectorielle.","title":"References for SPLITTER"},{"location":"api/lib/doc_loader/splitter_references/#doc_loader.src.splitter.fallback_segmentation_stream","text":"Version streaming de la segmentation de secours pour \u00e9conomiser de la m\u00e9moire. Parameters: text ( str ) \u2013 Texte \u00e0 segmenter. max_length ( int ) \u2013 Longueur maximale d'un chunk. Yields: ChunkCreate ( ChunkCreate ) \u2013 Les chunks g\u00e9n\u00e9r\u00e9s un par un.","title":"fallback_segmentation_stream"},{"location":"api/lib/doc_loader/splitter_references/#doc_loader.src.splitter.semantic_segmentation_stream","text":"G\u00e9n\u00e8re les chunks s\u00e9mantiques d'un document au fil de l'eau. Version optimis\u00e9e pour les grands corpus non structur\u00e9s avec une meilleure extraction des fronti\u00e8res naturelles de texte et davantage de chunks. Parameters: text ( str ) \u2013 Texte \u00e0 segmenter. max_length ( int ) \u2013 Longueur maximale d'un chunk. Yields: ChunkCreate ( ChunkCreate ) \u2013 Un chunk \u00e0 la fois, g\u00e9n\u00e9r\u00e9 dans l'ordre hi\u00e9rarchique.","title":"semantic_segmentation_stream"},{"location":"api/lib/pipeline/pipeline_references/","text":"References for PIPELINE determine_document_type ( file_path ) D\u00e9termine le type de document \u00e0 partir du chemin du fichier. Parameters: file_path ( str ) \u2013 Chemin complet vers le fichier. Returns: str \u2013 Type de document d\u00e9tect\u00e9 en fonction de l'extension. process_and_store ( file_path , max_length = 500 , overlap = 100 , theme = 'Th\u00e8me g\u00e9n\u00e9rique' , corpus_id = None ) Traite un fichier et l'ins\u00e8re dans la base de donn\u00e9es avec une structure hi\u00e9rarchique. Cette fonction effectue l'extraction du texte, la segmentation hi\u00e9rarchique et l'insertion en base de donn\u00e9es avec gestion des embeddings. Parameters: file_path ( str ) \u2013 Chemin du fichier \u00e0 traiter. max_length ( int , default: 500 ) \u2013 Taille maximale d'un chunk final. Par d\u00e9faut \u00e0 500 caract\u00e8res. overlap ( int , default: 100 ) \u2013 Chevauchement entre les chunks. Par d\u00e9faut \u00e0 100 caract\u00e8res. theme ( Optional [ str ] , default: 'Th\u00e8me g\u00e9n\u00e9rique' ) \u2013 Th\u00e8me \u00e0 appliquer au document. Par d\u00e9faut \"Th\u00e8me g\u00e9n\u00e9rique\". document_type \u2013 Type du document (d\u00e9termin\u00e9 automatiquement si None). corpus_id ( Optional [ str ] , default: None ) \u2013 Identifiant du corpus (g\u00e9n\u00e9r\u00e9 automatiquement si None). Returns: Dict [ str , Any ] \u2013 Dict[str, Any]: R\u00e9sultat de l'op\u00e9ration avec l'ID du document et les statistiques. Raises: FileNotFoundError \u2013 Si le fichier sp\u00e9cifi\u00e9 n'existe pas. ValueError \u2013 Si aucun contenu n'a pu \u00eatre extrait du document ou si une erreur survient lors de l'insertion en base.","title":"Pipeline"},{"location":"api/lib/pipeline/pipeline_references/#references-for-pipeline","text":"","title":"References for PIPELINE"},{"location":"api/lib/pipeline/pipeline_references/#pipeline.src.pipeline.determine_document_type","text":"D\u00e9termine le type de document \u00e0 partir du chemin du fichier. Parameters: file_path ( str ) \u2013 Chemin complet vers le fichier. Returns: str \u2013 Type de document d\u00e9tect\u00e9 en fonction de l'extension.","title":"determine_document_type"},{"location":"api/lib/pipeline/pipeline_references/#pipeline.src.pipeline.process_and_store","text":"Traite un fichier et l'ins\u00e8re dans la base de donn\u00e9es avec une structure hi\u00e9rarchique. Cette fonction effectue l'extraction du texte, la segmentation hi\u00e9rarchique et l'insertion en base de donn\u00e9es avec gestion des embeddings. Parameters: file_path ( str ) \u2013 Chemin du fichier \u00e0 traiter. max_length ( int , default: 500 ) \u2013 Taille maximale d'un chunk final. Par d\u00e9faut \u00e0 500 caract\u00e8res. overlap ( int , default: 100 ) \u2013 Chevauchement entre les chunks. Par d\u00e9faut \u00e0 100 caract\u00e8res. theme ( Optional [ str ] , default: 'Th\u00e8me g\u00e9n\u00e9rique' ) \u2013 Th\u00e8me \u00e0 appliquer au document. Par d\u00e9faut \"Th\u00e8me g\u00e9n\u00e9rique\". document_type \u2013 Type du document (d\u00e9termin\u00e9 automatiquement si None). corpus_id ( Optional [ str ] , default: None ) \u2013 Identifiant du corpus (g\u00e9n\u00e9r\u00e9 automatiquement si None). Returns: Dict [ str , Any ] \u2013 Dict[str, Any]: R\u00e9sultat de l'op\u00e9ration avec l'ID du document et les statistiques. Raises: FileNotFoundError \u2013 Si le fichier sp\u00e9cifi\u00e9 n'existe pas. ValueError \u2013 Si aucun contenu n'a pu \u00eatre extrait du document ou si une erreur survient lors de l'insertion en base.","title":"process_and_store"},{"location":"api/lib/vectordb/crud_references/","text":"References for CRUD add_document_with_chunks ( db , doc , chunks , batch_size = 10 ) Ajoute un document et ses chunks \u00e0 la base de donn\u00e9es en g\u00e9n\u00e9rant les embeddings en mode lot. Cette fonction ins\u00e8re un document et tous ses chunks associ\u00e9s, g\u00e9n\u00e8re les embeddings par lots pour am\u00e9liorer les performances, et pr\u00e9serve les relations hi\u00e9rarchiques entre les chunks. Parameters: db ( Session ) \u2013 Session SQLAlchemy active. doc ( DocumentCreate ) \u2013 Donn\u00e9es du document \u00e0 cr\u00e9er. chunks ( List [ Dict [ str , Any ]] ) \u2013 Liste des chunks d\u00e9finis par l'utilisateur. batch_size ( int , default: 10 ) \u2013 Nombre de chunks \u00e0 traiter par lot. Returns: Dict [ str , Any ] \u2013 Dict contenant l'ID du document, le nombre de chunks, le corpus_id et si un index doit \u00eatre cr\u00e9\u00e9. delete_document ( document_id ) Supprime un document de la base de donn\u00e9es avec tous ses chunks associ\u00e9s. Parameters: document_id ( int ) \u2013 ID du document \u00e0 supprimer. Returns: Dict [ str , Any ] \u2013 Dict[str, Any]: R\u00e9sultat de l'op\u00e9ration. delete_document_chunks ( document_id , chunk_ids = None ) Supprime des chunks sp\u00e9cifiques d'un document ou tous les chunks si aucun ID n'est sp\u00e9cifi\u00e9. Parameters: document_id ( int ) \u2013 ID du document. chunk_ids ( Optional [ List [ int ]] , default: None ) \u2013 Liste des IDs des chunks \u00e0 supprimer (si None, supprime tous les chunks). Returns: Dict [ str , Any ] \u2013 Dict[str, Any]: R\u00e9sultat de l'op\u00e9ration. update_document_with_chunks ( document_update , new_chunks = None ) Met \u00e0 jour un document existant et ses chunks dans la base de donn\u00e9es. Parameters: document_update ( DocumentUpdate ) \u2013 Donn\u00e9es de mise \u00e0 jour du document. new_chunks ( Optional [ List [ Dict [ str , Any ]]] , default: None ) \u2013 Nouveaux chunks \u00e0 ajouter ou \u00e0 remplacer (si replace_chunks=True). Returns: Dict [ str , Any ] \u2013 Dict[str, Any]: R\u00e9sultat de l'op\u00e9ration avec les informations mises \u00e0 jour.","title":"CRUD"},{"location":"api/lib/vectordb/crud_references/#references-for-crud","text":"","title":"References for CRUD"},{"location":"api/lib/vectordb/crud_references/#vectordb.src.crud.add_document_with_chunks","text":"Ajoute un document et ses chunks \u00e0 la base de donn\u00e9es en g\u00e9n\u00e9rant les embeddings en mode lot. Cette fonction ins\u00e8re un document et tous ses chunks associ\u00e9s, g\u00e9n\u00e8re les embeddings par lots pour am\u00e9liorer les performances, et pr\u00e9serve les relations hi\u00e9rarchiques entre les chunks. Parameters: db ( Session ) \u2013 Session SQLAlchemy active. doc ( DocumentCreate ) \u2013 Donn\u00e9es du document \u00e0 cr\u00e9er. chunks ( List [ Dict [ str , Any ]] ) \u2013 Liste des chunks d\u00e9finis par l'utilisateur. batch_size ( int , default: 10 ) \u2013 Nombre de chunks \u00e0 traiter par lot. Returns: Dict [ str , Any ] \u2013 Dict contenant l'ID du document, le nombre de chunks, le corpus_id et si un index doit \u00eatre cr\u00e9\u00e9.","title":"add_document_with_chunks"},{"location":"api/lib/vectordb/crud_references/#vectordb.src.crud.delete_document","text":"Supprime un document de la base de donn\u00e9es avec tous ses chunks associ\u00e9s. Parameters: document_id ( int ) \u2013 ID du document \u00e0 supprimer. Returns: Dict [ str , Any ] \u2013 Dict[str, Any]: R\u00e9sultat de l'op\u00e9ration.","title":"delete_document"},{"location":"api/lib/vectordb/crud_references/#vectordb.src.crud.delete_document_chunks","text":"Supprime des chunks sp\u00e9cifiques d'un document ou tous les chunks si aucun ID n'est sp\u00e9cifi\u00e9. Parameters: document_id ( int ) \u2013 ID du document. chunk_ids ( Optional [ List [ int ]] , default: None ) \u2013 Liste des IDs des chunks \u00e0 supprimer (si None, supprime tous les chunks). Returns: Dict [ str , Any ] \u2013 Dict[str, Any]: R\u00e9sultat de l'op\u00e9ration.","title":"delete_document_chunks"},{"location":"api/lib/vectordb/crud_references/#vectordb.src.crud.update_document_with_chunks","text":"Met \u00e0 jour un document existant et ses chunks dans la base de donn\u00e9es. Parameters: document_update ( DocumentUpdate ) \u2013 Donn\u00e9es de mise \u00e0 jour du document. new_chunks ( Optional [ List [ Dict [ str , Any ]]] , default: None ) \u2013 Nouveaux chunks \u00e0 ajouter ou \u00e0 remplacer (si replace_chunks=True). Returns: Dict [ str , Any ] \u2013 Dict[str, Any]: R\u00e9sultat de l'op\u00e9ration avec les informations mises \u00e0 jour.","title":"update_document_with_chunks"},{"location":"api/lib/vectordb/index_references/","text":"References for INDEX Module de gestion des index vectoriels pour pgvector. Ce module fournit des fonctions simples pour cr\u00e9er et g\u00e9rer des index vectoriels sur les chunks de documents. check_all_indexes () V\u00e9rifie l'\u00e9tat de tous les index vectoriels. Returns: dict ( dict ) \u2013 \u00c9tat des index pour tous les corpus. check_index_status ( corpus_id ) V\u00e9rifie l'\u00e9tat de l'index pour un corpus. Parameters: corpus_id ( str ) \u2013 Identifiant UUID du corpus. Returns: dict ( dict ) \u2013 \u00c9tat de l'index et m\u00e9tadonn\u00e9es. create_simple_index ( corpus_id ) Cr\u00e9e un index vectoriel simple pour un corpus donn\u00e9. Cette fonction cr\u00e9e un index IVFFLAT standard pour acc\u00e9l\u00e9rer les recherches vectorielles sur un corpus sp\u00e9cifique. Parameters: corpus_id ( str ) \u2013 Identifiant UUID du corpus \u00e0 indexer. Returns: dict ( dict ) \u2013 R\u00e9sultat de l'op\u00e9ration avec statut et message. drop_index ( corpus_id ) Supprime l'index vectoriel pour un corpus. Parameters: corpus_id ( str ) \u2013 Identifiant UUID du corpus. Returns: dict ( dict ) \u2013 R\u00e9sultat de l'op\u00e9ration.","title":"Indexation"},{"location":"api/lib/vectordb/index_references/#references-for-index","text":"Module de gestion des index vectoriels pour pgvector. Ce module fournit des fonctions simples pour cr\u00e9er et g\u00e9rer des index vectoriels sur les chunks de documents.","title":"References for INDEX"},{"location":"api/lib/vectordb/index_references/#vectordb.src.index_manager.check_all_indexes","text":"V\u00e9rifie l'\u00e9tat de tous les index vectoriels. Returns: dict ( dict ) \u2013 \u00c9tat des index pour tous les corpus.","title":"check_all_indexes"},{"location":"api/lib/vectordb/index_references/#vectordb.src.index_manager.check_index_status","text":"V\u00e9rifie l'\u00e9tat de l'index pour un corpus. Parameters: corpus_id ( str ) \u2013 Identifiant UUID du corpus. Returns: dict ( dict ) \u2013 \u00c9tat de l'index et m\u00e9tadonn\u00e9es.","title":"check_index_status"},{"location":"api/lib/vectordb/index_references/#vectordb.src.index_manager.create_simple_index","text":"Cr\u00e9e un index vectoriel simple pour un corpus donn\u00e9. Cette fonction cr\u00e9e un index IVFFLAT standard pour acc\u00e9l\u00e9rer les recherches vectorielles sur un corpus sp\u00e9cifique. Parameters: corpus_id ( str ) \u2013 Identifiant UUID du corpus \u00e0 indexer. Returns: dict ( dict ) \u2013 R\u00e9sultat de l'op\u00e9ration avec statut et message.","title":"create_simple_index"},{"location":"api/lib/vectordb/index_references/#vectordb.src.index_manager.drop_index","text":"Supprime l'index vectoriel pour un corpus. Parameters: corpus_id ( str ) \u2013 Identifiant UUID du corpus. Returns: dict ( dict ) \u2013 R\u00e9sultat de l'op\u00e9ration.","title":"drop_index"},{"location":"api/lib/vectordb/search_references/","text":"References for SEARCH search.py \u2013 Hybrid semantic / metadata search. Hybrid document-search framework for PostgreSQL + pgvector SearchEngine Moteur de recherche hybride (m\u00e9tadonn\u00e9es + vecteur) avec rerank optionnel via Cross-Encoder. Utilise \u00e0 la fois une recherche par similarit\u00e9 vectorielle et un rerank avec un mod\u00e8le Cross-Encoder. __init__ () Initialise le moteur de recherche avec son g\u00e9n\u00e9rateur d\u2019embeddings et son ranker. hybrid_search ( db , req ) Ex\u00e9cute une recherche hybride et renvoie une r\u00e9ponse format\u00e9e selon le mod\u00e8le SearchResponse. Parameters: db ( Session ) \u2013 Session SQLAlchemy. req ( SearchRequest ) \u2013 Param\u00e8tres de la recherche. Returns: SearchResponse ( SearchResponse ) \u2013 R\u00e9ponse contenant la requ\u00eate, le nombre de r\u00e9sultats demand\u00e9s, SearchResponse \u2013 le nombre total de r\u00e9sultats et une liste de chunks expos\u00e9s en camelCase.","title":"Recherche"},{"location":"api/lib/vectordb/search_references/#references-for-search","text":"search.py \u2013 Hybrid semantic / metadata search. Hybrid document-search framework for PostgreSQL + pgvector","title":"References for SEARCH"},{"location":"api/lib/vectordb/search_references/#vectordb.src.search.SearchEngine","text":"Moteur de recherche hybride (m\u00e9tadonn\u00e9es + vecteur) avec rerank optionnel via Cross-Encoder. Utilise \u00e0 la fois une recherche par similarit\u00e9 vectorielle et un rerank avec un mod\u00e8le Cross-Encoder.","title":"SearchEngine"},{"location":"api/lib/vectordb/search_references/#vectordb.src.search.SearchEngine.__init__","text":"Initialise le moteur de recherche avec son g\u00e9n\u00e9rateur d\u2019embeddings et son ranker.","title":"__init__"},{"location":"api/lib/vectordb/search_references/#vectordb.src.search.SearchEngine.hybrid_search","text":"Ex\u00e9cute une recherche hybride et renvoie une r\u00e9ponse format\u00e9e selon le mod\u00e8le SearchResponse. Parameters: db ( Session ) \u2013 Session SQLAlchemy. req ( SearchRequest ) \u2013 Param\u00e8tres de la recherche. Returns: SearchResponse ( SearchResponse ) \u2013 R\u00e9ponse contenant la requ\u00eate, le nombre de r\u00e9sultats demand\u00e9s, SearchResponse \u2013 le nombre total de r\u00e9sultats et une liste de chunks expos\u00e9s en camelCase.","title":"hybrid_search"},{"location":"api/rest/rest_api/","text":"REST API Documentation Cl\u00e9a API 0.1.1 API pour g\u00e9rer les documents et effectuer des recherches s\u00e9mantiques. Database POST /database/documents Ajouter un document avec ses chunks Description Ins\u00e8re le document puis ses chunks dans une transaction unique. Args: payload: Objet contenant les donn\u00e9es du document et ses chunks. db: Session de base de donn\u00e9es inject\u00e9e par d\u00e9pendance. Returns: DocumentResponse: Document cr\u00e9\u00e9 avec le nombre de chunks associ\u00e9s. Raises: HTTPException: Si une erreur survient pendant l'insertion. Request body application/json { \"document\" : { \"title\" : \"string\" , \"theme\" : \"string\" , \"documentType\" : \"string\" , \"publishDate\" : \"2022-04-13\" , \"corpusId\" : null }, \"chunks\" : [ { \"id\" : null , \"content\" : \"string\" , \"startChar\" : 0 , \"endChar\" : 0 , \"hierarchyLevel\" : 0 , \"parentChunkId\" : null } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the request body { \"properties\" : { \"document\" : { \"$ref\" : \"#/components/schemas/DocumentCreate\" }, \"chunks\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ChunkCreate\" }, \"type\" : \"array\" , \"title\" : \"Chunks\" } }, \"type\" : \"object\" , \"required\" : [ \"document\" , \"chunks\" ], \"title\" : \"DocumentWithChunks\" , \"description\" : \"Payload complet pour `POST /database/documents`.\" } Response 200 OK application/json { \"id\" : 0 , \"title\" : \"string\" , \"theme\" : \"string\" , \"documentType\" : \"string\" , \"publishDate\" : \"2022-04-13\" , \"corpusId\" : null , \"chunkCount\" : 0 , \"indexNeeded\" : true } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"id\" : { \"type\" : \"integer\" , \"title\" : \"Id\" }, \"title\" : { \"type\" : \"string\" , \"title\" : \"Title\" }, \"theme\" : { \"type\" : \"string\" , \"title\" : \"Theme\" }, \"documentType\" : { \"type\" : \"string\" , \"title\" : \"Documenttype\" }, \"publishDate\" : { \"type\" : \"string\" , \"format\" : \"date\" , \"title\" : \"Publishdate\" }, \"corpusId\" : { \"anyOf\" : [ { \"type\" : \"string\" }, { \"type\" : \"null\" } ], \"title\" : \"Corpusid\" }, \"chunkCount\" : { \"type\" : \"integer\" , \"minimum\" : 0 , \"title\" : \"Chunkcount\" }, \"indexNeeded\" : { \"type\" : \"boolean\" , \"title\" : \"Indexneeded\" , \"default\" : false } }, \"type\" : \"object\" , \"required\" : [ \"id\" , \"title\" , \"theme\" , \"documentType\" , \"publishDate\" , \"chunkCount\" ], \"title\" : \"DocumentResponse\" , \"description\" : \"R\u00e9ponse standard lorsqu\u2019un document est renvoy\u00e9 c\u00f4t\u00e9 API.\\n\\nAttributs:\\n id (int): Identifiant du document.\\n title (str): Titre du document.\\n theme (str): Th\u00e8me du document.\\n document_type (str): Type de document.\\n publish_date (date): Date de publication.\\n corpus_id (Optional[str]): Identifiant du corpus.\\n chunk_count (int): Nombre de chunks associ\u00e9s (>= 0).\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" } GET /database/documents Lister les documents Description Liste l'ensemble des documents de la base de donn\u00e9es avec leur nombre de chunks. Cette fonction permet de r\u00e9cup\u00e9rer un ensemble pagin\u00e9 de documents avec possibilit\u00e9 de filtrage sur diff\u00e9rents crit\u00e8res. Args: theme: Filtre optionnel pour le th\u00e8me du document. document_type: Filtre optionnel pour le type du document. corpus_id: Filtre optionnel sur l'identifiant du corpus. skip: Nombre de documents \u00e0 ignorer (pour la pagination). limit: Nombre maximal de documents \u00e0 retourner. db: Session de base de donn\u00e9es fournie par d\u00e9pendance. Returns: Liste des documents format\u00e9s avec leur nombre de chunks associ\u00e9s. Input parameters Parameter In Type Default Nullable Description corpusId query None No documentType query None No limit query integer 100 No skip query integer 0 No theme query None No Response 200 OK application/json [ { \"id\" : 0 , \"title\" : \"string\" , \"theme\" : \"string\" , \"documentType\" : \"string\" , \"publishDate\" : \"2022-04-13\" , \"corpusId\" : null , \"chunkCount\" : 0 , \"indexNeeded\" : true } ] \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"type\" : \"array\" , \"items\" : { \"$ref\" : \"#/components/schemas/DocumentResponse\" }, \"title\" : \"Response List Documents Database Documents Get\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" } PUT /database/documents/ {document_id} Mettre \u00e0 jour un document (et/ou ajouter des chunks) Description Met \u00e0 jour les m\u00e9tadonn\u00e9es d'un document et peut ajouter de nouveaux chunks. Cette fonction permet de modifier les informations d'un document existant et d'y ajouter de nouveaux fragments de texte (chunks) en une seule op\u00e9ration. Args: payload: Objet de mise \u00e0 jour contenant le document et \u00e9ventuellement des nouveaux chunks. document_id: Identifiant num\u00e9rique du document \u00e0 mettre \u00e0 jour (\u2265 1). db: Session de base de donn\u00e9es inject\u00e9e par d\u00e9pendance. Returns: DocumentResponse: Document mis \u00e0 jour avec le nombre total de chunks associ\u00e9s. Raises: HTTPException: - 404: Si le document n'existe pas dans la base Input parameters Parameter In Type Default Nullable Description document_id path integer No Identifiant du document \u00e0 mettre \u00e0 jour (>= 1). Request body application/json { \"document\" : { \"id\" : 0 , \"title\" : null , \"theme\" : null , \"documentType\" : null , \"publishDate\" : null , \"corpusId\" : null }, \"newChunks\" : null } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the request body { \"properties\" : { \"document\" : { \"$ref\" : \"#/components/schemas/DocumentUpdate\" }, \"newChunks\" : { \"anyOf\" : [ { \"items\" : { \"$ref\" : \"#/components/schemas/ChunkCreate\" }, \"type\" : \"array\" }, { \"type\" : \"null\" } ], \"title\" : \"Newchunks\" } }, \"type\" : \"object\" , \"required\" : [ \"document\" ], \"title\" : \"UpdateWithChunks\" , \"description\" : \"Payload de mise-\u00e0-jour :\\n\\n* `document` \u2192 DTO `DocumentUpdate`\\n* `newChunks` \u2192 \u00e9ventuelle liste de nouveaux chunks \u00e0 ajouter\" } Response 200 OK application/json { \"id\" : 0 , \"title\" : \"string\" , \"theme\" : \"string\" , \"documentType\" : \"string\" , \"publishDate\" : \"2022-04-13\" , \"corpusId\" : null , \"chunkCount\" : 0 , \"indexNeeded\" : true } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"id\" : { \"type\" : \"integer\" , \"title\" : \"Id\" }, \"title\" : { \"type\" : \"string\" , \"title\" : \"Title\" }, \"theme\" : { \"type\" : \"string\" , \"title\" : \"Theme\" }, \"documentType\" : { \"type\" : \"string\" , \"title\" : \"Documenttype\" }, \"publishDate\" : { \"type\" : \"string\" , \"format\" : \"date\" , \"title\" : \"Publishdate\" }, \"corpusId\" : { \"anyOf\" : [ { \"type\" : \"string\" }, { \"type\" : \"null\" } ], \"title\" : \"Corpusid\" }, \"chunkCount\" : { \"type\" : \"integer\" , \"minimum\" : 0 , \"title\" : \"Chunkcount\" }, \"indexNeeded\" : { \"type\" : \"boolean\" , \"title\" : \"Indexneeded\" , \"default\" : false } }, \"type\" : \"object\" , \"required\" : [ \"id\" , \"title\" , \"theme\" , \"documentType\" , \"publishDate\" , \"chunkCount\" ], \"title\" : \"DocumentResponse\" , \"description\" : \"R\u00e9ponse standard lorsqu\u2019un document est renvoy\u00e9 c\u00f4t\u00e9 API.\\n\\nAttributs:\\n id (int): Identifiant du document.\\n title (str): Titre du document.\\n theme (str): Th\u00e8me du document.\\n document_type (str): Type de document.\\n publish_date (date): Date de publication.\\n corpus_id (Optional[str]): Identifiant du corpus.\\n chunk_count (int): Nombre de chunks associ\u00e9s (>= 0).\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" } DELETE /database/documents/ {document_id} Supprimer un document et tous ses chunks Input parameters Parameter In Type Default Nullable Description document_id path integer No Response 200 OK application/json Schema of the response body { \"type\" : \"object\" , \"additionalProperties\" : true , \"title\" : \"Response Remove Document Database Documents Document Id Delete\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" } GET /database/documents/ {document_id} R\u00e9cup\u00e9rer un document Description R\u00e9cup\u00e8re un document \u00e0 partir de son identifiant et le formate en DocumentResponse. Args: document_id (int): Identifiant du document \u00e0 r\u00e9cup\u00e9rer. db (Session): Session de base de donn\u00e9es fournie par d\u00e9pendance. Returns: DocumentResponse: Document format\u00e9 avec le nombre de chunks associ\u00e9s. Raises: HTTPException: Si le document n'existe pas dans la base. Input parameters Parameter In Type Default Nullable Description document_id path integer No Response 200 OK application/json { \"id\" : 0 , \"title\" : \"string\" , \"theme\" : \"string\" , \"documentType\" : \"string\" , \"publishDate\" : \"2022-04-13\" , \"corpusId\" : null , \"chunkCount\" : 0 , \"indexNeeded\" : true } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"id\" : { \"type\" : \"integer\" , \"title\" : \"Id\" }, \"title\" : { \"type\" : \"string\" , \"title\" : \"Title\" }, \"theme\" : { \"type\" : \"string\" , \"title\" : \"Theme\" }, \"documentType\" : { \"type\" : \"string\" , \"title\" : \"Documenttype\" }, \"publishDate\" : { \"type\" : \"string\" , \"format\" : \"date\" , \"title\" : \"Publishdate\" }, \"corpusId\" : { \"anyOf\" : [ { \"type\" : \"string\" }, { \"type\" : \"null\" } ], \"title\" : \"Corpusid\" }, \"chunkCount\" : { \"type\" : \"integer\" , \"minimum\" : 0 , \"title\" : \"Chunkcount\" }, \"indexNeeded\" : { \"type\" : \"boolean\" , \"title\" : \"Indexneeded\" , \"default\" : false } }, \"type\" : \"object\" , \"required\" : [ \"id\" , \"title\" , \"theme\" , \"documentType\" , \"publishDate\" , \"chunkCount\" ], \"title\" : \"DocumentResponse\" , \"description\" : \"R\u00e9ponse standard lorsqu\u2019un document est renvoy\u00e9 c\u00f4t\u00e9 API.\\n\\nAttributs:\\n id (int): Identifiant du document.\\n title (str): Titre du document.\\n theme (str): Th\u00e8me du document.\\n document_type (str): Type de document.\\n publish_date (date): Date de publication.\\n corpus_id (Optional[str]): Identifiant du corpus.\\n chunk_count (int): Nombre de chunks associ\u00e9s (>= 0).\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" } DELETE /database/documents/ {document_id} /chunks Supprimer des chunks d'un document Input parameters Parameter In Type Default Nullable Description chunk_ids query None No IDs \u00e0 supprimer ; vide \u21d2 tous les chunks du document document_id path integer No Response 200 OK application/json Schema of the response body { \"type\" : \"object\" , \"additionalProperties\" : true , \"title\" : \"Response Remove Chunks Database Documents Document Id Chunks Delete\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" } GET /database/documents/ {document_id} /chunks R\u00e9cup\u00e9rer les chunks d'un document Input parameters Parameter In Type Default Nullable Description document_id path integer No hierarchyLevel query None No limit query integer 100 No parentChunkId query None No skip query integer 0 No Response 200 OK application/json [ null ] \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"type\" : \"array\" , \"items\" : {}, \"title\" : \"Response Get Chunks Database Documents Document Id Chunks Get\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" } Search POST /search/hybrid_search Recherche hybride (vecteur + filtres) Description Retourne les top k chunks les plus pertinents. Le moteur combine : Filtres SQL (theme, document_type , dates, corpus_id ) Index vectoriel pgvector (IVFFLAT ou HNSW) Rerank Cross-Encoder sur un sous-ensemble \u00e9largi ( top k \u00d7 3 ) Request body application/json { \"query\" : \"string\" , \"topK\" : 0 , \"theme\" : null , \"documentType\" : null , \"startDate\" : null , \"endDate\" : null , \"corpusId\" : null , \"hierarchical\" : true , \"hierarchyLevel\" : null } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the request body { \"properties\" : { \"query\" : { \"type\" : \"string\" , \"title\" : \"Query\" }, \"topK\" : { \"type\" : \"integer\" , \"minimum\" : 1 , \"title\" : \"Topk\" , \"default\" : 10 }, \"theme\" : { \"anyOf\" : [ { \"type\" : \"string\" }, { \"type\" : \"null\" } ], \"title\" : \"Theme\" }, \"documentType\" : { \"anyOf\" : [ { \"type\" : \"string\" }, { \"type\" : \"null\" } ], \"title\" : \"Documenttype\" }, \"startDate\" : { \"anyOf\" : [ { \"type\" : \"string\" , \"format\" : \"date\" }, { \"type\" : \"null\" } ], \"title\" : \"Startdate\" }, \"endDate\" : { \"anyOf\" : [ { \"type\" : \"string\" , \"format\" : \"date\" }, { \"type\" : \"null\" } ], \"title\" : \"Enddate\" }, \"corpusId\" : { \"anyOf\" : [ { \"type\" : \"string\" }, { \"type\" : \"null\" } ], \"title\" : \"Corpusid\" }, \"hierarchical\" : { \"type\" : \"boolean\" , \"title\" : \"Hierarchical\" , \"default\" : false }, \"hierarchyLevel\" : { \"anyOf\" : [ { \"type\" : \"integer\" }, { \"type\" : \"null\" } ], \"title\" : \"Hierarchylevel\" } }, \"type\" : \"object\" , \"required\" : [ \"query\" ], \"title\" : \"SearchRequest\" , \"description\" : \"Param\u00e8tres accept\u00e9s par `POST /search/hybrid_search`.\" } Response 200 OK application/json { \"query\" : \"string\" , \"topK\" : 0 , \"totalResults\" : 0 , \"results\" : [ { \"chunkId\" : 0 , \"documentId\" : 0 , \"title\" : \"string\" , \"content\" : \"string\" , \"theme\" : \"string\" , \"documentType\" : \"string\" , \"publishDate\" : \"2022-04-13\" , \"score\" : 10.12 , \"hierarchyLevel\" : 0 , \"context\" : null } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"query\" : { \"type\" : \"string\" , \"title\" : \"Query\" }, \"topK\" : { \"type\" : \"integer\" , \"title\" : \"Topk\" }, \"totalResults\" : { \"type\" : \"integer\" , \"title\" : \"Totalresults\" }, \"results\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ChunkResult\" }, \"type\" : \"array\" , \"title\" : \"Results\" } }, \"type\" : \"object\" , \"required\" : [ \"query\" , \"topK\" , \"totalResults\" , \"results\" ], \"title\" : \"SearchResponse\" , \"description\" : \"R\u00e9ponse compl\u00e8te du moteur de recherche.\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" } Index POST /index/create-index/ {corpus_id} Cr\u00e9er un index vectoriel pour un corpus Description Cr\u00e9e un index vectoriel pour un corpus sp\u00e9cifique. Cette fonction cr\u00e9e un index IVFFLAT simple pour acc\u00e9l\u00e9rer les recherches vectorielles sur le corpus sp\u00e9cifi\u00e9. Args: corpus_id: Identifiant UUID du corpus \u00e0 indexer. Returns: dict: R\u00e9sultat de l'op\u00e9ration avec statut et message. Raises: HTTPException: Si une erreur survient lors de la cr\u00e9ation de l'index. Input parameters Parameter In Type Default Nullable Description corpus_id path string No Identifiant UUID du corpus Response 200 OK application/json Schema of the response body { \"type\" : \"object\" , \"additionalProperties\" : true , \"title\" : \"Response Create Corpus Index Index Create Index Corpus Id Post\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" } DELETE /index/drop-index/ {corpus_id} Supprimer l'index vectoriel d'un corpus Description Supprime l'index vectoriel pour un corpus sp\u00e9cifique. Args: corpus_id: Identifiant UUID du corpus. Returns: dict: R\u00e9sultat de l'op\u00e9ration. Raises: HTTPException: Si une erreur survient lors de la suppression de l'index. Input parameters Parameter In Type Default Nullable Description corpus_id path string No Identifiant UUID du corpus Response 200 OK application/json Schema of the response body { \"type\" : \"object\" , \"additionalProperties\" : true , \"title\" : \"Response Remove Corpus Index Index Drop Index Corpus Id Delete\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" } GET /index/index-status/ {corpus_id} V\u00e9rifier l'\u00e9tat de l'index pour un corpus Description V\u00e9rifie l'\u00e9tat de l'index pour un corpus sp\u00e9cifique. Args: corpus_id: Identifiant UUID du corpus. Returns: dict: \u00c9tat de l'index et m\u00e9tadonn\u00e9es. Input parameters Parameter In Type Default Nullable Description corpus_id path string No Identifiant UUID du corpus Response 200 OK application/json Schema of the response body { \"type\" : \"object\" , \"additionalProperties\" : true , \"title\" : \"Response Get Index Status Index Index Status Corpus Id Get\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" } GET /index/indexes V\u00e9rifier l'\u00e9tat de tous les index vectoriels Description V\u00e9rifie l'\u00e9tat de tous les index vectoriels. Returns: dict: \u00c9tat des index pour tous les corpus. Response 200 OK application/json Schema of the response body { \"additionalProperties\" : true , \"type\" : \"object\" , \"title\" : \"Response Get All Indexes Index Indexes Get\" } DocLoader POST /doc_loader/upload-file Uploader un fichier et le traiter Description Uploade un fichier, l'extrait et le divise en chunks. Args: file (UploadFile): Fichier upload\u00e9 par l'utilisateur. max_length (int): Taille maximale d'un chunk. Par d\u00e9faut 1000. theme (str): Th\u00e8me du document. Par d\u00e9faut \"Th\u00e8me g\u00e9n\u00e9rique\". Returns: List[DocumentWithChunks]: Liste des documents extraits. Raises: HTTPException: Si une erreur survient lors du traitement ou si aucun contenu n'est extrait. Input parameters Parameter In Type Default Nullable Description max_length query integer 1000 No Taille maximale d'un chunk theme query string Th\u00e8me g\u00e9n\u00e9rique No Th\u00e8me du document Request body multipart/form-data { \"file\" : \"TG9yZW0gaXBzdW0gZG9sb3Igc2l0IGFtZXQ=\" } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the request body { \"properties\" : { \"file\" : { \"type\" : \"string\" , \"format\" : \"binary\" , \"title\" : \"File\" , \"description\" : \"Fichier \u00e0 traiter\" } }, \"type\" : \"object\" , \"required\" : [ \"file\" ], \"title\" : \"Body_upload_and_process_file_doc_loader_upload_file_post\" } Response 200 OK application/json { \"document\" : { \"title\" : \"string\" , \"theme\" : \"string\" , \"documentType\" : \"string\" , \"publishDate\" : \"2022-04-13\" , \"corpusId\" : null }, \"chunks\" : [ { \"id\" : null , \"content\" : \"string\" , \"startChar\" : 0 , \"endChar\" : 0 , \"hierarchyLevel\" : 0 , \"parentChunkId\" : null } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"document\" : { \"$ref\" : \"#/components/schemas/DocumentCreate\" }, \"chunks\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ChunkCreate\" }, \"type\" : \"array\" , \"title\" : \"Chunks\" } }, \"type\" : \"object\" , \"required\" : [ \"document\" , \"chunks\" ], \"title\" : \"DocumentWithChunks\" , \"description\" : \"Payload complet pour `POST /database/documents`.\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" } Pipeline POST /pipeline/process-and-store Charger un fichier, l'extraire et l'ins\u00e9rer dans la base de donn\u00e9es avec segmentation adaptative Description Charge un fichier, l'extrait avec segmentation hi\u00e9rarchique et l'ins\u00e8re dans la base de donn\u00e9es. Le fichier est temporairement sauvegard\u00e9 sur le disque, trait\u00e9 pour en extraire le contenu textuel, segment\u00e9 selon une approche hi\u00e9rarchique, puis ins\u00e9r\u00e9 dans la base de donn\u00e9es avec g\u00e9n\u00e9ration automatique d'embeddings. Args: file: Fichier upload\u00e9 par l'utilisateur. max_length: Taille maximale d'un chunk final. overlap: Chevauchement entre les chunks. theme: Th\u00e8me \u00e0 appliquer au document. document_type: Type de document (d\u00e9termin\u00e9 automatiquement si non sp\u00e9cifi\u00e9). corpus_id: Identifiant du corpus (g\u00e9n\u00e9r\u00e9 si non sp\u00e9cifi\u00e9). Returns: R\u00e9sultats de l'op\u00e9ration avec l'ID du document et les statistiques de segmentation. Raises: HTTPException: Si une erreur survient pendant le traitement du document. Input parameters Parameter In Type Default Nullable Description corpus_id query None No Identifiant du corpus (g\u00e9n\u00e9r\u00e9 si non sp\u00e9cifi\u00e9) max_length query integer 500 No Taille maximale d'un chunk overlap query integer 100 No Chevauchement entre les chunks theme query string Th\u00e8me g\u00e9n\u00e9rique No Th\u00e8me du document Request body multipart/form-data { \"file\" : \"TG9yZW0gaXBzdW0gZG9sb3Igc2l0IGFtZXQ=\" } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the request body { \"properties\" : { \"file\" : { \"type\" : \"string\" , \"format\" : \"binary\" , \"title\" : \"File\" , \"description\" : \"Fichier \u00e0 traiter\" } }, \"type\" : \"object\" , \"required\" : [ \"file\" ], \"title\" : \"Body_process_and_store_endpoint_pipeline_process_and_store_post\" } Response 200 OK application/json Schema of the response body { \"type\" : \"object\" , \"additionalProperties\" : true , \"title\" : \"Response Process And Store Endpoint Pipeline Process And Store Post\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" } POST /pipeline/process-and-store-async Traiter un fichier en arri\u00e8re-plan et l'ins\u00e9rer dans la base de donn\u00e9es Description Traite un fichier en arri\u00e8re-plan et l'ins\u00e8re dans la base de donn\u00e9es. Similaire \u00e0 process-and-store mais s'ex\u00e9cute de mani\u00e8re asynchrone pour les fichiers volumineux. Le client re\u00e7oit imm\u00e9diatement une r\u00e9ponse avec un identifiant de t\u00e2che pendant que le traitement se poursuit en arri\u00e8re-plan. Args: background_tasks: Gestionnaire de t\u00e2ches en arri\u00e8re-plan de FastAPI. file: Fichier upload\u00e9 par l'utilisateur. max_length: Taille maximale d'un chunk final. overlap: Chevauchement entre les chunks. theme: Th\u00e8me \u00e0 appliquer au document. document_type: Type de document (d\u00e9termin\u00e9 automatiquement si non sp\u00e9cifi\u00e9). corpus_id: Identifiant du corpus (g\u00e9n\u00e9r\u00e9 si non sp\u00e9cifi\u00e9). Returns: Informations sur la t\u00e2che en arri\u00e8re-plan cr\u00e9\u00e9e. Input parameters Parameter In Type Default Nullable Description corpus_id query None No Identifiant du corpus (g\u00e9n\u00e9r\u00e9 si non sp\u00e9cifi\u00e9) max_length query integer 500 No Taille maximale d'un chunk overlap query integer 100 No Chevauchement entre les chunks theme query string Th\u00e8me g\u00e9n\u00e9rique No Th\u00e8me du document Request body multipart/form-data { \"file\" : \"TG9yZW0gaXBzdW0gZG9sb3Igc2l0IGFtZXQ=\" } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the request body { \"properties\" : { \"file\" : { \"type\" : \"string\" , \"format\" : \"binary\" , \"title\" : \"File\" , \"description\" : \"Fichier \u00e0 traiter\" } }, \"type\" : \"object\" , \"required\" : [ \"file\" ], \"title\" : \"Body_process_and_store_async_endpoint_pipeline_process_and_store_async_post\" } Response 200 OK application/json Schema of the response body { \"type\" : \"object\" , \"additionalProperties\" : true , \"title\" : \"Response Process And Store Async Endpoint Pipeline Process And Store Async Post\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" } Endpoints GET / Root Description Endpoint racine pour v\u00e9rifier l'\u00e9tat de l'API. Returns: dict: Message indiquant que l'API est en cours d'ex\u00e9cution. Response 200 OK application/json Schema of the response body Schemas Body_process_and_store_async_endpoint_pipeline_process_and_store_async_post Name Type file string ( binary ) Body_process_and_store_endpoint_pipeline_process_and_store_post Name Type file string ( binary ) Body_upload_and_process_file_doc_loader_upload_file_post Name Type file string ( binary ) ChunkCreate Name Type content string endChar integer hierarchyLevel integer id parentChunkId startChar integer ChunkResult Name Type chunkId integer content string context documentId integer documentType string hierarchyLevel integer publishDate string ( date ) score number theme string title string DocumentCreate Name Type corpusId documentType string publishDate string ( date ) theme string title string DocumentResponse Name Type chunkCount integer corpusId documentType string id integer indexNeeded boolean publishDate string ( date ) theme string title string DocumentUpdate Name Type corpusId documentType id integer publishDate theme title DocumentWithChunks Name Type chunks Array< ChunkCreate > document DocumentCreate HierarchicalContext Name Type level0 level1 level2 HTTPValidationError Name Type detail Array< ValidationError > SearchRequest Name Type corpusId documentType endDate hierarchical boolean hierarchyLevel query string startDate theme topK integer SearchResponse Name Type query string results Array< ChunkResult > topK integer totalResults integer UpdateWithChunks Name Type document DocumentUpdate newChunks ValidationError Name Type loc Array<> msg string type string","title":"Documentation Swagger"},{"location":"api/rest/rest_api/#rest-api-documentation","text":"","title":"REST API Documentation"},{"location":"api/rest/rest_api/#clea-api-011","text":"API pour g\u00e9rer les documents et effectuer des recherches s\u00e9mantiques.","title":"Cl\u00e9a API 0.1.1"},{"location":"api/rest/rest_api/#database","text":"","title":"Database"},{"location":"api/rest/rest_api/#post-databasedocuments","text":"Ajouter un document avec ses chunks Description Ins\u00e8re le document puis ses chunks dans une transaction unique. Args: payload: Objet contenant les donn\u00e9es du document et ses chunks. db: Session de base de donn\u00e9es inject\u00e9e par d\u00e9pendance. Returns: DocumentResponse: Document cr\u00e9\u00e9 avec le nombre de chunks associ\u00e9s. Raises: HTTPException: Si une erreur survient pendant l'insertion. Request body application/json { \"document\" : { \"title\" : \"string\" , \"theme\" : \"string\" , \"documentType\" : \"string\" , \"publishDate\" : \"2022-04-13\" , \"corpusId\" : null }, \"chunks\" : [ { \"id\" : null , \"content\" : \"string\" , \"startChar\" : 0 , \"endChar\" : 0 , \"hierarchyLevel\" : 0 , \"parentChunkId\" : null } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the request body { \"properties\" : { \"document\" : { \"$ref\" : \"#/components/schemas/DocumentCreate\" }, \"chunks\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ChunkCreate\" }, \"type\" : \"array\" , \"title\" : \"Chunks\" } }, \"type\" : \"object\" , \"required\" : [ \"document\" , \"chunks\" ], \"title\" : \"DocumentWithChunks\" , \"description\" : \"Payload complet pour `POST /database/documents`.\" } Response 200 OK application/json { \"id\" : 0 , \"title\" : \"string\" , \"theme\" : \"string\" , \"documentType\" : \"string\" , \"publishDate\" : \"2022-04-13\" , \"corpusId\" : null , \"chunkCount\" : 0 , \"indexNeeded\" : true } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"id\" : { \"type\" : \"integer\" , \"title\" : \"Id\" }, \"title\" : { \"type\" : \"string\" , \"title\" : \"Title\" }, \"theme\" : { \"type\" : \"string\" , \"title\" : \"Theme\" }, \"documentType\" : { \"type\" : \"string\" , \"title\" : \"Documenttype\" }, \"publishDate\" : { \"type\" : \"string\" , \"format\" : \"date\" , \"title\" : \"Publishdate\" }, \"corpusId\" : { \"anyOf\" : [ { \"type\" : \"string\" }, { \"type\" : \"null\" } ], \"title\" : \"Corpusid\" }, \"chunkCount\" : { \"type\" : \"integer\" , \"minimum\" : 0 , \"title\" : \"Chunkcount\" }, \"indexNeeded\" : { \"type\" : \"boolean\" , \"title\" : \"Indexneeded\" , \"default\" : false } }, \"type\" : \"object\" , \"required\" : [ \"id\" , \"title\" , \"theme\" , \"documentType\" , \"publishDate\" , \"chunkCount\" ], \"title\" : \"DocumentResponse\" , \"description\" : \"R\u00e9ponse standard lorsqu\u2019un document est renvoy\u00e9 c\u00f4t\u00e9 API.\\n\\nAttributs:\\n id (int): Identifiant du document.\\n title (str): Titre du document.\\n theme (str): Th\u00e8me du document.\\n document_type (str): Type de document.\\n publish_date (date): Date de publication.\\n corpus_id (Optional[str]): Identifiant du corpus.\\n chunk_count (int): Nombre de chunks associ\u00e9s (>= 0).\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" }","title":"POST /database/documents"},{"location":"api/rest/rest_api/#get-databasedocuments","text":"Lister les documents Description Liste l'ensemble des documents de la base de donn\u00e9es avec leur nombre de chunks. Cette fonction permet de r\u00e9cup\u00e9rer un ensemble pagin\u00e9 de documents avec possibilit\u00e9 de filtrage sur diff\u00e9rents crit\u00e8res. Args: theme: Filtre optionnel pour le th\u00e8me du document. document_type: Filtre optionnel pour le type du document. corpus_id: Filtre optionnel sur l'identifiant du corpus. skip: Nombre de documents \u00e0 ignorer (pour la pagination). limit: Nombre maximal de documents \u00e0 retourner. db: Session de base de donn\u00e9es fournie par d\u00e9pendance. Returns: Liste des documents format\u00e9s avec leur nombre de chunks associ\u00e9s. Input parameters Parameter In Type Default Nullable Description corpusId query None No documentType query None No limit query integer 100 No skip query integer 0 No theme query None No Response 200 OK application/json [ { \"id\" : 0 , \"title\" : \"string\" , \"theme\" : \"string\" , \"documentType\" : \"string\" , \"publishDate\" : \"2022-04-13\" , \"corpusId\" : null , \"chunkCount\" : 0 , \"indexNeeded\" : true } ] \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"type\" : \"array\" , \"items\" : { \"$ref\" : \"#/components/schemas/DocumentResponse\" }, \"title\" : \"Response List Documents Database Documents Get\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" }","title":"GET /database/documents"},{"location":"api/rest/rest_api/#put-databasedocumentsdocument_id","text":"Mettre \u00e0 jour un document (et/ou ajouter des chunks) Description Met \u00e0 jour les m\u00e9tadonn\u00e9es d'un document et peut ajouter de nouveaux chunks. Cette fonction permet de modifier les informations d'un document existant et d'y ajouter de nouveaux fragments de texte (chunks) en une seule op\u00e9ration. Args: payload: Objet de mise \u00e0 jour contenant le document et \u00e9ventuellement des nouveaux chunks. document_id: Identifiant num\u00e9rique du document \u00e0 mettre \u00e0 jour (\u2265 1). db: Session de base de donn\u00e9es inject\u00e9e par d\u00e9pendance. Returns: DocumentResponse: Document mis \u00e0 jour avec le nombre total de chunks associ\u00e9s. Raises: HTTPException: - 404: Si le document n'existe pas dans la base Input parameters Parameter In Type Default Nullable Description document_id path integer No Identifiant du document \u00e0 mettre \u00e0 jour (>= 1). Request body application/json { \"document\" : { \"id\" : 0 , \"title\" : null , \"theme\" : null , \"documentType\" : null , \"publishDate\" : null , \"corpusId\" : null }, \"newChunks\" : null } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the request body { \"properties\" : { \"document\" : { \"$ref\" : \"#/components/schemas/DocumentUpdate\" }, \"newChunks\" : { \"anyOf\" : [ { \"items\" : { \"$ref\" : \"#/components/schemas/ChunkCreate\" }, \"type\" : \"array\" }, { \"type\" : \"null\" } ], \"title\" : \"Newchunks\" } }, \"type\" : \"object\" , \"required\" : [ \"document\" ], \"title\" : \"UpdateWithChunks\" , \"description\" : \"Payload de mise-\u00e0-jour :\\n\\n* `document` \u2192 DTO `DocumentUpdate`\\n* `newChunks` \u2192 \u00e9ventuelle liste de nouveaux chunks \u00e0 ajouter\" } Response 200 OK application/json { \"id\" : 0 , \"title\" : \"string\" , \"theme\" : \"string\" , \"documentType\" : \"string\" , \"publishDate\" : \"2022-04-13\" , \"corpusId\" : null , \"chunkCount\" : 0 , \"indexNeeded\" : true } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"id\" : { \"type\" : \"integer\" , \"title\" : \"Id\" }, \"title\" : { \"type\" : \"string\" , \"title\" : \"Title\" }, \"theme\" : { \"type\" : \"string\" , \"title\" : \"Theme\" }, \"documentType\" : { \"type\" : \"string\" , \"title\" : \"Documenttype\" }, \"publishDate\" : { \"type\" : \"string\" , \"format\" : \"date\" , \"title\" : \"Publishdate\" }, \"corpusId\" : { \"anyOf\" : [ { \"type\" : \"string\" }, { \"type\" : \"null\" } ], \"title\" : \"Corpusid\" }, \"chunkCount\" : { \"type\" : \"integer\" , \"minimum\" : 0 , \"title\" : \"Chunkcount\" }, \"indexNeeded\" : { \"type\" : \"boolean\" , \"title\" : \"Indexneeded\" , \"default\" : false } }, \"type\" : \"object\" , \"required\" : [ \"id\" , \"title\" , \"theme\" , \"documentType\" , \"publishDate\" , \"chunkCount\" ], \"title\" : \"DocumentResponse\" , \"description\" : \"R\u00e9ponse standard lorsqu\u2019un document est renvoy\u00e9 c\u00f4t\u00e9 API.\\n\\nAttributs:\\n id (int): Identifiant du document.\\n title (str): Titre du document.\\n theme (str): Th\u00e8me du document.\\n document_type (str): Type de document.\\n publish_date (date): Date de publication.\\n corpus_id (Optional[str]): Identifiant du corpus.\\n chunk_count (int): Nombre de chunks associ\u00e9s (>= 0).\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" }","title":"PUT /database/documents/{document_id}"},{"location":"api/rest/rest_api/#delete-databasedocumentsdocument_id","text":"Supprimer un document et tous ses chunks Input parameters Parameter In Type Default Nullable Description document_id path integer No Response 200 OK application/json Schema of the response body { \"type\" : \"object\" , \"additionalProperties\" : true , \"title\" : \"Response Remove Document Database Documents Document Id Delete\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" }","title":"DELETE /database/documents/{document_id}"},{"location":"api/rest/rest_api/#get-databasedocumentsdocument_id","text":"R\u00e9cup\u00e9rer un document Description R\u00e9cup\u00e8re un document \u00e0 partir de son identifiant et le formate en DocumentResponse. Args: document_id (int): Identifiant du document \u00e0 r\u00e9cup\u00e9rer. db (Session): Session de base de donn\u00e9es fournie par d\u00e9pendance. Returns: DocumentResponse: Document format\u00e9 avec le nombre de chunks associ\u00e9s. Raises: HTTPException: Si le document n'existe pas dans la base. Input parameters Parameter In Type Default Nullable Description document_id path integer No Response 200 OK application/json { \"id\" : 0 , \"title\" : \"string\" , \"theme\" : \"string\" , \"documentType\" : \"string\" , \"publishDate\" : \"2022-04-13\" , \"corpusId\" : null , \"chunkCount\" : 0 , \"indexNeeded\" : true } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"id\" : { \"type\" : \"integer\" , \"title\" : \"Id\" }, \"title\" : { \"type\" : \"string\" , \"title\" : \"Title\" }, \"theme\" : { \"type\" : \"string\" , \"title\" : \"Theme\" }, \"documentType\" : { \"type\" : \"string\" , \"title\" : \"Documenttype\" }, \"publishDate\" : { \"type\" : \"string\" , \"format\" : \"date\" , \"title\" : \"Publishdate\" }, \"corpusId\" : { \"anyOf\" : [ { \"type\" : \"string\" }, { \"type\" : \"null\" } ], \"title\" : \"Corpusid\" }, \"chunkCount\" : { \"type\" : \"integer\" , \"minimum\" : 0 , \"title\" : \"Chunkcount\" }, \"indexNeeded\" : { \"type\" : \"boolean\" , \"title\" : \"Indexneeded\" , \"default\" : false } }, \"type\" : \"object\" , \"required\" : [ \"id\" , \"title\" , \"theme\" , \"documentType\" , \"publishDate\" , \"chunkCount\" ], \"title\" : \"DocumentResponse\" , \"description\" : \"R\u00e9ponse standard lorsqu\u2019un document est renvoy\u00e9 c\u00f4t\u00e9 API.\\n\\nAttributs:\\n id (int): Identifiant du document.\\n title (str): Titre du document.\\n theme (str): Th\u00e8me du document.\\n document_type (str): Type de document.\\n publish_date (date): Date de publication.\\n corpus_id (Optional[str]): Identifiant du corpus.\\n chunk_count (int): Nombre de chunks associ\u00e9s (>= 0).\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" }","title":"GET /database/documents/{document_id}"},{"location":"api/rest/rest_api/#delete-databasedocumentsdocument_idchunks","text":"Supprimer des chunks d'un document Input parameters Parameter In Type Default Nullable Description chunk_ids query None No IDs \u00e0 supprimer ; vide \u21d2 tous les chunks du document document_id path integer No Response 200 OK application/json Schema of the response body { \"type\" : \"object\" , \"additionalProperties\" : true , \"title\" : \"Response Remove Chunks Database Documents Document Id Chunks Delete\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" }","title":"DELETE /database/documents/{document_id}/chunks"},{"location":"api/rest/rest_api/#get-databasedocumentsdocument_idchunks","text":"R\u00e9cup\u00e9rer les chunks d'un document Input parameters Parameter In Type Default Nullable Description document_id path integer No hierarchyLevel query None No limit query integer 100 No parentChunkId query None No skip query integer 0 No Response 200 OK application/json [ null ] \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"type\" : \"array\" , \"items\" : {}, \"title\" : \"Response Get Chunks Database Documents Document Id Chunks Get\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" }","title":"GET /database/documents/{document_id}/chunks"},{"location":"api/rest/rest_api/#search","text":"","title":"Search"},{"location":"api/rest/rest_api/#post-searchhybrid_search","text":"Recherche hybride (vecteur + filtres) Description Retourne les top k chunks les plus pertinents. Le moteur combine : Filtres SQL (theme, document_type , dates, corpus_id ) Index vectoriel pgvector (IVFFLAT ou HNSW) Rerank Cross-Encoder sur un sous-ensemble \u00e9largi ( top k \u00d7 3 ) Request body application/json { \"query\" : \"string\" , \"topK\" : 0 , \"theme\" : null , \"documentType\" : null , \"startDate\" : null , \"endDate\" : null , \"corpusId\" : null , \"hierarchical\" : true , \"hierarchyLevel\" : null } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the request body { \"properties\" : { \"query\" : { \"type\" : \"string\" , \"title\" : \"Query\" }, \"topK\" : { \"type\" : \"integer\" , \"minimum\" : 1 , \"title\" : \"Topk\" , \"default\" : 10 }, \"theme\" : { \"anyOf\" : [ { \"type\" : \"string\" }, { \"type\" : \"null\" } ], \"title\" : \"Theme\" }, \"documentType\" : { \"anyOf\" : [ { \"type\" : \"string\" }, { \"type\" : \"null\" } ], \"title\" : \"Documenttype\" }, \"startDate\" : { \"anyOf\" : [ { \"type\" : \"string\" , \"format\" : \"date\" }, { \"type\" : \"null\" } ], \"title\" : \"Startdate\" }, \"endDate\" : { \"anyOf\" : [ { \"type\" : \"string\" , \"format\" : \"date\" }, { \"type\" : \"null\" } ], \"title\" : \"Enddate\" }, \"corpusId\" : { \"anyOf\" : [ { \"type\" : \"string\" }, { \"type\" : \"null\" } ], \"title\" : \"Corpusid\" }, \"hierarchical\" : { \"type\" : \"boolean\" , \"title\" : \"Hierarchical\" , \"default\" : false }, \"hierarchyLevel\" : { \"anyOf\" : [ { \"type\" : \"integer\" }, { \"type\" : \"null\" } ], \"title\" : \"Hierarchylevel\" } }, \"type\" : \"object\" , \"required\" : [ \"query\" ], \"title\" : \"SearchRequest\" , \"description\" : \"Param\u00e8tres accept\u00e9s par `POST /search/hybrid_search`.\" } Response 200 OK application/json { \"query\" : \"string\" , \"topK\" : 0 , \"totalResults\" : 0 , \"results\" : [ { \"chunkId\" : 0 , \"documentId\" : 0 , \"title\" : \"string\" , \"content\" : \"string\" , \"theme\" : \"string\" , \"documentType\" : \"string\" , \"publishDate\" : \"2022-04-13\" , \"score\" : 10.12 , \"hierarchyLevel\" : 0 , \"context\" : null } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"query\" : { \"type\" : \"string\" , \"title\" : \"Query\" }, \"topK\" : { \"type\" : \"integer\" , \"title\" : \"Topk\" }, \"totalResults\" : { \"type\" : \"integer\" , \"title\" : \"Totalresults\" }, \"results\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ChunkResult\" }, \"type\" : \"array\" , \"title\" : \"Results\" } }, \"type\" : \"object\" , \"required\" : [ \"query\" , \"topK\" , \"totalResults\" , \"results\" ], \"title\" : \"SearchResponse\" , \"description\" : \"R\u00e9ponse compl\u00e8te du moteur de recherche.\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" }","title":"POST /search/hybrid_search"},{"location":"api/rest/rest_api/#index","text":"","title":"Index"},{"location":"api/rest/rest_api/#post-indexcreate-indexcorpus_id","text":"Cr\u00e9er un index vectoriel pour un corpus Description Cr\u00e9e un index vectoriel pour un corpus sp\u00e9cifique. Cette fonction cr\u00e9e un index IVFFLAT simple pour acc\u00e9l\u00e9rer les recherches vectorielles sur le corpus sp\u00e9cifi\u00e9. Args: corpus_id: Identifiant UUID du corpus \u00e0 indexer. Returns: dict: R\u00e9sultat de l'op\u00e9ration avec statut et message. Raises: HTTPException: Si une erreur survient lors de la cr\u00e9ation de l'index. Input parameters Parameter In Type Default Nullable Description corpus_id path string No Identifiant UUID du corpus Response 200 OK application/json Schema of the response body { \"type\" : \"object\" , \"additionalProperties\" : true , \"title\" : \"Response Create Corpus Index Index Create Index Corpus Id Post\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" }","title":"POST /index/create-index/{corpus_id}"},{"location":"api/rest/rest_api/#delete-indexdrop-indexcorpus_id","text":"Supprimer l'index vectoriel d'un corpus Description Supprime l'index vectoriel pour un corpus sp\u00e9cifique. Args: corpus_id: Identifiant UUID du corpus. Returns: dict: R\u00e9sultat de l'op\u00e9ration. Raises: HTTPException: Si une erreur survient lors de la suppression de l'index. Input parameters Parameter In Type Default Nullable Description corpus_id path string No Identifiant UUID du corpus Response 200 OK application/json Schema of the response body { \"type\" : \"object\" , \"additionalProperties\" : true , \"title\" : \"Response Remove Corpus Index Index Drop Index Corpus Id Delete\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" }","title":"DELETE /index/drop-index/{corpus_id}"},{"location":"api/rest/rest_api/#get-indexindex-statuscorpus_id","text":"V\u00e9rifier l'\u00e9tat de l'index pour un corpus Description V\u00e9rifie l'\u00e9tat de l'index pour un corpus sp\u00e9cifique. Args: corpus_id: Identifiant UUID du corpus. Returns: dict: \u00c9tat de l'index et m\u00e9tadonn\u00e9es. Input parameters Parameter In Type Default Nullable Description corpus_id path string No Identifiant UUID du corpus Response 200 OK application/json Schema of the response body { \"type\" : \"object\" , \"additionalProperties\" : true , \"title\" : \"Response Get Index Status Index Index Status Corpus Id Get\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" }","title":"GET /index/index-status/{corpus_id}"},{"location":"api/rest/rest_api/#get-indexindexes","text":"V\u00e9rifier l'\u00e9tat de tous les index vectoriels Description V\u00e9rifie l'\u00e9tat de tous les index vectoriels. Returns: dict: \u00c9tat des index pour tous les corpus. Response 200 OK application/json Schema of the response body { \"additionalProperties\" : true , \"type\" : \"object\" , \"title\" : \"Response Get All Indexes Index Indexes Get\" }","title":"GET /index/indexes"},{"location":"api/rest/rest_api/#docloader","text":"","title":"DocLoader"},{"location":"api/rest/rest_api/#post-doc_loaderupload-file","text":"Uploader un fichier et le traiter Description Uploade un fichier, l'extrait et le divise en chunks. Args: file (UploadFile): Fichier upload\u00e9 par l'utilisateur. max_length (int): Taille maximale d'un chunk. Par d\u00e9faut 1000. theme (str): Th\u00e8me du document. Par d\u00e9faut \"Th\u00e8me g\u00e9n\u00e9rique\". Returns: List[DocumentWithChunks]: Liste des documents extraits. Raises: HTTPException: Si une erreur survient lors du traitement ou si aucun contenu n'est extrait. Input parameters Parameter In Type Default Nullable Description max_length query integer 1000 No Taille maximale d'un chunk theme query string Th\u00e8me g\u00e9n\u00e9rique No Th\u00e8me du document Request body multipart/form-data { \"file\" : \"TG9yZW0gaXBzdW0gZG9sb3Igc2l0IGFtZXQ=\" } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the request body { \"properties\" : { \"file\" : { \"type\" : \"string\" , \"format\" : \"binary\" , \"title\" : \"File\" , \"description\" : \"Fichier \u00e0 traiter\" } }, \"type\" : \"object\" , \"required\" : [ \"file\" ], \"title\" : \"Body_upload_and_process_file_doc_loader_upload_file_post\" } Response 200 OK application/json { \"document\" : { \"title\" : \"string\" , \"theme\" : \"string\" , \"documentType\" : \"string\" , \"publishDate\" : \"2022-04-13\" , \"corpusId\" : null }, \"chunks\" : [ { \"id\" : null , \"content\" : \"string\" , \"startChar\" : 0 , \"endChar\" : 0 , \"hierarchyLevel\" : 0 , \"parentChunkId\" : null } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"document\" : { \"$ref\" : \"#/components/schemas/DocumentCreate\" }, \"chunks\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ChunkCreate\" }, \"type\" : \"array\" , \"title\" : \"Chunks\" } }, \"type\" : \"object\" , \"required\" : [ \"document\" , \"chunks\" ], \"title\" : \"DocumentWithChunks\" , \"description\" : \"Payload complet pour `POST /database/documents`.\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" }","title":"POST /doc_loader/upload-file"},{"location":"api/rest/rest_api/#pipeline","text":"","title":"Pipeline"},{"location":"api/rest/rest_api/#post-pipelineprocess-and-store","text":"Charger un fichier, l'extraire et l'ins\u00e9rer dans la base de donn\u00e9es avec segmentation adaptative Description Charge un fichier, l'extrait avec segmentation hi\u00e9rarchique et l'ins\u00e8re dans la base de donn\u00e9es. Le fichier est temporairement sauvegard\u00e9 sur le disque, trait\u00e9 pour en extraire le contenu textuel, segment\u00e9 selon une approche hi\u00e9rarchique, puis ins\u00e9r\u00e9 dans la base de donn\u00e9es avec g\u00e9n\u00e9ration automatique d'embeddings. Args: file: Fichier upload\u00e9 par l'utilisateur. max_length: Taille maximale d'un chunk final. overlap: Chevauchement entre les chunks. theme: Th\u00e8me \u00e0 appliquer au document. document_type: Type de document (d\u00e9termin\u00e9 automatiquement si non sp\u00e9cifi\u00e9). corpus_id: Identifiant du corpus (g\u00e9n\u00e9r\u00e9 si non sp\u00e9cifi\u00e9). Returns: R\u00e9sultats de l'op\u00e9ration avec l'ID du document et les statistiques de segmentation. Raises: HTTPException: Si une erreur survient pendant le traitement du document. Input parameters Parameter In Type Default Nullable Description corpus_id query None No Identifiant du corpus (g\u00e9n\u00e9r\u00e9 si non sp\u00e9cifi\u00e9) max_length query integer 500 No Taille maximale d'un chunk overlap query integer 100 No Chevauchement entre les chunks theme query string Th\u00e8me g\u00e9n\u00e9rique No Th\u00e8me du document Request body multipart/form-data { \"file\" : \"TG9yZW0gaXBzdW0gZG9sb3Igc2l0IGFtZXQ=\" } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the request body { \"properties\" : { \"file\" : { \"type\" : \"string\" , \"format\" : \"binary\" , \"title\" : \"File\" , \"description\" : \"Fichier \u00e0 traiter\" } }, \"type\" : \"object\" , \"required\" : [ \"file\" ], \"title\" : \"Body_process_and_store_endpoint_pipeline_process_and_store_post\" } Response 200 OK application/json Schema of the response body { \"type\" : \"object\" , \"additionalProperties\" : true , \"title\" : \"Response Process And Store Endpoint Pipeline Process And Store Post\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" }","title":"POST /pipeline/process-and-store"},{"location":"api/rest/rest_api/#post-pipelineprocess-and-store-async","text":"Traiter un fichier en arri\u00e8re-plan et l'ins\u00e9rer dans la base de donn\u00e9es Description Traite un fichier en arri\u00e8re-plan et l'ins\u00e8re dans la base de donn\u00e9es. Similaire \u00e0 process-and-store mais s'ex\u00e9cute de mani\u00e8re asynchrone pour les fichiers volumineux. Le client re\u00e7oit imm\u00e9diatement une r\u00e9ponse avec un identifiant de t\u00e2che pendant que le traitement se poursuit en arri\u00e8re-plan. Args: background_tasks: Gestionnaire de t\u00e2ches en arri\u00e8re-plan de FastAPI. file: Fichier upload\u00e9 par l'utilisateur. max_length: Taille maximale d'un chunk final. overlap: Chevauchement entre les chunks. theme: Th\u00e8me \u00e0 appliquer au document. document_type: Type de document (d\u00e9termin\u00e9 automatiquement si non sp\u00e9cifi\u00e9). corpus_id: Identifiant du corpus (g\u00e9n\u00e9r\u00e9 si non sp\u00e9cifi\u00e9). Returns: Informations sur la t\u00e2che en arri\u00e8re-plan cr\u00e9\u00e9e. Input parameters Parameter In Type Default Nullable Description corpus_id query None No Identifiant du corpus (g\u00e9n\u00e9r\u00e9 si non sp\u00e9cifi\u00e9) max_length query integer 500 No Taille maximale d'un chunk overlap query integer 100 No Chevauchement entre les chunks theme query string Th\u00e8me g\u00e9n\u00e9rique No Th\u00e8me du document Request body multipart/form-data { \"file\" : \"TG9yZW0gaXBzdW0gZG9sb3Igc2l0IGFtZXQ=\" } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the request body { \"properties\" : { \"file\" : { \"type\" : \"string\" , \"format\" : \"binary\" , \"title\" : \"File\" , \"description\" : \"Fichier \u00e0 traiter\" } }, \"type\" : \"object\" , \"required\" : [ \"file\" ], \"title\" : \"Body_process_and_store_async_endpoint_pipeline_process_and_store_async_post\" } Response 200 OK application/json Schema of the response body { \"type\" : \"object\" , \"additionalProperties\" : true , \"title\" : \"Response Process And Store Async Endpoint Pipeline Process And Store Async Post\" } Response 422 Unprocessable Content application/json { \"detail\" : [ { \"loc\" : [ null ], \"msg\" : \"string\" , \"type\" : \"string\" } ] } \u26a0\ufe0f This example has been generated automatically from the schema and it is not accurate. Refer to the schema for more information. Schema of the response body { \"properties\" : { \"detail\" : { \"items\" : { \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" }","title":"POST /pipeline/process-and-store-async"},{"location":"api/rest/rest_api/#endpoints","text":"","title":"Endpoints"},{"location":"api/rest/rest_api/#get","text":"Root Description Endpoint racine pour v\u00e9rifier l'\u00e9tat de l'API. Returns: dict: Message indiquant que l'API est en cours d'ex\u00e9cution. Response 200 OK application/json Schema of the response body","title":"GET /"},{"location":"api/rest/rest_api/#schemas","text":"","title":"Schemas"},{"location":"api/rest/rest_api/#body_process_and_store_async_endpoint_pipeline_process_and_store_async_post","text":"Name Type file string ( binary )","title":"Body_process_and_store_async_endpoint_pipeline_process_and_store_async_post"},{"location":"api/rest/rest_api/#body_process_and_store_endpoint_pipeline_process_and_store_post","text":"Name Type file string ( binary )","title":"Body_process_and_store_endpoint_pipeline_process_and_store_post"},{"location":"api/rest/rest_api/#body_upload_and_process_file_doc_loader_upload_file_post","text":"Name Type file string ( binary )","title":"Body_upload_and_process_file_doc_loader_upload_file_post"},{"location":"api/rest/rest_api/#chunkcreate","text":"Name Type content string endChar integer hierarchyLevel integer id parentChunkId startChar integer","title":"ChunkCreate"},{"location":"api/rest/rest_api/#chunkresult","text":"Name Type chunkId integer content string context documentId integer documentType string hierarchyLevel integer publishDate string ( date ) score number theme string title string","title":"ChunkResult"},{"location":"api/rest/rest_api/#documentcreate","text":"Name Type corpusId documentType string publishDate string ( date ) theme string title string","title":"DocumentCreate"},{"location":"api/rest/rest_api/#documentresponse","text":"Name Type chunkCount integer corpusId documentType string id integer indexNeeded boolean publishDate string ( date ) theme string title string","title":"DocumentResponse"},{"location":"api/rest/rest_api/#documentupdate","text":"Name Type corpusId documentType id integer publishDate theme title","title":"DocumentUpdate"},{"location":"api/rest/rest_api/#documentwithchunks","text":"Name Type chunks Array< ChunkCreate > document DocumentCreate","title":"DocumentWithChunks"},{"location":"api/rest/rest_api/#hierarchicalcontext","text":"Name Type level0 level1 level2","title":"HierarchicalContext"},{"location":"api/rest/rest_api/#httpvalidationerror","text":"Name Type detail Array< ValidationError >","title":"HTTPValidationError"},{"location":"api/rest/rest_api/#searchrequest","text":"Name Type corpusId documentType endDate hierarchical boolean hierarchyLevel query string startDate theme topK integer","title":"SearchRequest"},{"location":"api/rest/rest_api/#searchresponse","text":"Name Type query string results Array< ChunkResult > topK integer totalResults integer","title":"SearchResponse"},{"location":"api/rest/rest_api/#updatewithchunks","text":"Name Type document DocumentUpdate newChunks","title":"UpdateWithChunks"},{"location":"api/rest/rest_api/#validationerror","text":"Name Type loc Array<> msg string type string","title":"ValidationError"},{"location":"lib/doc_loader/extractor_lib/","text":"G\u00e9n\u00e9ration de la documentation de la lib extractor L'utilisateur souhaite g\u00e9n\u00e9rer la documentation de la biblioth\u00e8que \"extractor\" \u00e0 partir d'un seul extracteur, tout en mentionnant qu'il y en a pour cinq types diff\u00e9rents : txt, json, docx, html et pdf. Je vais donc cr\u00e9er un fichier markdown qui d\u00e9crit la biblioth\u00e8que, en couvrant la classe de base, la m\u00e9thode build_document_with_chunks , et chaque extracteur sp\u00e9cifique. Il faudra aussi int\u00e9grer des r\u00e9f\u00e9rences crois\u00e9es dans le style de la documentation Python dans MkDocs. R\u00e9flexion durant 5 secondes # Librairie **doc_loader** (Extraction de documents) Ce module fournit une abstraction et des impl\u00e9mentations concr\u00e8tes pour charger et d\u00e9couper des documents de diff\u00e9rents formats en **`DocumentWithChunks`** , pr\u00eat \u00e0 \u00eatre inject\u00e9 dans la base de donn\u00e9es via l\u2019API **vectordb** . --- ## Installation La librairie `doc_loader` fait partie du package **`clea-doc-loader`** . Elle se charge automatiquement des d\u00e9pendances via votre `pyproject.toml` / `requirements.txt` . ```bash # depuis la racine du projet uv install . Structure du package doc_loader/ \u251c\u2500\u2500 extractor_factory.py # S\u00e9lection de l\u2019extracteur selon l\u2019extension \u251c\u2500\u2500 base.py # Interface et helpers communs \u251c\u2500\u2500 docs_loader.py # Point d\u2019entr\u00e9e : DocsLoader \u2514\u2500\u2500 data_extractor/ # 5 extracteurs concrets \u251c\u2500\u2500 txt_extractor.py # .txt \u251c\u2500\u2500 json_extractor.py # .json \u251c\u2500\u2500 docx_extractor.py # .docx \u251c\u2500\u2500 html_extractor.py # .html \u2514\u2500\u2500 pdf_extractor.py # .pdf 1. Interface commune BaseExtractor ( base.py ) class BaseExtractor ( ABC ): def __init__ ( self , file_path : str ) -> None : \"\"\"Chemin vers le fichier \u00e0 traiter.\"\"\" self . file_path = Path ( file_path ) @abstractmethod def extract_one ( self , * , max_length : int = 1000 ) -> DocumentWithChunks : \"\"\" Extrait l\u2019ensemble du document en un seul objet `DocumentWithChunks`. Args: max_length: taille cible des chunks finaux. Returns: DocumentWithChunks(document: DocumentCreate, chunks: List[ChunkCreate]) \"\"\" 2. Constructeur de payloads build_document_with_chunks(...) ( base.py ) Cette fonction choisit automatiquement entre : Mini-document (un seul chunk si len(full_text) \u2264 max_length ), Segmentation s\u00e9mantique (via NLP, _semantic_segmentation ), Fallback (d\u00e9coupage fixe + overlap). doc_with_chunks = build_document_with_chunks ( title = \"Rapport 2024\" , theme = \"RSE\" , document_type = \"PDF\" , publish_date = date . today (), max_length = 1000 , full_text = \"... texte complet ...\" ) # \u2192 DocumentWithChunks(document=DocumentCreate(...), # chunks=[ChunkCreate(...), ...]) 3. S\u00e9lection de l\u2019extracteur get_extractor(file_path: str) \u2192 BaseExtractor ( extractor_factory.py ) ext = get_extractor ( \"/chemin/vers/fichier.docx\" ) # ext est une instance de DocxExtractor, PdfExtractor, JsonExtractor, HtmlExtractor ou TxtExtractor. L\u00e8ve UnsupportedFileTypeError pour les extensions non list\u00e9es. 4. Point d\u2019entr\u00e9e : DocsLoader DocsLoader ( docs_loader.py ) loader = DocsLoader ( \"/chemin/fichier.txt\" ) doc_with_chunks = loader . extract_documents ( max_length = 1200 ) # renvoie un DocumentWithChunks unique extract_documents(...) d\u00e9l\u00e8gue \u00e0 extract_one() de l\u2019extracteur choisi. 5. Exemple d\u00e9taill\u00e9 : TxtExtractor TxtExtractor ( txt_extractor.py ) class TxtExtractor ( BaseExtractor ): def extract_one ( self , max_length : int = 1000 ) -> DocumentWithChunks : # 1) lit tout le fichier # 2) d\u00e9tecte si c\u2019est un JSON list\u00e9 \u2192 extrait m\u00e9tadonn\u00e9es + contenu # 3) sinon, m\u00e9tadonn\u00e9es par d\u00e9faut (stem, \"G\u00e9n\u00e9rique\", date.today()) # 4) appelle build_document_with_chunks(...) G\u00e8re automatiquement : Fichiers TXT \u00ab bruts \u00bb Fichiers TXT au format JSON [{\"title\":\u2026, \"theme\":\u2026, \"publish_date\":\u2026, \"content\":\u2026}, \u2026] 6. Autres extracteurs Chaque extracteur h\u00e9rite de BaseExtractor et impl\u00e9mente extract_one(...) de mani\u00e8re similaire, en utilisant : DocxExtractor ( .docx ) \u2192 segmentation par paragraphes et m\u00e9tadonn\u00e9es Office \ufe52 PdfExtractor ( .pdf ) \u2192 lecture pypdf , segmentation stream ou adaptive \ufe52 HtmlExtractor ( .html ) \u2192 BeautifulSoup , get_text() , segmentation \ufe52 JsonExtractor ( .json ) \u2192 parse JSON, extrait entries et segmente le contenu. Vous retrouverez la logique sp\u00e9cifique dans data_extractor/{docx,json,html,pdf}_extractor.py . 7. Usage typique from doc_loader.docs_loader import DocsLoader # 1. Choix de l\u2019extracteur et extraction loader = DocsLoader ( \"mon_fichier.pdf\" ) doc_payload = loader . extract_documents ( max_length = 800 ) # 2. Insertion en base (via vectordb) from vectordb.src.database import get_db , add_document_with_chunks db = next ( get_db ()) result = add_document_with_chunks ( db , doc_payload . document , doc_payload . chunks ) Module doc_loader stable \u2013 derni\u00e8re mise \u00e0 jour : 02 mai 2025 .","title":"Extracteurs"},{"location":"lib/doc_loader/extractor_lib/#structure-du-package","text":"doc_loader/ \u251c\u2500\u2500 extractor_factory.py # S\u00e9lection de l\u2019extracteur selon l\u2019extension \u251c\u2500\u2500 base.py # Interface et helpers communs \u251c\u2500\u2500 docs_loader.py # Point d\u2019entr\u00e9e : DocsLoader \u2514\u2500\u2500 data_extractor/ # 5 extracteurs concrets \u251c\u2500\u2500 txt_extractor.py # .txt \u251c\u2500\u2500 json_extractor.py # .json \u251c\u2500\u2500 docx_extractor.py # .docx \u251c\u2500\u2500 html_extractor.py # .html \u2514\u2500\u2500 pdf_extractor.py # .pdf","title":"Structure du package"},{"location":"lib/doc_loader/extractor_lib/#1-interface-commune","text":"","title":"1. Interface commune"},{"location":"lib/doc_loader/extractor_lib/#baseextractor-basepy","text":"class BaseExtractor ( ABC ): def __init__ ( self , file_path : str ) -> None : \"\"\"Chemin vers le fichier \u00e0 traiter.\"\"\" self . file_path = Path ( file_path ) @abstractmethod def extract_one ( self , * , max_length : int = 1000 ) -> DocumentWithChunks : \"\"\" Extrait l\u2019ensemble du document en un seul objet `DocumentWithChunks`. Args: max_length: taille cible des chunks finaux. Returns: DocumentWithChunks(document: DocumentCreate, chunks: List[ChunkCreate]) \"\"\"","title":"BaseExtractor (base.py)&#x20;"},{"location":"lib/doc_loader/extractor_lib/#2-constructeur-de-payloads","text":"","title":"2. Constructeur de payloads"},{"location":"lib/doc_loader/extractor_lib/#build_document_with_chunks-basepy","text":"Cette fonction choisit automatiquement entre : Mini-document (un seul chunk si len(full_text) \u2264 max_length ), Segmentation s\u00e9mantique (via NLP, _semantic_segmentation ), Fallback (d\u00e9coupage fixe + overlap). doc_with_chunks = build_document_with_chunks ( title = \"Rapport 2024\" , theme = \"RSE\" , document_type = \"PDF\" , publish_date = date . today (), max_length = 1000 , full_text = \"... texte complet ...\" ) # \u2192 DocumentWithChunks(document=DocumentCreate(...), # chunks=[ChunkCreate(...), ...])","title":"build_document_with_chunks(...) (base.py)&#x20;"},{"location":"lib/doc_loader/extractor_lib/#3-selection-de-lextracteur","text":"","title":"3. S\u00e9lection de l\u2019extracteur"},{"location":"lib/doc_loader/extractor_lib/#get_extractorfile_path-str-baseextractor-extractor_factorypy","text":"ext = get_extractor ( \"/chemin/vers/fichier.docx\" ) # ext est une instance de DocxExtractor, PdfExtractor, JsonExtractor, HtmlExtractor ou TxtExtractor. L\u00e8ve UnsupportedFileTypeError pour les extensions non list\u00e9es.","title":"get_extractor(file_path: str) \u2192 BaseExtractor (extractor_factory.py)&#x20;"},{"location":"lib/doc_loader/extractor_lib/#4-point-dentree-docsloader","text":"","title":"4. Point d\u2019entr\u00e9e : DocsLoader"},{"location":"lib/doc_loader/extractor_lib/#docsloader-docs_loaderpy","text":"loader = DocsLoader ( \"/chemin/fichier.txt\" ) doc_with_chunks = loader . extract_documents ( max_length = 1200 ) # renvoie un DocumentWithChunks unique extract_documents(...) d\u00e9l\u00e8gue \u00e0 extract_one() de l\u2019extracteur choisi.","title":"DocsLoader (docs_loader.py)&#x20;"},{"location":"lib/doc_loader/extractor_lib/#5-exemple-detaille-txtextractor","text":"","title":"5. Exemple d\u00e9taill\u00e9 : TxtExtractor"},{"location":"lib/doc_loader/extractor_lib/#txtextractor-txt_extractorpy","text":"class TxtExtractor ( BaseExtractor ): def extract_one ( self , max_length : int = 1000 ) -> DocumentWithChunks : # 1) lit tout le fichier # 2) d\u00e9tecte si c\u2019est un JSON list\u00e9 \u2192 extrait m\u00e9tadonn\u00e9es + contenu # 3) sinon, m\u00e9tadonn\u00e9es par d\u00e9faut (stem, \"G\u00e9n\u00e9rique\", date.today()) # 4) appelle build_document_with_chunks(...) G\u00e8re automatiquement : Fichiers TXT \u00ab bruts \u00bb Fichiers TXT au format JSON [{\"title\":\u2026, \"theme\":\u2026, \"publish_date\":\u2026, \"content\":\u2026}, \u2026]","title":"TxtExtractor (txt_extractor.py)&#x20;"},{"location":"lib/doc_loader/extractor_lib/#6-autres-extracteurs","text":"Chaque extracteur h\u00e9rite de BaseExtractor et impl\u00e9mente extract_one(...) de mani\u00e8re similaire, en utilisant : DocxExtractor ( .docx ) \u2192 segmentation par paragraphes et m\u00e9tadonn\u00e9es Office \ufe52 PdfExtractor ( .pdf ) \u2192 lecture pypdf , segmentation stream ou adaptive \ufe52 HtmlExtractor ( .html ) \u2192 BeautifulSoup , get_text() , segmentation \ufe52 JsonExtractor ( .json ) \u2192 parse JSON, extrait entries et segmente le contenu. Vous retrouverez la logique sp\u00e9cifique dans data_extractor/{docx,json,html,pdf}_extractor.py .","title":"6. Autres extracteurs"},{"location":"lib/doc_loader/extractor_lib/#7-usage-typique","text":"from doc_loader.docs_loader import DocsLoader # 1. Choix de l\u2019extracteur et extraction loader = DocsLoader ( \"mon_fichier.pdf\" ) doc_payload = loader . extract_documents ( max_length = 800 ) # 2. Insertion en base (via vectordb) from vectordb.src.database import get_db , add_document_with_chunks db = next ( get_db ()) result = add_document_with_chunks ( db , doc_payload . document , doc_payload . chunks ) Module doc_loader stable \u2013 derni\u00e8re mise \u00e0 jour : 02 mai 2025 .","title":"7. Usage typique"},{"location":"lib/doc_loader/splitter_lib/","text":"Librairie splitter Ce module fournit des algorithmes de segmentation hi\u00e9rarchique et s\u00e9mantique de textes, avec des strat\u00e9gies de secours pour les corpus non structur\u00e9s. Il est con\u00e7u pour d\u00e9couper efficacement de grands documents en chunks exploitables par le pipeline d'indexation vectorielle. Table des mati\u00e8res Constantes globales Segmentation principale semantic_segmentation_stream _semantic_segmentation fallback_segmentation_stream _fallback_segmentation Extraction s\u00e9mantique _extract_semantic_sections _extract_semantic_paragraphs _create_semantic_chunks Utilitaires texte _get_meaningful_preview is_sentence_boundary find_paragraph_boundaries Exemple d'utilisation 1. Constantes globales D\u00e9finissent les param\u00e8tres limites pour le d\u00e9coupage et le mode de fonctionnement \"stream\" ou \"fallback\". Constante Valeur Description THRESHOLD_LARGE 5 000 000 Seuil (en octets) pour basculer en mode \"stream\" sur gros fichiers MAX_CHUNKS 5 000 Nombre max de chunks g\u00e9n\u00e9r\u00e9s MAX_TEXT_LENGTH 20 000 000 Longueur texte max support\u00e9e MAX_CHUNK_SIZE 8 000 Taille max d'un chunk (chars) MIN_LEVEL3_LENGTH 200 Seuil min pour chunks niveau 3 MAX_LEVEL3_CHUNKS 100 Nombre max de sous-chunks niveau 3 par paragraphe 2. Segmentation principale ( segmentation.py ) semantic_segmentation_stream(text: str, max_length: int) \u2192 Iterator[ChunkCreate] Description: R\u00e9alise un d\u00e9coupage s\u00e9mantique hi\u00e9rarchique en 4 niveaux (0-3) avec gestion optimis\u00e9e de la m\u00e9moire. Args: text: Texte source \u00e0 segmenter max_length: Longueur maximale souhait\u00e9e des chunks Yields: Objets ChunkCreate g\u00e9n\u00e9r\u00e9s s\u00e9quentiellement (mode streaming) \u00c9tapes: Chunk racine (niveau 0) Sections s\u00e9mantiques (niv. 1) via _extract_semantic_sections Paragraphes (niv. 2) via _extract_semantic_paragraphs Sous-chunks (niv. 3) via _create_semantic_chunks S\u00e9curit\u00e9: \u00c9vite les duplications S'arr\u00eate \u00e0 MAX_CHUNKS _semantic_segmentation(text: str, max_length: int) \u2192 List[ChunkCreate] Description: Retourne la liste compl\u00e8te des chunks en utilisant le g\u00e9n\u00e9rateur semantic_segmentation_stream . Args: text: Texte source \u00e0 segmenter max_length: Longueur maximale souhait\u00e9e des chunks Returns: Liste des chunks g\u00e9n\u00e9r\u00e9s, limit\u00e9e \u00e0 MAX_CHUNKS fallback_segmentation_stream(text: str, max_length: int) \u2192 Iterator[ChunkCreate] Description: M\u00e9thode de segmentation de secours robuste pour textes non structur\u00e9s. Args: text: Texte source \u00e0 segmenter max_length: Longueur maximale souhait\u00e9e des chunks Yields: Objets ChunkCreate g\u00e9n\u00e9r\u00e9s s\u00e9quentiellement Strat\u00e9gie: Chunk racine (aper\u00e7u) Segments glissants de taille min(max_length*2, MAX_CHUNK_SIZE) Tentatives de coupure naturelle (phrases, paragraphes) Chevauchement d'environ 10% _fallback_segmentation(text: str, max_length: int) \u2192 List[ChunkCreate] Description: Retourne la segmentation compl\u00e8te des chunks produits par fallback_segmentation_stream . Args: text: Texte source \u00e0 segmenter max_length: Longueur maximale souhait\u00e9e des chunks Returns: Liste des chunks g\u00e9n\u00e9r\u00e9s, limit\u00e9e \u00e0 MAX_CHUNKS 3. Extraction s\u00e9mantique ( text_analysis.py ) _extract_semantic_sections(text: str, max_sections: int = 20) \u2192 List[Dict] Description: Identifie les sections s\u00e9mantiques dans un texte. Args: text: Texte \u00e0 analyser max_sections: Nombre maximal de sections \u00e0 extraire Returns: Liste de dictionnaires [{title, content, start_char, end_char}, ...] M\u00e9thode: D\u00e9tecte titres formels (Markdown, soulign\u00e9s) puis s\u00e9parateurs naturels (sauts de ligne multiples) Si insuffisant, d\u00e9coupe artificiellement en blocs _extract_semantic_paragraphs(text: str, base_offset: int = 0, max_paragraphs: int = 20) \u2192 List[Dict] Description: Divise un texte en paragraphes s\u00e9mantiques. Args: text: Texte \u00e0 analyser base_offset: D\u00e9calage de caract\u00e8res \u00e0 appliquer dans le document original max_paragraphs: Nombre maximal de paragraphes \u00e0 extraire Returns: Liste de dictionnaires [{content, start_char, end_char}, ...] M\u00e9thode: S\u00e9pare sur \\n\\n Si trop peu de blocs, d\u00e9coupes par phrases ou artificiellement Regroupe petits blocs pour coh\u00e9rence _create_semantic_chunks(text: str, max_length: int, min_overlap: int = 50, base_offset: int = 0, max_chunks: int = 20) \u2192 List[Dict] Description: Cr\u00e9e des chunks s\u00e9mantiques \u00e0 partir d'un texte. Args: text: Texte \u00e0 d\u00e9couper max_length: Longueur maximale d'un chunk min_overlap: Chevauchement minimal entre chunks cons\u00e9cutifs base_offset: D\u00e9calage de caract\u00e8res \u00e0 appliquer dans le document original max_chunks: Nombre maximal de chunks \u00e0 g\u00e9n\u00e9rer Returns: Liste de dictionnaires [{content, start_char, end_char}, ...] M\u00e9thode: D\u00e9coupage sur fronti\u00e8res de phrases ou paragraphes Ajustement de effective_max et effective_overlap selon longueur du texte 4. Utilitaires texte ( text_utils.py ) _get_meaningful_preview(text: str, max_length: int) \u2192 str Description: Extrait un aper\u00e7u significatif d'un texte. Args: text: Texte source max_length: Longueur maximale de l'aper\u00e7u Returns: Aper\u00e7u combinant d\u00e9but, phrases cl\u00e9s et fin du texte M\u00e9thode: Extrait du d\u00e9but du texte S\u00e9lectionne phrases cl\u00e9s du milieu (contenant: \"essentiel\", \"cl\u00e9\", etc.) Inclut la fin du texte is_sentence_boundary(text: str, pos: int) \u2192 bool Description: V\u00e9rifie si une position donn\u00e9e correspond \u00e0 une fronti\u00e8re de phrase. Args: text: Texte \u00e0 analyser pos: Position \u00e0 v\u00e9rifier Returns: True si la position marque une fin de phrase, False sinon Crit\u00e8res: Pr\u00e9sence de . ! ? suivi d'un espace ou de la fin de cha\u00eene. find_paragraph_boundaries(text: str) \u2192 List[int] Description: Identifie les positions de d\u00e9but de chaque paragraphe. Args: text: Texte \u00e0 analyser Returns: Liste des indices de d\u00e9but de paragraphe M\u00e9thode: D\u00e9tection des s\u00e9parations de type \\n\\s*\\n . 5. Exemple d'utilisation from splitter.segmentation import semantic_segmentation_stream , fallback_segmentation_stream text = open ( \"mon_document.txt\" , \"r\" , encoding = \"utf-8\" ) . read () # 1. Segmentation s\u00e9mantique (recommand\u00e9e) chunks = list ( semantic_segmentation_stream ( text , max_length = 1000 )) print ( f \" { len ( chunks ) } chunks g\u00e9n\u00e9r\u00e9s (niveaux 0\u20133)\" ) # 2. Fallback si \u00e9chec ou corpus simple if len ( chunks ) == 1 : chunks = list ( fallback_segmentation_stream ( text , max_length = 800 )) print ( f \"Fallback : { len ( chunks ) } chunks g\u00e9n\u00e9r\u00e9s\" )","title":"Segmentation"},{"location":"lib/doc_loader/splitter_lib/#librairie-splitter","text":"Ce module fournit des algorithmes de segmentation hi\u00e9rarchique et s\u00e9mantique de textes, avec des strat\u00e9gies de secours pour les corpus non structur\u00e9s. Il est con\u00e7u pour d\u00e9couper efficacement de grands documents en chunks exploitables par le pipeline d'indexation vectorielle.","title":"Librairie splitter"},{"location":"lib/doc_loader/splitter_lib/#table-des-matieres","text":"Constantes globales Segmentation principale semantic_segmentation_stream _semantic_segmentation fallback_segmentation_stream _fallback_segmentation Extraction s\u00e9mantique _extract_semantic_sections _extract_semantic_paragraphs _create_semantic_chunks Utilitaires texte _get_meaningful_preview is_sentence_boundary find_paragraph_boundaries Exemple d'utilisation","title":"Table des mati\u00e8res"},{"location":"lib/doc_loader/splitter_lib/#1-constantes-globales","text":"D\u00e9finissent les param\u00e8tres limites pour le d\u00e9coupage et le mode de fonctionnement \"stream\" ou \"fallback\". Constante Valeur Description THRESHOLD_LARGE 5 000 000 Seuil (en octets) pour basculer en mode \"stream\" sur gros fichiers MAX_CHUNKS 5 000 Nombre max de chunks g\u00e9n\u00e9r\u00e9s MAX_TEXT_LENGTH 20 000 000 Longueur texte max support\u00e9e MAX_CHUNK_SIZE 8 000 Taille max d'un chunk (chars) MIN_LEVEL3_LENGTH 200 Seuil min pour chunks niveau 3 MAX_LEVEL3_CHUNKS 100 Nombre max de sous-chunks niveau 3 par paragraphe","title":"1. Constantes globales"},{"location":"lib/doc_loader/splitter_lib/#2-segmentation-principale-segmentationpy","text":"","title":"2. Segmentation principale (segmentation.py)"},{"location":"lib/doc_loader/splitter_lib/#semantic_segmentation_streamtext-str-max_length-int-iteratorchunkcreate","text":"Description: R\u00e9alise un d\u00e9coupage s\u00e9mantique hi\u00e9rarchique en 4 niveaux (0-3) avec gestion optimis\u00e9e de la m\u00e9moire. Args: text: Texte source \u00e0 segmenter max_length: Longueur maximale souhait\u00e9e des chunks Yields: Objets ChunkCreate g\u00e9n\u00e9r\u00e9s s\u00e9quentiellement (mode streaming) \u00c9tapes: Chunk racine (niveau 0) Sections s\u00e9mantiques (niv. 1) via _extract_semantic_sections Paragraphes (niv. 2) via _extract_semantic_paragraphs Sous-chunks (niv. 3) via _create_semantic_chunks S\u00e9curit\u00e9: \u00c9vite les duplications S'arr\u00eate \u00e0 MAX_CHUNKS","title":"semantic_segmentation_stream(text: str, max_length: int) \u2192 Iterator[ChunkCreate]"},{"location":"lib/doc_loader/splitter_lib/#_semantic_segmentationtext-str-max_length-int-listchunkcreate","text":"Description: Retourne la liste compl\u00e8te des chunks en utilisant le g\u00e9n\u00e9rateur semantic_segmentation_stream . Args: text: Texte source \u00e0 segmenter max_length: Longueur maximale souhait\u00e9e des chunks Returns: Liste des chunks g\u00e9n\u00e9r\u00e9s, limit\u00e9e \u00e0 MAX_CHUNKS","title":"_semantic_segmentation(text: str, max_length: int) \u2192 List[ChunkCreate]"},{"location":"lib/doc_loader/splitter_lib/#fallback_segmentation_streamtext-str-max_length-int-iteratorchunkcreate","text":"Description: M\u00e9thode de segmentation de secours robuste pour textes non structur\u00e9s. Args: text: Texte source \u00e0 segmenter max_length: Longueur maximale souhait\u00e9e des chunks Yields: Objets ChunkCreate g\u00e9n\u00e9r\u00e9s s\u00e9quentiellement Strat\u00e9gie: Chunk racine (aper\u00e7u) Segments glissants de taille min(max_length*2, MAX_CHUNK_SIZE) Tentatives de coupure naturelle (phrases, paragraphes) Chevauchement d'environ 10%","title":"fallback_segmentation_stream(text: str, max_length: int) \u2192 Iterator[ChunkCreate]"},{"location":"lib/doc_loader/splitter_lib/#_fallback_segmentationtext-str-max_length-int-listchunkcreate","text":"Description: Retourne la segmentation compl\u00e8te des chunks produits par fallback_segmentation_stream . Args: text: Texte source \u00e0 segmenter max_length: Longueur maximale souhait\u00e9e des chunks Returns: Liste des chunks g\u00e9n\u00e9r\u00e9s, limit\u00e9e \u00e0 MAX_CHUNKS","title":"_fallback_segmentation(text: str, max_length: int) \u2192 List[ChunkCreate]"},{"location":"lib/doc_loader/splitter_lib/#3-extraction-semantique-text_analysispy","text":"","title":"3. Extraction s\u00e9mantique (text_analysis.py)"},{"location":"lib/doc_loader/splitter_lib/#_extract_semantic_sectionstext-str-max_sections-int-20-listdict","text":"Description: Identifie les sections s\u00e9mantiques dans un texte. Args: text: Texte \u00e0 analyser max_sections: Nombre maximal de sections \u00e0 extraire Returns: Liste de dictionnaires [{title, content, start_char, end_char}, ...] M\u00e9thode: D\u00e9tecte titres formels (Markdown, soulign\u00e9s) puis s\u00e9parateurs naturels (sauts de ligne multiples) Si insuffisant, d\u00e9coupe artificiellement en blocs","title":"_extract_semantic_sections(text: str, max_sections: int = 20) \u2192 List[Dict]"},{"location":"lib/doc_loader/splitter_lib/#_extract_semantic_paragraphstext-str-base_offset-int-0-max_paragraphs-int-20-listdict","text":"Description: Divise un texte en paragraphes s\u00e9mantiques. Args: text: Texte \u00e0 analyser base_offset: D\u00e9calage de caract\u00e8res \u00e0 appliquer dans le document original max_paragraphs: Nombre maximal de paragraphes \u00e0 extraire Returns: Liste de dictionnaires [{content, start_char, end_char}, ...] M\u00e9thode: S\u00e9pare sur \\n\\n Si trop peu de blocs, d\u00e9coupes par phrases ou artificiellement Regroupe petits blocs pour coh\u00e9rence","title":"_extract_semantic_paragraphs(text: str, base_offset: int = 0, max_paragraphs: int = 20) \u2192 List[Dict]"},{"location":"lib/doc_loader/splitter_lib/#_create_semantic_chunkstext-str-max_length-int-min_overlap-int-50-base_offset-int-0-max_chunks-int-20-listdict","text":"Description: Cr\u00e9e des chunks s\u00e9mantiques \u00e0 partir d'un texte. Args: text: Texte \u00e0 d\u00e9couper max_length: Longueur maximale d'un chunk min_overlap: Chevauchement minimal entre chunks cons\u00e9cutifs base_offset: D\u00e9calage de caract\u00e8res \u00e0 appliquer dans le document original max_chunks: Nombre maximal de chunks \u00e0 g\u00e9n\u00e9rer Returns: Liste de dictionnaires [{content, start_char, end_char}, ...] M\u00e9thode: D\u00e9coupage sur fronti\u00e8res de phrases ou paragraphes Ajustement de effective_max et effective_overlap selon longueur du texte","title":"_create_semantic_chunks(text: str, max_length: int, min_overlap: int = 50, base_offset: int = 0, max_chunks: int = 20) \u2192 List[Dict]"},{"location":"lib/doc_loader/splitter_lib/#4-utilitaires-texte-text_utilspy","text":"","title":"4. Utilitaires texte (text_utils.py)"},{"location":"lib/doc_loader/splitter_lib/#_get_meaningful_previewtext-str-max_length-int-str","text":"Description: Extrait un aper\u00e7u significatif d'un texte. Args: text: Texte source max_length: Longueur maximale de l'aper\u00e7u Returns: Aper\u00e7u combinant d\u00e9but, phrases cl\u00e9s et fin du texte M\u00e9thode: Extrait du d\u00e9but du texte S\u00e9lectionne phrases cl\u00e9s du milieu (contenant: \"essentiel\", \"cl\u00e9\", etc.) Inclut la fin du texte","title":"_get_meaningful_preview(text: str, max_length: int) \u2192 str"},{"location":"lib/doc_loader/splitter_lib/#is_sentence_boundarytext-str-pos-int-bool","text":"Description: V\u00e9rifie si une position donn\u00e9e correspond \u00e0 une fronti\u00e8re de phrase. Args: text: Texte \u00e0 analyser pos: Position \u00e0 v\u00e9rifier Returns: True si la position marque une fin de phrase, False sinon Crit\u00e8res: Pr\u00e9sence de . ! ? suivi d'un espace ou de la fin de cha\u00eene.","title":"is_sentence_boundary(text: str, pos: int) \u2192 bool"},{"location":"lib/doc_loader/splitter_lib/#find_paragraph_boundariestext-str-listint","text":"Description: Identifie les positions de d\u00e9but de chaque paragraphe. Args: text: Texte \u00e0 analyser Returns: Liste des indices de d\u00e9but de paragraphe M\u00e9thode: D\u00e9tection des s\u00e9parations de type \\n\\s*\\n .","title":"find_paragraph_boundaries(text: str) \u2192 List[int]"},{"location":"lib/doc_loader/splitter_lib/#5-exemple-dutilisation","text":"from splitter.segmentation import semantic_segmentation_stream , fallback_segmentation_stream text = open ( \"mon_document.txt\" , \"r\" , encoding = \"utf-8\" ) . read () # 1. Segmentation s\u00e9mantique (recommand\u00e9e) chunks = list ( semantic_segmentation_stream ( text , max_length = 1000 )) print ( f \" { len ( chunks ) } chunks g\u00e9n\u00e9r\u00e9s (niveaux 0\u20133)\" ) # 2. Fallback si \u00e9chec ou corpus simple if len ( chunks ) == 1 : chunks = list ( fallback_segmentation_stream ( text , max_length = 800 )) print ( f \"Fallback : { len ( chunks ) } chunks g\u00e9n\u00e9r\u00e9s\" )","title":"5. Exemple d'utilisation"},{"location":"lib/pipeline/pipeline_lib/","text":"G\u00e9n\u00e9ration de documentation L'utilisateur demande \u00e0 g\u00e9n\u00e9rer une documentation pour la biblioth\u00e8que \"pipeline\". Il semble que le fichier \"pipeline.py\" actuel soit plus simple et contienne l'impl\u00e9mentation de \"process_and_store\" et \"determine_document_type\", alors qu'une version ant\u00e9rieure incluait d'autres \u00e9l\u00e9ments comme la classe \"InterfaceDocument\". Il semble qu'ils veuillent une documentation markdown bas\u00e9e sur ce script actuel. Il faudra alors bien documenter ces deux fonctions principales ainsi que leur r\u00f4le dans le cadre de cette biblioth\u00e8que. Documentant la biblioth\u00e8que pipeline L'utilisateur souhaite une documentation markdown pour la biblioth\u00e8que pipeline, bas\u00e9e sur \"pipeline.py\". Il semble que ce fichier d\u00e9finisse uniquement deux fonctions : process_and_store et determine_document_type . Nous devrons fournir des informations sur leur signature, leur fonctionnement, leurs arguments, valeurs de retour, exceptions et exemples d'utilisation, sans mentionner de classe \"InterfaceDocument\" qui n'est pas pr\u00e9sente ici. Le tout sera structur\u00e9 avec une introduction, installation et exemples, en pr\u00e9cisant que cette biblioth\u00e8que fait partie de clea-pipeline . R\u00e9flexion durant 13 secondes # Module **pipeline** Orchestration du traitement de documents : extraction, segmentation et insertion en base. --- ## Installation Le module `pipeline.py` fait partie du package **clea-pipeline** . Pour l\u2019installer : ```bash pip install clea-pipeline Fonctions principales process_and_store def process_and_store ( file_path : str , max_length : int = 500 , overlap : int = 100 , theme : Optional [ str ] = \"Th\u00e8me g\u00e9n\u00e9rique\" , corpus_id : Optional [ str ] = None , ) -> Dict [ str , Any ]: Description V\u00e9rifie que le fichier existe (l\u00e8ve FileNotFoundError sinon). Extrait et segmente le document en chunks hi\u00e9rarchiques via DocsLoader . Applique le th\u00e8me si fourni. Ins\u00e8re le document et ses chunks en base via add_document_with_chunks . Retourne le r\u00e9sultat contenant : document_id : ID du document cr\u00e9\u00e9 chunks : nombre de chunks ins\u00e9r\u00e9s corpus_id : UUID du corpus create_index : bool\u00e9en indiquant si un index doit \u00eatre (re)cr\u00e9\u00e9 index_message : message d\u2019instruction pour la cr\u00e9ation d\u2019index (si applicable) Param\u00e8tres Nom Type Description file_path str Chemin vers le fichier \u00e0 traiter max_length int Taille max d\u2019un chunk final (d\u00e9faut 500) overlap int Chevauchement entre chunks (d\u00e9faut 100) theme Optional[str] Th\u00e8me \u00e0 appliquer (d\u00e9faut \"Th\u00e8me g\u00e9n\u00e9rique\") corpus_id Optional[str] UUID du corpus (g\u00e9n\u00e9r\u00e9 si None) Retour { \"document_id\" : 1 , \"chunks\" : 2 , \"corpus_id\" : \"e0428ce9-4a0a-445d-8f35-f5c9bed89c67\" , \"create_index\" : true , \"index_message\" : \"Un nouvel index pour le corpus \u2026 doit \u00eatre cr\u00e9\u00e9. Utilisez POST /database/indexes/{corpus_id}/create.\" } Exceptions FileNotFoundError : si file_path inexistant ValueError : si aucune extraction ou si l\u2019insertion \u00e9choue Exemple from pipeline import process_and_store result = process_and_store ( \"demo/report.pdf\" , max_length = 800 , overlap = 150 , theme = \"Finance\" , ) print ( result ) # { # \"document_id\": 10, # \"chunks\": 42, # \"corpus_id\": \"abcd-1234-\u2026\", # \"create_index\": true, # \"index_message\": \"\u2026\" # } determine_document_type def determine_document_type ( file_path : str ) -> str : Description D\u00e9duit le type du document ( PDF , TXT , WORD , etc.) \u00e0 partir de l\u2019extension du fichier. Param\u00e8tre Nom Type Description file_path str Chemin complet vers le fichier Retour Une des valeurs suivantes : PDF, TXT, MARKDOWN, WORD, HTML, XML, CSV, JSON, POWERPOINT, EXCEL, UNKNOWN Exemple >>> determine_document_type ( \"guide.docx\" ) \"WORD\" >>> determine_document_type ( \"notes.md\" ) \"MARKDOWN\" Utilisation typique from pipeline import process_and_store , determine_document_type # 1. D\u00e9terminer le type (facultatif) doc_type = determine_document_type ( \"report.pdf\" ) # 2. Traiter et stocker en base res = process_and_store ( \"report.pdf\" , max_length = 1000 , overlap = 200 , theme = \"RSE\" , ) print ( res ) Module : pipeline.py Derni\u00e8re mise \u00e0 jour : 02 mai 2025 Source : pipeline/src/pipeline.py","title":"Pipeline end-to-end"},{"location":"lib/pipeline/pipeline_lib/#fonctions-principales","text":"","title":"Fonctions principales"},{"location":"lib/pipeline/pipeline_lib/#process_and_store","text":"def process_and_store ( file_path : str , max_length : int = 500 , overlap : int = 100 , theme : Optional [ str ] = \"Th\u00e8me g\u00e9n\u00e9rique\" , corpus_id : Optional [ str ] = None , ) -> Dict [ str , Any ]:","title":"process_and_store&#x20;"},{"location":"lib/pipeline/pipeline_lib/#description","text":"V\u00e9rifie que le fichier existe (l\u00e8ve FileNotFoundError sinon). Extrait et segmente le document en chunks hi\u00e9rarchiques via DocsLoader . Applique le th\u00e8me si fourni. Ins\u00e8re le document et ses chunks en base via add_document_with_chunks . Retourne le r\u00e9sultat contenant : document_id : ID du document cr\u00e9\u00e9 chunks : nombre de chunks ins\u00e9r\u00e9s corpus_id : UUID du corpus create_index : bool\u00e9en indiquant si un index doit \u00eatre (re)cr\u00e9\u00e9 index_message : message d\u2019instruction pour la cr\u00e9ation d\u2019index (si applicable)","title":"Description"},{"location":"lib/pipeline/pipeline_lib/#parametres","text":"Nom Type Description file_path str Chemin vers le fichier \u00e0 traiter max_length int Taille max d\u2019un chunk final (d\u00e9faut 500) overlap int Chevauchement entre chunks (d\u00e9faut 100) theme Optional[str] Th\u00e8me \u00e0 appliquer (d\u00e9faut \"Th\u00e8me g\u00e9n\u00e9rique\") corpus_id Optional[str] UUID du corpus (g\u00e9n\u00e9r\u00e9 si None)","title":"Param\u00e8tres"},{"location":"lib/pipeline/pipeline_lib/#retour","text":"{ \"document_id\" : 1 , \"chunks\" : 2 , \"corpus_id\" : \"e0428ce9-4a0a-445d-8f35-f5c9bed89c67\" , \"create_index\" : true , \"index_message\" : \"Un nouvel index pour le corpus \u2026 doit \u00eatre cr\u00e9\u00e9. Utilisez POST /database/indexes/{corpus_id}/create.\" }","title":"Retour"},{"location":"lib/pipeline/pipeline_lib/#exceptions","text":"FileNotFoundError : si file_path inexistant ValueError : si aucune extraction ou si l\u2019insertion \u00e9choue","title":"Exceptions"},{"location":"lib/pipeline/pipeline_lib/#exemple","text":"from pipeline import process_and_store result = process_and_store ( \"demo/report.pdf\" , max_length = 800 , overlap = 150 , theme = \"Finance\" , ) print ( result ) # { # \"document_id\": 10, # \"chunks\": 42, # \"corpus_id\": \"abcd-1234-\u2026\", # \"create_index\": true, # \"index_message\": \"\u2026\" # }","title":"Exemple"},{"location":"lib/pipeline/pipeline_lib/#determine_document_type","text":"def determine_document_type ( file_path : str ) -> str :","title":"determine_document_type&#x20;"},{"location":"lib/pipeline/pipeline_lib/#description_1","text":"D\u00e9duit le type du document ( PDF , TXT , WORD , etc.) \u00e0 partir de l\u2019extension du fichier.","title":"Description"},{"location":"lib/pipeline/pipeline_lib/#parametre","text":"Nom Type Description file_path str Chemin complet vers le fichier","title":"Param\u00e8tre"},{"location":"lib/pipeline/pipeline_lib/#retour_1","text":"Une des valeurs suivantes : PDF, TXT, MARKDOWN, WORD, HTML, XML, CSV, JSON, POWERPOINT, EXCEL, UNKNOWN","title":"Retour"},{"location":"lib/pipeline/pipeline_lib/#exemple_1","text":">>> determine_document_type ( \"guide.docx\" ) \"WORD\" >>> determine_document_type ( \"notes.md\" ) \"MARKDOWN\"","title":"Exemple"},{"location":"lib/pipeline/pipeline_lib/#utilisation-typique","text":"from pipeline import process_and_store , determine_document_type # 1. D\u00e9terminer le type (facultatif) doc_type = determine_document_type ( \"report.pdf\" ) # 2. Traiter et stocker en base res = process_and_store ( \"report.pdf\" , max_length = 1000 , overlap = 200 , theme = \"RSE\" , ) print ( res ) Module : pipeline.py Derni\u00e8re mise \u00e0 jour : 02 mai 2025 Source : pipeline/src/pipeline.py","title":"Utilisation typique"},{"location":"lib/vectordb/crud_lib/","text":"R\u00e9flexion durant 4 secondes # Module `crud` Ce module fournit des op\u00e9rations CRUD de haut niveau sur les entit\u00e9s **Document** , **Chunk** et **IndexConfig** , avec gestion des embeddings et des index pgvector. Fichier source : `vectordb/src/crud.py` :contentReference[oaicite:0]{index=0}:contentReference[oaicite:1]{index=1} --- ## Installation Ce module est inclus dans le package `vectordb` . Pour l\u2019installer : ```bash pip install vectordb Table des mati\u00e8res add_document_with_chunks update_document_with_chunks delete_document_chunks delete_document 1. add_document_with_chunks(db, doc, chunks, batch_size=10) \u2192 Dict[str, Any] Ajoute un document et ses chunks en base, g\u00e9n\u00e8re les embeddings en lot, et g\u00e8re la configuration de l\u2019index. from sqlalchemy.orm import Session from vectordb.src.schemas import DocumentCreate from vectordb.src.crud import add_document_with_chunks result = add_document_with_chunks ( db : Session , doc : DocumentCreate , chunks : List [ dict ], batch_size = 20 ) Description G\u00e9n\u00e8re corpus_id si manquant. Ins\u00e8re le document ( Document ) avec un flag index_needed . Pour chaque lot de batch_size chunks : Calcule les embeddings via EmbeddingGenerator.generate_embeddings_batch . Ins\u00e8re les objets Chunk (avec flush interm\u00e9diaires). Construit les relations parent\u2194enfant. Met \u00e0 jour ou cr\u00e9e la configuration d\u2019index ( IndexConfig ). Commit ou rollback en cas d\u2019erreur. Param\u00e8tres Nom Type Description db Session Session SQLAlchemy active. doc DocumentCreate M\u00e9tadonn\u00e9es du document \u00e0 cr\u00e9er. chunks List[dict] Liste de chunks { content, hierarchy_level, ... } . batch_size int (d\u00e9faut 10) Taille des sous-lots pour la g\u00e9n\u00e9ration d\u2019embeddings. Retour { \"document_id\" : i nt , \"chunks\" : i nt , \"corpus_id\" : s tr , \"index_needed\" : bool } index_needed = True si un nouvel index doit \u00eatre (re)cr\u00e9\u00e9. 2. update_document_with_chunks(document_update, new_chunks=None) \u2192 Dict[str, Any] Met \u00e0 jour un document existant et ajoute \u00e9ventuellement de nouveaux chunks. from vectordb.src.schemas import DocumentUpdate from vectordb.src.crud import update_document_with_chunks result = update_document_with_chunks ( document_update : DocumentUpdate , new_chunks : List [ dict ] # facultatif ) Description Charge le Document par son id . Met \u00e0 jour les champs fournis ( title , theme , etc.). Si new_chunks est fourni : Calcule les embeddings un par un. Ins\u00e8re en bulk via insert(Chunk) . Met \u00e0 jour le compteur chunk_count dans IndexConfig . Si corpus_id change, ajuste les compteurs sur les anciennes/nouvelles configurations d\u2019index. Retourne les m\u00e9tadonn\u00e9es mises \u00e0 jour et si index_needed suite \u00e0 un changement de corpus. Param\u00e8tres Nom Type Description document_update DocumentUpdate DTO avec l\u2019 id du document et champs modifi\u00e9s. new_chunks List[dict] (optionnel) Nouveaux chunks \u00e0 ajouter. Retour { \"id\" : i nt , \"title\" : s tr , \"theme\" : s tr , \"document_type\" : s tr , \"publish_date\" : da te , \"corpus_id\" : s tr , \"chunks\" : { \"total\" : i nt , \"added\" : i nt }, \"index_needed\" : bool } En cas de document introuvable : {\"error\": \"\u2026 introuvable.\"} 3. delete_document_chunks(document_id, chunk_ids=None) \u2192 Dict[str, Any] Supprime un ou plusieurs chunks d\u2019un document, ou tous si chunk_ids non fourni. from vectordb.src.crud import delete_document_chunks result = delete_document_chunks ( document_id : int , chunk_ids : Optional [ List [ int ]]) Description V\u00e9rifie l\u2019existence du Document . Si chunk_ids est une liste : Supprime uniquement ces chunks. Sinon : Supprime tous les chunks associ\u00e9s. Met \u00e0 jour chunk_count dans IndexConfig . Commit ou rollback en cas d\u2019erreur. Param\u00e8tres Nom Type Description document_id int Identifiant du document. chunk_ids List[int] (optionnel) Liste d\u2019IDs de chunks \u00e0 supprimer (None \u2192 tous). Retour { \"document_id\" : i nt , \"chunks_deleted\" : i nt , \"remaining_chunks\" : i nt } Remarque : si document introuvable \u2192 {\"error\": \"\u2026 introuvable.\"} 4. delete_document(document_id) \u2192 Dict[str, Any] Supprime un document et tous ses chunks en cascade. from vectordb.src.crud import delete_document result = delete_document ( document_id : int ) Description Charge le Document par id . Met \u00e0 jour chunk_count dans la configuration d\u2019index (diminue du nombre de chunks supprim\u00e9s). Supprime le document (cascade supprime les chunks). Commit ou rollback en cas d\u2019erreur. Param\u00e8tres Nom Type Description document_id int Identifiant du document \u00e0 supprimer. Retour En cas de succ\u00e8s : { \"success\" : \"Document avec ID X supprim\u00e9 avec succ\u00e8s.\" } * Si introuvable : { \"error\" : \"Document avec ID X introuvable.\" } ``` : co ntent Re feren ce [ oaici te : 8 ]{ i n dex= 8 }: co ntent Re feren ce [ oaici te : 9 ]{ i n dex= 9 } Module : vectordb/src/crud.py Derni\u00e8re mise \u00e0 jour : 02 mai 2025","title":"Op\u00e9rations CRUD"},{"location":"lib/vectordb/crud_lib/#table-des-matieres","text":"add_document_with_chunks update_document_with_chunks delete_document_chunks delete_document","title":"Table des mati\u00e8res"},{"location":"lib/vectordb/crud_lib/#1-add_document_with_chunksdb-doc-chunks-batch_size10-dictstr-any","text":"Ajoute un document et ses chunks en base, g\u00e9n\u00e8re les embeddings en lot, et g\u00e8re la configuration de l\u2019index. from sqlalchemy.orm import Session from vectordb.src.schemas import DocumentCreate from vectordb.src.crud import add_document_with_chunks result = add_document_with_chunks ( db : Session , doc : DocumentCreate , chunks : List [ dict ], batch_size = 20 )","title":"1. add_document_with_chunks(db, doc, chunks, batch_size=10) \u2192 Dict[str, Any]"},{"location":"lib/vectordb/crud_lib/#description","text":"G\u00e9n\u00e8re corpus_id si manquant. Ins\u00e8re le document ( Document ) avec un flag index_needed . Pour chaque lot de batch_size chunks : Calcule les embeddings via EmbeddingGenerator.generate_embeddings_batch . Ins\u00e8re les objets Chunk (avec flush interm\u00e9diaires). Construit les relations parent\u2194enfant. Met \u00e0 jour ou cr\u00e9e la configuration d\u2019index ( IndexConfig ). Commit ou rollback en cas d\u2019erreur.","title":"Description"},{"location":"lib/vectordb/crud_lib/#parametres","text":"Nom Type Description db Session Session SQLAlchemy active. doc DocumentCreate M\u00e9tadonn\u00e9es du document \u00e0 cr\u00e9er. chunks List[dict] Liste de chunks { content, hierarchy_level, ... } . batch_size int (d\u00e9faut 10) Taille des sous-lots pour la g\u00e9n\u00e9ration d\u2019embeddings.","title":"Param\u00e8tres"},{"location":"lib/vectordb/crud_lib/#retour","text":"{ \"document_id\" : i nt , \"chunks\" : i nt , \"corpus_id\" : s tr , \"index_needed\" : bool } index_needed = True si un nouvel index doit \u00eatre (re)cr\u00e9\u00e9.","title":"Retour"},{"location":"lib/vectordb/crud_lib/#2-update_document_with_chunksdocument_update-new_chunksnone-dictstr-any","text":"Met \u00e0 jour un document existant et ajoute \u00e9ventuellement de nouveaux chunks. from vectordb.src.schemas import DocumentUpdate from vectordb.src.crud import update_document_with_chunks result = update_document_with_chunks ( document_update : DocumentUpdate , new_chunks : List [ dict ] # facultatif )","title":"2. update_document_with_chunks(document_update, new_chunks=None) \u2192 Dict[str, Any]"},{"location":"lib/vectordb/crud_lib/#description_1","text":"Charge le Document par son id . Met \u00e0 jour les champs fournis ( title , theme , etc.). Si new_chunks est fourni : Calcule les embeddings un par un. Ins\u00e8re en bulk via insert(Chunk) . Met \u00e0 jour le compteur chunk_count dans IndexConfig . Si corpus_id change, ajuste les compteurs sur les anciennes/nouvelles configurations d\u2019index. Retourne les m\u00e9tadonn\u00e9es mises \u00e0 jour et si index_needed suite \u00e0 un changement de corpus.","title":"Description"},{"location":"lib/vectordb/crud_lib/#parametres_1","text":"Nom Type Description document_update DocumentUpdate DTO avec l\u2019 id du document et champs modifi\u00e9s. new_chunks List[dict] (optionnel) Nouveaux chunks \u00e0 ajouter.","title":"Param\u00e8tres"},{"location":"lib/vectordb/crud_lib/#retour_1","text":"{ \"id\" : i nt , \"title\" : s tr , \"theme\" : s tr , \"document_type\" : s tr , \"publish_date\" : da te , \"corpus_id\" : s tr , \"chunks\" : { \"total\" : i nt , \"added\" : i nt }, \"index_needed\" : bool } En cas de document introuvable : {\"error\": \"\u2026 introuvable.\"}","title":"Retour"},{"location":"lib/vectordb/crud_lib/#3-delete_document_chunksdocument_id-chunk_idsnone-dictstr-any","text":"Supprime un ou plusieurs chunks d\u2019un document, ou tous si chunk_ids non fourni. from vectordb.src.crud import delete_document_chunks result = delete_document_chunks ( document_id : int , chunk_ids : Optional [ List [ int ]])","title":"3. delete_document_chunks(document_id, chunk_ids=None) \u2192 Dict[str, Any]"},{"location":"lib/vectordb/crud_lib/#description_2","text":"V\u00e9rifie l\u2019existence du Document . Si chunk_ids est une liste : Supprime uniquement ces chunks. Sinon : Supprime tous les chunks associ\u00e9s. Met \u00e0 jour chunk_count dans IndexConfig . Commit ou rollback en cas d\u2019erreur.","title":"Description"},{"location":"lib/vectordb/crud_lib/#parametres_2","text":"Nom Type Description document_id int Identifiant du document. chunk_ids List[int] (optionnel) Liste d\u2019IDs de chunks \u00e0 supprimer (None \u2192 tous).","title":"Param\u00e8tres"},{"location":"lib/vectordb/crud_lib/#retour_2","text":"{ \"document_id\" : i nt , \"chunks_deleted\" : i nt , \"remaining_chunks\" : i nt } Remarque : si document introuvable \u2192 {\"error\": \"\u2026 introuvable.\"}","title":"Retour"},{"location":"lib/vectordb/crud_lib/#4-delete_documentdocument_id-dictstr-any","text":"Supprime un document et tous ses chunks en cascade. from vectordb.src.crud import delete_document result = delete_document ( document_id : int )","title":"4. delete_document(document_id) \u2192 Dict[str, Any]"},{"location":"lib/vectordb/crud_lib/#description_3","text":"Charge le Document par id . Met \u00e0 jour chunk_count dans la configuration d\u2019index (diminue du nombre de chunks supprim\u00e9s). Supprime le document (cascade supprime les chunks). Commit ou rollback en cas d\u2019erreur.","title":"Description"},{"location":"lib/vectordb/crud_lib/#parametres_3","text":"Nom Type Description document_id int Identifiant du document \u00e0 supprimer.","title":"Param\u00e8tres"},{"location":"lib/vectordb/crud_lib/#retour_3","text":"En cas de succ\u00e8s : { \"success\" : \"Document avec ID X supprim\u00e9 avec succ\u00e8s.\" } * Si introuvable : { \"error\" : \"Document avec ID X introuvable.\" } ``` : co ntent Re feren ce [ oaici te : 8 ]{ i n dex= 8 }: co ntent Re feren ce [ oaici te : 9 ]{ i n dex= 9 } Module : vectordb/src/crud.py Derni\u00e8re mise \u00e0 jour : 02 mai 2025","title":"Retour"},{"location":"lib/vectordb/index_lib/","text":"R\u00e9flexion durant 7 secondes # Module `index_manager` Gestion des index vectoriels pour pgvector au sein de **Cl\u00e9a-API** . Ce module offre une API simple pour cr\u00e9er, supprimer et contr\u00f4ler l\u2019\u00e9tat des index IVFFLAT associ\u00e9s aux chunks de documents. --- ## Installation Le module `index_manager` fait partie du package `vectordb` . Aucun paquet externe n\u2019est n\u00e9cessaire, si ce n\u2019est votre installation PostgreSQL/pgvector et SQLAlchemy. ```bash pip install vectordb Table des mati\u00e8res Cr\u00e9er un index simple Supprimer un index V\u00e9rifier l\u2019\u00e9tat d\u2019un index V\u00e9rifier tous les index 1. create_simple_index(corpus_id: str) \u2192 dict Cr\u00e9e un index IVFFLAT standard sur une vue mat\u00e9rialis\u00e9e des chunks appartenant au corpus. from vectordb.src.index_manager import create_simple_index result = create_simple_index ( \"3159e84e-9dc6-41a7-a464-bb6c3894a5ad\" ) print ( result ) Param\u00e8tre Type Description corpus_id str UUID du corpus \u00e0 indexer (avec tirets). Retourne un dictionnaire comportant : status : \"success\" , \"exists\" ou \"error\" . message : message human-readable. Sur succ\u00e8s : index_type : toujours \"ivfflat\" . lists : nombre de listes utilis\u00e9es pour IVFFLAT. documents_updated : nombre de documents flagu\u00e9s index_needed=False . view_name : nom de la vue mat\u00e9rialis\u00e9e cr\u00e9\u00e9e. Exemple de sortie { \"status\" : \"success\" , \"message\" : \"Index vectoriel cr\u00e9\u00e9 pour 123 chunks dans le corpus 3159e84e-9dc6-41a7-a464-bb6c3894a5ad\" , \"index_type\" : \"ivfflat\" , \"lists\" : 11 , \"documents_updated\" : 42 , \"view_name\" : \"temp_corpus_chunks_3159e84e_9dc6_41a7_a464_bb6c3894a5ad\" } 2. drop_index(corpus_id: str) \u2192 dict Supprime l\u2019index et la vue mat\u00e9rialis\u00e9e correspondant au corpus. from vectordb.src.index_manager import drop_index result = drop_index ( \"3159e84e-9dc6-41a7-a464-bb6c3894a5ad\" ) print ( result ) Param\u00e8tre Type Description corpus_id str UUID du corpus dont on veut supprimer l\u2019index. Retourne : status : \"success\" , \"warning\" (si l\u2019index n\u2019existait pas) ou \"error\" . message : explication de l\u2019op\u00e9ration. Exemple de sortie { \"status\" : \"success\" , \"message\" : \"Index idx_vector_3159e84e_9dc6_41a7_a464_bb6c3894a5ad et vue temp_corpus_chunks_3159e84e_9dc6_41a7_a464_bb6c3894a5ad supprim\u00e9s avec succ\u00e8s\" } 3. check_index_status(corpus_id: str) \u2192 dict R\u00e9cup\u00e8re l\u2019\u00e9tat courant de l\u2019index vectoriel pour un corpus donn\u00e9. from vectordb.src.index_manager import check_index_status status = check_index_status ( \"3159e84e-9dc6-41a7-a464-bb6c3894a5ad\" ) print ( status ) Param\u00e8tre Type Description corpus_id str UUID du corpus. Retourne un objet contenant : corpus_id : UUID interrog\u00e9. index_exists : bool\u00e9en, l\u2019index existe-t-il en base ? config_exists : bool\u00e9en, la config Pydantic/SQLAlchemy existe-t-elle ? is_indexed : bool\u00e9en, l\u2019index est-il actif selon la config ? index_type : \"ivfflat\" ou \"hnsw\" ou null . chunk_count : nombre total de chunks dans le corpus. indexed_chunks : nombre de chunks r\u00e9ellement index\u00e9s (config). last_indexed : date du dernier indexe ( datetime ) ou null . Exemple de sortie { \"corpus_id\" : \"3159e84e-9dc6-41a7-a464-bb6c3894a5ad\" , \"index_exists\" : true , \"config_exists\" : true , \"is_indexed\" : true , \"index_type\" : \"ivfflat\" , \"chunk_count\" : 123 , \"indexed_chunks\" : 123 , \"last_indexed\" : \"2025-05-02T14:23:10.123456\" } 4. check_all_indexes() \u2192 dict Balaye tous les corpus en base et renvoie l\u2019\u00e9tat de leurs index. from vectordb.src.index_manager import check_all_indexes all_status = check_all_indexes () print ( all_status ) Retourne : status : \"success\" ou \"error\" . corpus_count : nombre de corpus trouv\u00e9s. indexes : tableau d\u2019objets identiques \u00e0 la sortie de check_index_status() . Exemple de sortie { \"status\" : \"success\" , \"corpus_count\" : 3 , \"indexes\" : [ { \"corpus_id\" : \"aaa111...\" , \"index_exists\" : true , \"config_exists\" : true , \"is_indexed\" : true , \"index_type\" : \"ivfflat\" , \"chunk_count\" : 200 , \"indexed_chunks\" : 200 , \"last_indexed\" : \"2025-05-01T09:12:34\" }, { \"corpus_id\" : \"bbb222...\" , \"index_exists\" : false , \"config_exists\" : false , \"is_indexed\" : false , \"index_type\" : null , \"chunk_count\" : 0 , \"indexed_chunks\" : 0 , \"last_indexed\" : null } ] } Logging & erreurs Toutes les op\u00e9rations journalisent en niveau INFO et WARNING via le logger standard. En cas d\u2019erreur, la transaction est roll-back\u00e9e et {\"status\":\"error\",\"message\":...} est retourn\u00e9. Module : vectordb/src/index_manager.py Derni\u00e8re mise \u00e0 jour : 02 mai 2025","title":"Indexation vectorielle"},{"location":"lib/vectordb/index_lib/#table-des-matieres","text":"Cr\u00e9er un index simple Supprimer un index V\u00e9rifier l\u2019\u00e9tat d\u2019un index V\u00e9rifier tous les index","title":"Table des mati\u00e8res"},{"location":"lib/vectordb/index_lib/#1-create_simple_indexcorpus_id-str-dict","text":"Cr\u00e9e un index IVFFLAT standard sur une vue mat\u00e9rialis\u00e9e des chunks appartenant au corpus. from vectordb.src.index_manager import create_simple_index result = create_simple_index ( \"3159e84e-9dc6-41a7-a464-bb6c3894a5ad\" ) print ( result ) Param\u00e8tre Type Description corpus_id str UUID du corpus \u00e0 indexer (avec tirets). Retourne un dictionnaire comportant : status : \"success\" , \"exists\" ou \"error\" . message : message human-readable. Sur succ\u00e8s : index_type : toujours \"ivfflat\" . lists : nombre de listes utilis\u00e9es pour IVFFLAT. documents_updated : nombre de documents flagu\u00e9s index_needed=False . view_name : nom de la vue mat\u00e9rialis\u00e9e cr\u00e9\u00e9e. Exemple de sortie { \"status\" : \"success\" , \"message\" : \"Index vectoriel cr\u00e9\u00e9 pour 123 chunks dans le corpus 3159e84e-9dc6-41a7-a464-bb6c3894a5ad\" , \"index_type\" : \"ivfflat\" , \"lists\" : 11 , \"documents_updated\" : 42 , \"view_name\" : \"temp_corpus_chunks_3159e84e_9dc6_41a7_a464_bb6c3894a5ad\" }","title":"1. create_simple_index(corpus_id: str) \u2192 dict"},{"location":"lib/vectordb/index_lib/#2-drop_indexcorpus_id-str-dict","text":"Supprime l\u2019index et la vue mat\u00e9rialis\u00e9e correspondant au corpus. from vectordb.src.index_manager import drop_index result = drop_index ( \"3159e84e-9dc6-41a7-a464-bb6c3894a5ad\" ) print ( result ) Param\u00e8tre Type Description corpus_id str UUID du corpus dont on veut supprimer l\u2019index. Retourne : status : \"success\" , \"warning\" (si l\u2019index n\u2019existait pas) ou \"error\" . message : explication de l\u2019op\u00e9ration. Exemple de sortie { \"status\" : \"success\" , \"message\" : \"Index idx_vector_3159e84e_9dc6_41a7_a464_bb6c3894a5ad et vue temp_corpus_chunks_3159e84e_9dc6_41a7_a464_bb6c3894a5ad supprim\u00e9s avec succ\u00e8s\" }","title":"2. drop_index(corpus_id: str) \u2192 dict"},{"location":"lib/vectordb/index_lib/#3-check_index_statuscorpus_id-str-dict","text":"R\u00e9cup\u00e8re l\u2019\u00e9tat courant de l\u2019index vectoriel pour un corpus donn\u00e9. from vectordb.src.index_manager import check_index_status status = check_index_status ( \"3159e84e-9dc6-41a7-a464-bb6c3894a5ad\" ) print ( status ) Param\u00e8tre Type Description corpus_id str UUID du corpus. Retourne un objet contenant : corpus_id : UUID interrog\u00e9. index_exists : bool\u00e9en, l\u2019index existe-t-il en base ? config_exists : bool\u00e9en, la config Pydantic/SQLAlchemy existe-t-elle ? is_indexed : bool\u00e9en, l\u2019index est-il actif selon la config ? index_type : \"ivfflat\" ou \"hnsw\" ou null . chunk_count : nombre total de chunks dans le corpus. indexed_chunks : nombre de chunks r\u00e9ellement index\u00e9s (config). last_indexed : date du dernier indexe ( datetime ) ou null . Exemple de sortie { \"corpus_id\" : \"3159e84e-9dc6-41a7-a464-bb6c3894a5ad\" , \"index_exists\" : true , \"config_exists\" : true , \"is_indexed\" : true , \"index_type\" : \"ivfflat\" , \"chunk_count\" : 123 , \"indexed_chunks\" : 123 , \"last_indexed\" : \"2025-05-02T14:23:10.123456\" }","title":"3. check_index_status(corpus_id: str) \u2192 dict"},{"location":"lib/vectordb/index_lib/#4-check_all_indexes-dict","text":"Balaye tous les corpus en base et renvoie l\u2019\u00e9tat de leurs index. from vectordb.src.index_manager import check_all_indexes all_status = check_all_indexes () print ( all_status ) Retourne : status : \"success\" ou \"error\" . corpus_count : nombre de corpus trouv\u00e9s. indexes : tableau d\u2019objets identiques \u00e0 la sortie de check_index_status() . Exemple de sortie { \"status\" : \"success\" , \"corpus_count\" : 3 , \"indexes\" : [ { \"corpus_id\" : \"aaa111...\" , \"index_exists\" : true , \"config_exists\" : true , \"is_indexed\" : true , \"index_type\" : \"ivfflat\" , \"chunk_count\" : 200 , \"indexed_chunks\" : 200 , \"last_indexed\" : \"2025-05-01T09:12:34\" }, { \"corpus_id\" : \"bbb222...\" , \"index_exists\" : false , \"config_exists\" : false , \"is_indexed\" : false , \"index_type\" : null , \"chunk_count\" : 0 , \"indexed_chunks\" : 0 , \"last_indexed\" : null } ] }","title":"4. check_all_indexes() \u2192 dict"},{"location":"lib/vectordb/index_lib/#logging-erreurs","text":"Toutes les op\u00e9rations journalisent en niveau INFO et WARNING via le logger standard. En cas d\u2019erreur, la transaction est roll-back\u00e9e et {\"status\":\"error\",\"message\":...} est retourn\u00e9. Module : vectordb/src/index_manager.py Derni\u00e8re mise \u00e0 jour : 02 mai 2025","title":"Logging &amp; erreurs"},{"location":"lib/vectordb/search_lib/","text":"G\u00e9n\u00e9ration de la documentation L'utilisateur veut que je g\u00e9n\u00e8re la documentation pour un module search.py . Je vais r\u00e9diger la doc en format Markdown pour d\u00e9crire les fonctions, classes et m\u00e9thodes principales du moteur de recherche, comme SearchEngine , avec ses m\u00e9thodes __init__ , hybrid_search , _build_sql et _get_context . Je vais inclure des descriptions, des exemples, des param\u00e8tres et les valeurs de retour. Je vais aussi v\u00e9rifier si les sch\u00e9mas Pydantic sont \u00e0 int\u00e9grer dans cette documentation, en particulier SearchRequest , SearchResponse , ChunkResult , et HierarchicalContext . R\u00e9flexion durant 6 secondes Module search (Hybrid Semantic / Metadata Search) Ce module impl\u00e9mente un moteur de recherche hybride combinant filtres SQL, similarit\u00e9 vectorielle (pgvector) et rerank via Cross-Encoder. Table des mati\u00e8res Installation Mod\u00e8les (schemas) Classe SearchEngine Constructeur M\u00e9thode hybrid_search M\u00e9thode priv\u00e9e _build_sql M\u00e9thode priv\u00e9e _get_context Exemple d\u2019utilisation Installation pip install clea_vectordb # ou via votre setup.py/pyproject.toml Mod\u00e8les (schemas) Les Pydantic schemas utilis\u00e9s par le moteur se trouvent dans vectordb/src/schemas.py : SearchRequest : param\u00e8tres de la recherche (requ\u00eate, filtres, pagination, etc.). ChunkResult : un chunk renvoy\u00e9 (camelCase). HierarchicalContext : contexte parent (niveaux 0\u20132). SearchResponse : enveloppe de r\u00e9ponse (requ\u00eate, total, liste des ChunkResult ). Pour la d\u00e9finition d\u00e9taill\u00e9e de ces sch\u00e9mas, r\u00e9f\u00e9rez-vous \u00e0 la section Components \u2192 Schemas dans votre OpenAPI/Swagger. Classe SearchEngine Constructeur engine = SearchEngine () Initialise : un g\u00e9n\u00e9rateur d\u2019embeddings ( EmbeddingGenerator ) un ranker Cross-Encoder ( ResultRanker ) M\u00e9thode hybrid_search def hybrid_search ( self , db : Session , req : SearchRequest ) -> SearchResponse : ... Arguments db: Session \u2013 session SQLAlchemy req: SearchRequest \u2013 param\u00e8tres de la recherche Fonctionnement G\u00e9n\u00e8re l\u2019embedding de la requ\u00eate. Monte la requ\u00eate SQL pour ANN + m\u00e9tadonn\u00e9es (via _build_sql ). Ex\u00e9cute db.execute(text(sql), params) . Si pas de r\u00e9sultat, renvoie un SearchResponse vide. Rerank les top k \u00d7 3 r\u00e9sultats avec un Cross-Encoder. Construit la liste finale de ChunkResult , en r\u00e9cup\u00e9rant le contexte hi\u00e9rarchique si req.hierarchical=True . Renvoie un SearchResponse(query, topK, totalResults, results) . Retour SearchResponse contenant : query (str) topK (int) totalResults (int) results ( List[ChunkResult] ) M\u00e9thode priv\u00e9e _build_sql @staticmethod def _build_sql ( req : SearchRequest ) -> Tuple [ str , dict [ str , Any ]]: ... But : assembler dynamiquement la clause WHERE SQL selon les filtres de req Filtres g\u00e9r\u00e9s : theme , document_type plage start_date \u2013 end_date corpus_id hierarchy_level Structure : WITH ranked AS ( SELECT \u2026 , c . embedding <=> (: query_embedding ):: vector AS distance FROM chunks c JOIN documents d ON \u2026 WHERE 1 = 1 [ AND d . theme = : theme ] [ AND \u2026 ] ORDER BY distance LIMIT : expanded_limit ) SELECT * FROM ranked ORDER BY distance LIMIT : top_k ; * Retour : tuple (sql: str, params: dict) . M\u00e9thode priv\u00e9e _get_context @staticmethod def _get_context ( db : Session , chunk_id : int ) -> Optional [ HierarchicalContext ]: ... But : pour un chunk donn\u00e9, remonter r\u00e9cursivement ses parents (niveaux 0\u20132) Retour : instanciation de HierarchicalContext ou None si pas de parent . Exemple d\u2019utilisation from sqlalchemy import create_engine from sqlalchemy.orm import sessionmaker from vectordb.src.search import SearchEngine , SearchRequest # 1. Pr\u00e9parer la session engine_db = create_engine ( \"postgresql://\u2026\" ) SessionLocal = sessionmaker ( bind = engine_db ) db = SessionLocal () # 2. Instancier le SearchEngine searcher = SearchEngine () # 3. Construire la requ\u00eate req = SearchRequest ( query = \"analyse risques climatiques\" , top_k = 5 , theme = \"RSE\" , corpus_id = \"0207a0ec-394b-475f-912e-edf0315f6fa3\" , hierarchical = True , ) # 4. Ex\u00e9cuter la recherche response = searcher . hybrid_search ( db , req ) # 5. Parcourir les r\u00e9sultats for chunk in response . results : print ( f \"[ { chunk . score : .2f } ] { chunk . title } \u2192 { chunk . content [: 60 ] } \u2026\" ) Voir aussi : les endpoints FastAPI dans search_endpoint.py \u2013 POST /search/hybrid_search \u2192 renvoie List[ChunkResult] .","title":"Recherche hybride"},{"location":"lib/vectordb/search_lib/#module-search-hybrid-semantic-metadata-search","text":"Ce module impl\u00e9mente un moteur de recherche hybride combinant filtres SQL, similarit\u00e9 vectorielle (pgvector) et rerank via Cross-Encoder.","title":"Module search (Hybrid Semantic / Metadata Search)"},{"location":"lib/vectordb/search_lib/#table-des-matieres","text":"Installation Mod\u00e8les (schemas) Classe SearchEngine Constructeur M\u00e9thode hybrid_search M\u00e9thode priv\u00e9e _build_sql M\u00e9thode priv\u00e9e _get_context Exemple d\u2019utilisation","title":"Table des mati\u00e8res"},{"location":"lib/vectordb/search_lib/#installation","text":"pip install clea_vectordb # ou via votre setup.py/pyproject.toml","title":"Installation"},{"location":"lib/vectordb/search_lib/#modeles-schemas","text":"Les Pydantic schemas utilis\u00e9s par le moteur se trouvent dans vectordb/src/schemas.py : SearchRequest : param\u00e8tres de la recherche (requ\u00eate, filtres, pagination, etc.). ChunkResult : un chunk renvoy\u00e9 (camelCase). HierarchicalContext : contexte parent (niveaux 0\u20132). SearchResponse : enveloppe de r\u00e9ponse (requ\u00eate, total, liste des ChunkResult ). Pour la d\u00e9finition d\u00e9taill\u00e9e de ces sch\u00e9mas, r\u00e9f\u00e9rez-vous \u00e0 la section Components \u2192 Schemas dans votre OpenAPI/Swagger.","title":"Mod\u00e8les (schemas)"},{"location":"lib/vectordb/search_lib/#classe-searchengine","text":"","title":"Classe SearchEngine&#x20;"},{"location":"lib/vectordb/search_lib/#constructeur","text":"engine = SearchEngine () Initialise : un g\u00e9n\u00e9rateur d\u2019embeddings ( EmbeddingGenerator ) un ranker Cross-Encoder ( ResultRanker )","title":"Constructeur"},{"location":"lib/vectordb/search_lib/#methode-hybrid_search","text":"def hybrid_search ( self , db : Session , req : SearchRequest ) -> SearchResponse : ... Arguments db: Session \u2013 session SQLAlchemy req: SearchRequest \u2013 param\u00e8tres de la recherche Fonctionnement G\u00e9n\u00e8re l\u2019embedding de la requ\u00eate. Monte la requ\u00eate SQL pour ANN + m\u00e9tadonn\u00e9es (via _build_sql ). Ex\u00e9cute db.execute(text(sql), params) . Si pas de r\u00e9sultat, renvoie un SearchResponse vide. Rerank les top k \u00d7 3 r\u00e9sultats avec un Cross-Encoder. Construit la liste finale de ChunkResult , en r\u00e9cup\u00e9rant le contexte hi\u00e9rarchique si req.hierarchical=True . Renvoie un SearchResponse(query, topK, totalResults, results) . Retour SearchResponse contenant : query (str) topK (int) totalResults (int) results ( List[ChunkResult] )","title":"M\u00e9thode hybrid_search"},{"location":"lib/vectordb/search_lib/#methode-privee-_build_sql","text":"@staticmethod def _build_sql ( req : SearchRequest ) -> Tuple [ str , dict [ str , Any ]]: ... But : assembler dynamiquement la clause WHERE SQL selon les filtres de req Filtres g\u00e9r\u00e9s : theme , document_type plage start_date \u2013 end_date corpus_id hierarchy_level Structure : WITH ranked AS ( SELECT \u2026 , c . embedding <=> (: query_embedding ):: vector AS distance FROM chunks c JOIN documents d ON \u2026 WHERE 1 = 1 [ AND d . theme = : theme ] [ AND \u2026 ] ORDER BY distance LIMIT : expanded_limit ) SELECT * FROM ranked ORDER BY distance LIMIT : top_k ; * Retour : tuple (sql: str, params: dict) .","title":"M\u00e9thode priv\u00e9e _build_sql"},{"location":"lib/vectordb/search_lib/#methode-privee-_get_context","text":"@staticmethod def _get_context ( db : Session , chunk_id : int ) -> Optional [ HierarchicalContext ]: ... But : pour un chunk donn\u00e9, remonter r\u00e9cursivement ses parents (niveaux 0\u20132) Retour : instanciation de HierarchicalContext ou None si pas de parent .","title":"M\u00e9thode priv\u00e9e _get_context"},{"location":"lib/vectordb/search_lib/#exemple-dutilisation","text":"from sqlalchemy import create_engine from sqlalchemy.orm import sessionmaker from vectordb.src.search import SearchEngine , SearchRequest # 1. Pr\u00e9parer la session engine_db = create_engine ( \"postgresql://\u2026\" ) SessionLocal = sessionmaker ( bind = engine_db ) db = SessionLocal () # 2. Instancier le SearchEngine searcher = SearchEngine () # 3. Construire la requ\u00eate req = SearchRequest ( query = \"analyse risques climatiques\" , top_k = 5 , theme = \"RSE\" , corpus_id = \"0207a0ec-394b-475f-912e-edf0315f6fa3\" , hierarchical = True , ) # 4. Ex\u00e9cuter la recherche response = searcher . hybrid_search ( db , req ) # 5. Parcourir les r\u00e9sultats for chunk in response . results : print ( f \"[ { chunk . score : .2f } ] { chunk . title } \u2192 { chunk . content [: 60 ] } \u2026\" ) Voir aussi : les endpoints FastAPI dans search_endpoint.py \u2013 POST /search/hybrid_search \u2192 renvoie List[ChunkResult] .","title":"Exemple d\u2019utilisation"}]}